{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import keras.backend.tensorflow_backend as tf\n",
    "import h5py\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "window_size = 22 # as a common trading month\n",
    "d = 0.7\n",
    "shape = [4, window_size, 1] # feature (OHLC), window_size, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 70\n",
    "\n",
    "file_csv_name = 'CSCO_2Y.csv' # Cisco stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_price(file_csv_name, normalize=True):\n",
    "    data = pd.read_csv(file_csv_name)\n",
    "    data = data.replace('null', 0)\n",
    "    data.index = pd.to_datetime(data.Date)\n",
    "    data.drop(['Date', 'Volume', 'Adj Close'], inplace=True, axis=1)\n",
    "    \n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        data['Open'] = min_max_scaler.fit_transform(data.Open.values.reshape(-1,1))\n",
    "        data['High'] = min_max_scaler.fit_transform(data.High.values.reshape(-1,1))\n",
    "        data['Low'] = min_max_scaler.fit_transform(data.Low.values.reshape(-1,1))\n",
    "        data['Close'] = min_max_scaler.fit_transform(data['Close'].values.reshape(-1,1))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file_csv_name)\n",
    "type(data.Open.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4VFX6B/DvIYGEEooJvYYqVRSkrIqAigoqioqurqCr\nKy6K7FrxJ6CLoOzaRdeuWCkiVlAsgIAFAZcWSugtlBBKEkrq/f3xcrh3JjOTOzN3ar6f5+G5M3du\nZg4i5Jv3nPseZRgGiIiIiCh4lSI9ACIiIqJ4wWBFRERE5BAGKyIiIiKHMFgREREROYTBioiIiMgh\nDFZEREREDmGwIiIiInIIgxURERGRQxisiIiIiBySGKkPTktLM1q0aBGpjyciIiKybcWKFQcNw6hb\n3nURC1YtWrTA8uXLI/XxRERERLYppXbYuY5TgUREREQOYbAiIiIicgiDFREREZFDIrbGypOioiLs\n3r0bJ0+ejPRQyAHJyclo0qQJKleuHOmhEBERhUVUBavdu3cjJSUFLVq0gFIq0sOhIBiGgZycHOze\nvRvp6emRHg4REVFYRNVU4MmTJ5GamspQFQeUUkhNTWX1kYiIKpSoClYAGKriCP8siYiooom6YEVE\nREQUqxisiIiIiBzCYOXBvn37cOONN6JVq1bo1q0bBg4ciMzMTHTq1Cnkn52fn48RI0ac/uy+ffti\n6dKlAIBJkyahY8eO6NKlC7p27Xr6fFFREcaMGYM2bdrgnHPOQe/evfHNN98AAI4ePYphw4ahdevW\naNWqFYYNG4ajR496/fyVK1eid+/epz9nxowZAIBHH30UDz/88OnrduzYgZYtW+LIkSOh+k9BREQU\nc6LqrkAX//gHsHKls+/ZtSvwwgs+LzEMA9dccw2GDx+O6dOnAwBWrVqF/fv3OzsWL+644w6kp6dj\n06ZNqFSpErZt24Z169bh119/xddff40//vgDSUlJOHjwIAoLCwEA48aNw969e7F27VokJSVh//79\n+OmnnwAAt99+Ozp16oT3338fAPDYY4/hjjvuwCeffOLx86tVq4b3338fbdq0QVZWFrp164ZLL70U\nY8eORdeuXXHrrbeiffv2GD16NJ544gnUrl07LP9diIiIYgErVm4WLFiAypUr46677jp97qyzzkLT\npk1PPz958iRuu+02dO7cGWeffTYWLFgAAMjIyECPHj3QtWtXdOnSBZs2bQIAfPjhh6fPjxgxAiUl\nJR4/e8uWLVi6dCkmTpyISpXkjyY9PR2DBg3C3r17kZaWhqSkJABAWloaGjVqhOPHj+PNN9/ElClT\nTr9Wv359DB06FJs3b8aKFSswbty4058xfvx4LF++HFu2bPE4hrZt26JNmzYAgEaNGqFevXrIzs5G\n1apV8fzzz+Puu+/G3LlzkZeXh5tvvjmg/8ZERETxKnorVuVUlkJl7dq16Natm89rXnnlFSilsGbN\nGmzYsAEDBgxAZmYmXnvtNYwePRo333wzCgsLUVJSgvXr12PGjBn4+eefUblyZYwcORIfffQRhg0b\nVuZ9MzIy0LVrVyQkJJR5bcCAAZgwYQLatm2Liy++GDfccAMuvPBCbN68Gc2aNUPNmjXLfM26devK\nvF9CQgK6du2KjIwMtGrVyufv8/fff0dhYeHp6wYOHIi3334bw4cPx5IlS3x+LRERUUUUvcEqii1Z\nsgSjRo0CAJx55plo3rw5MjMz0bt3b0yaNAm7d+/GkCFD0KZNG/z4449YsWIFzj33XADAiRMnUK9e\nPb8/s0aNGlixYgUWL16MBQsW4IYbbsDkyZNxzjnnOPp70/bu3YtbbrkF77333unqGQDcfffdOHHi\nBNq1axeSzyUiIopl5QYrpdQ7AK4AcMAwjDKrt5U0K3oRwEAAxwHcahjGH04PNFw6duyIWbNmBfS1\nN910E3r27Ik5c+Zg4MCBeP3112EYBoYPH46nnnrK1mevWrUKJSUlHqtWCQkJ6Nu3L/r27YvOnTvj\nvffew9ChQ7Fz507k5uaWqVp16NABK1euRGlp6elwVFpaipUrV6JDhw5ex5Gbm4tBgwZh0qRJ6NWr\nl8trlSpVcglaREREZLLzHXIqgMt8vH45gDanft0J4NXghxU5/fv3R0FBAd54443T51avXo1du3ad\nfn7BBRfgo48+AgBkZmZi586daNeuHbZu3YqWLVvi3nvvxeDBg7F69WpcdNFFmDVrFg4cOAAAOHTo\nEHbs2OHxs1u1aoXu3bvjscceg2EYAIDt27djzpw52Lhx4+k1W4Dcvde8eXNUq1YNt99+O0aPHn16\nMXt2djY++eQTtG7dGmeffTYmTpx4+usmTpyIc845B61bt/Y4hsLCQlxzzTUYNmwYrrvuukD+ExIR\nEVVY5QYrwzAWATjk45LBAN43xG8AaiulGjo1wHBTSuGzzz7DDz/8gFatWqFjx4545JFH0KBBg9PX\njBw5EqWlpejcuTNuuOEGTJ06FUlJSZg5cyY6deqErl27Yu3atRg2bBg6dOiAiRMnYsCAAejSpQsu\nueQS7N271+vnv/XWW9i/fz9at26NTp064dZbb0W9evWQn5+P4cOHo0OHDujSpQvWrVuHxx9/HICE\npbp166JDhw7o1KkTrrjiitPVq7fffhuZmZlo1aoVWrVqhczMTLz99tteP3/mzJlYtGgRpk6diq5d\nu6Jr165YWc7dmWPGjMG8efP8+K9MREQVTmlppEcQFkpXRnxepFQLAF97mQr8GsBkwzCWnHr+I4CH\nDcNY7us9u3fvbixf7nrJ+vXr0b59e9uDp+jHP1MiIsIffwD9+gHffQf07Bnp0QREKbXCMIzu5V0X\n1sUySqk7lVLLlVLLs7Ozw/nRREREFAmGIb0pc3OBNWsiPZqQc+KuwD0AmlqeNzl1rgzDMN4A8AYg\nFSsHPjtm9ezZEwUFBS7nPvjgA3Tu3Dksn79mzRrccsstLueSkpJOd3MnIiJyxOzZwOLF8jhMzbYj\nyYlg9SWAe5RS0wH0BHDUMAzvi4jKYRgG5EbD+BbpANO5c+dy104Fy840MxERxbi8PCA5Gahcuexr\nBQXAgw8CnToBO3YAp27kimflTgUqpaYB+BVAO6XUbqXU7Uqpu5RSujX5XABbAWwG8CaAkYEOJjk5\nGTk5OfyGHAcMw0BOTg6Sk5MjPRQiIgqlmjWBwYM9v7ZkCbBtG/CvfwH16wNZWdIA3MdNXLGu3IqV\nYRh/Lud1A8DdTgymSZMm2L17N7j+Kj4kJyejSZMmkR4GERGFit6i7ZtvPL+eny/HFi0kWM2aJb8O\nHQImTAjLEMMtqjqvV65cGenp6ZEeBhEREdlx5Ijv10+ckGPVqkDduub5Y8dCN6YIYwttIiIiCkxO\nju/XT56UY3Ky6/TfHo/3uMUFBisiIiIKjDVYFReXfd1asdJLQ9q1Y7AiIiIiKuPgQfOxp1YK1orV\nG28AixYB3bsDu3eHZ3wRwGBFREREgbFWrDyFJV2xSk4GzjgDuOACoHFjuTswTjsAMFgRERFRYKzB\naufOsq/rilVSknmuSROgsBCI0w4ADFZEREQUGGuw2rat7OsnTki1ytr4u0UL79fHAQYrIiIiCkxO\njvSnSk31HJROnpSF61atW8tx8+bQjy8CoqqPFREREcWQgwclVFWrBmzdWvb1kyelYmWVni4VrC1b\nwjPGMGPFioiIiAKTkyPBqmVL71OB7hWr5GRZwM5gRURERGShg1WzZsCuXWVf91SxAoBWreJ2KpDB\nioiIiAKjg1WNGhKi9N6BmqeKFSDrrFixIiIiIjrFMCRYpaWZ4Um3V9B8Vaz27zc3aY4jDFZERETk\nv/x86UeVmuo9WHmrWLVqJcc4rFoxWBEREZH/dA8ra7DSndY1XxUrgMGKiIiICIBrsNLh6cQJ4Phx\n2Wj5888997EC4jpYsY8VERER+U8Hq7Q0oLhYHp84ASxfDmRmAgsWmJ3X3dWuLYEsDu8MZMWKiIiI\n/OdtKvDXX+VxZqb3ihUgVas4rFgxWBEREZFvBQVlWykcPChHX8HKW8UKkGD144/AZZeVfe8YxmBF\nREREvvXuDfztb67ndMWqTh3PwWrrVuDoUaBmTc/v2bKlHOfNAz74wPkxRwiDFREREXlXWAisWgVM\nnQqsX2+ez8mRtVKJiWawWrcOOHAAuPBC87pevTy/b/Xq5uPffnN82JHCYEVERETebd8OlJZKQ9AJ\nE8zzujkoYAarBQvkaL3uvPM8v+/IkXJdvXoy1RgnGKyIiIjIO73AvE8fYMYMICNDnuvtbABzHdX8\n+VKJ+tOfgFtvlcpV7dqe37dWLWDcOJkqZLAiIiKiCmHrVjm+8oqEJl2NOnjQDFbWNVbnnivTg+++\nCyxcWP77JyUxWBEREVEFsW2bBKeOHYHRo4FPPpGqlbViZW2p0Lu3f+9vDVa7dwNTpjgz7ghhsCIi\nIiLv8vOBlBRAKWDECFlrtXCh5zVWAHDOOf69f1KSucfgxx8D994LHD7syNAjgcGKiIiIvCsuBipX\nlse6QnX0qASuOnXkeUKCeb3ersYua8VKB6oYnhpksCIiIiLviopkzRQglSmlzOagnrqq6/5UdlmD\n1ZEjcmSwIiIiorhkrVgpJQvYs7PluadgVauWf+/vKVgVFgY21ijAYEVERETeWStWAFCjhlmx8rZd\njT8YrIiIiKjCKC52DVbVq5vb2bgHq6ZN/X//OJsKTCz/EiIiIqqwrFOBgPeK1ZEjrtfZxYoVERER\nVRh2pwJr1QKqVfP//a3B6uhROTJYERERUVzyNBWYlyePPS1e9xcrVkRERFRheJoK1JxcvF5QIFvi\nADG9xorBioiIiLzzNBWoORWsiotdu62zYkVERERxyb1iVb26+dipYAUA+/eb5xisiIiIKC75qlg5\ntcYKcA1WnAokIiKiuOS+eD0UU4EAK1ZERERUAUTzVOCrrwJDhgQ/BgexQSgRERF55z4VmJJiPg5V\nsLI7FThypBwNQ/YxjAKsWBEREZF37lOB9eqZj3UoCoYTU4G5ucGPwyEMVkREROSd+1Rggwbm40oO\nxAhd9dq/33w/f4PV3r3Bj8MhDFZERETknftUYP36zr6/tWJ1xhkSrvy9KzAry9kxBYHBioiIiLxz\nr1iFMljVrg1UqWK/YlWrlhxZsSIiIqKY4F6xCmSjZV/0VGB2tgSrpCT7wap2bTmyYkVEREQxwX3x\nutP0XYYlJWbFyu5UoB4XgxURERHFBPepQKdZ2zf4OxV4/LgczzvP+XEFiMGKiIiIvHOfCgSAd94B\nXnjBmfevWdN8XKuWTAXu3Ancf7+EOl+OHwfuvRe47jpnxuIABisiIqKK5JdfgAED7E23lZbKL/eK\n1W23AaNHOzMe6xY5umL1ww/Ac88BGRm+v/bECWf2K3QQgxUREVFF8txzwPff21uXpCtGoVxjlZho\nhiMdrLRDh7x/XUmJTBk6vZg+SAxWREREFcXRo8DXX5uPyxOOYAWY04H6rkAtJ8f715w4IUdWrIiI\niCgiZs82pwDtbAOjg1UoF68D5gJ2fypWeuE6K1ZEREQUER9/bFaf7FSsiorkGOqKlbdgZadixWBF\nREREYbd3LzB/PjBkiDz3p2IVzqlA69Ser2ClK1acCiQiIqKw++wzucPvrrvkuT9rrMI5Fag7sQOc\nCiQiIqIotWOHTLP16iXP7VSswjUV6G/FyjCAP/1JHrNiRURERGGXlycBJjlZKlCxXLEqLja7s1uv\njwIMVkRERBVBXp4EGKUkYEVTxSotTSpP1avbq1hZm5vWrRvasfmJwYqIiKgi0MEKkK1joqmP1ejR\n0m1dKXsVK12teugh4MwzQzs2P4X4vxQRERFFBWuwsluxCtdUYN26ZuXJvWJlGBK4rHTFKj09tOMK\nACtWREREFUFurmuwiqY+VlbWYFVcDOTnl71GV6ysXdqjBIMVERFRRaAXrwP+V6zCGazcF6N7Wmel\nK1bWZqJRgsGKiIgonpWWSmuCjRvNilVKiudKkLtwTQVaubdP8LTOihUrIiIiiojPPwd+/VUe62BV\no4a9YBWJqUBWrIiIiChqffON+TjQYMWKlW0MVkRERPHs4EHzsXuwKi31/bWR2DZGV6yqV5cjK1ZE\nREQUNazB6uRJOeqApYOTN5EIVrpiVbu2HFmxIiIioqhhDVb6cY0acixvOjCSFavERBknK1ZEREQU\nNQ4eBPr1k8fXXCNHf4OVnpYLB12xMgwgNZUVKyIiIooSpaUSTM4/X4JK375yXgervDzfXx/JipVh\nAGecYVasFi4EbrpJzsd6xUopdZlSaqNSarNSaoyH12sppb5SSq1SSmUopW5zfqhERETklyNHJFyl\nprqe12us7FSsEhIic1ege8XqkkuAadOAEydiu2KllEoA8AqAywF0APBnpVQHt8vuBrDOMIyzAPQF\n8KxSKvpiJBERUUWi11Slpbme1xWruXN9f/2xY1Ktct+rL5R0WCotda1Y6WalBQVmxSoWgxWAHgA2\nG4ax1TCMQgDTAQx2u8YAkKKUUgBqADgEoNjRkRIREZF/ygtWkycDy5Z5//rjx8M7DQhIhQwwK1bu\ni9dPnjQrVjE6FdgYwC7L892nzlm9DKA9gCwAawCMNgyjTHMMpdSdSqnlSqnl2dnZAQ6ZiIiIbFm3\nTo7Nmrme18EKkKk1byIRrNLSZHzPPCMVq8OHXfttnTwZ8xUrOy4FsBJAIwBdAbyslKrpfpFhGG8Y\nhtHdMIzudevWdeijiYiIyKPvvgMaNwbOPNP1vF5jBZi9rTyJRLCqUkUW1d90k1SsSkuBo0fN163B\nKkYrVnsANLU8b3LqnNVtAGYbYjOAbQDc/hSJiIgobEpKgB9+AAYMKLtGyhqsjh3z/h6RCFZWetG9\nteWCdSownIvqbbITrJYBaKOUSj+1IP1GAF+6XbMTwEUAoJSqD6AdgK1ODpSIiIj8sHy5TKNdemnZ\n16pUAd55Rx77ujMw0sHqjDPkaF1npStWVaqEd1G9TeVuV20YRrFS6h4A8wAkAHjHMIwMpdRdp15/\nDcATAKYqpdYAUAAeNgzjoNc3JSIiotCaN0+Cx8UXe3798svlWF7Fql4958dml6+KVRSurwJsBCsA\nMAxjLoC5budeszzOAjDA2aERERFRwObNA7p3L9vDStPd1KN5KlBXrEaMMM9ZK1ZRiJ3XiYiI4s3G\njcAvv8j6Km90YPI1Faj7WEWKDoU7d5rnorxixWBFREQUT/btM+8C9LS+SktIkO1jorliVbt22XOs\nWBEREVHYzJolx3vuAc47z/e1NWpEd7BK9LBiKcorVrbWWBEREVGM+PproH17YMqU8q+tXt37VGBp\nqYQuvRYrWrBiRURERGFz4ADQqpW9a6tX916x0oGrVi1nxhWs8ePlePIkkJVlLmyPMgxWRERE8SQv\nz7UBqC++pgJzc+VYs8xGKpHxl7/I8dAh4I8/gN69IzseLxisiIiI4klurv1g5WsqMNqCVdNTm8As\nWgQUFwPnnx/Z8XjBYEVERBRP8vLshyFfU4F5eXK0G9JC5cIL5ZicLL9+/VWes2JFREREIVVcDJw4\nEV9Tgd98Iy0kAAlWRUVAx45cY0VEREQhYBgSPkpL/a8y1agR/VOBVasC9evL4+RkOZbXRiKCGKyI\niIhi2dKlwMCBwNSp8RmsrHSwitL1VQCDFRERUWzTGxT/979msLIbhnSwMoyyr0VzsGLFioiIiEJC\nV5xWrADmz5fH/lSsSkulN5Q7HawivXjdKjkZaNgQSE+P9Ei8Yud1IiKiWKYXnysFPPOMPPYnWAES\nzqpWdX0tL0/OVa7szDidcP75ciejUpEeiVcMVkRERLFMB6shQ4BPP5XH/garvDygbl3X13Jzo2sa\nELC3TU+EcSqQiIgolumpwAcfNM8FUrFy50+jUTqNwYqIiCiWHTsmU2M9eph3y9mtNOng5ClYZWcD\naWnOjLECYbAiIiKKZceOmeuOJkwABg8G6tSx97W+KlZ798pCcfILgxUREVEsy883A1K/fsDnnwMJ\nCfa+1lew2rcPaNDAmTFWIAxWREREsUxXrALhLVgVFgI5OQxWAWCwIiIiimVOBKu8POln9fLLwJEj\nwP79cp5TgX5juwUiIqJYZp0K9Je1YvXpp8CoUcCOHcDQoXKeFSu/sWJFREQUy4KpWFWtClSqJMHq\n99/lXEqKLFwHWLEKAIMVERFRLAsmWCll7he4erWcq1dPFq4DrFgFgFOBREREsSyYqUDADFbr18vz\noiJzY+f69YMfXwXDihUREVEsC6ZiBUiwyssDTpyQ54WFUrFKS4uufQJjBIMVERFRrCookLYI7vv8\n+UNXrLSiIvawCgKDFRERUazauBEoKQE6dgz8PXSwMgx5XljIrutBYLAiIiKKRb/+Kr8AoEOHwN9H\nB6uSEnnOilVQuHidiIgo1mRny4bLul1Cu3aBv1dKCrBli0wrAnJkxSpgrFgRERHFmp9+kk7px44B\nrVsDSUmBv5devF5YKM+zs+UxK1YBYbAiIiKKNT/9ZD4OZn0VIMHq6FFzKnDnTjkyWAWEwYqIiCjW\nLFxoPg5mfRUgwerYMfO5DlacCgwIgxUREVEsyc4G1q6VPlOAMxUrK1asgsJgRUREFEsWLZLjyJGy\ncL1bt+Dezz1Y6bVWrFgFhMGKiIgolixcCFSrBowdC2zbBrRtG9z7edoOJzkZqFkzuPetoBisiIiI\nYsnChcB558l2M82aBf9+KSllzzVsKBs0k98YrIiIiGLFkSOyvurCC517T08VK26+HDAGKyIiolix\nb58cW7Vy7j09BavatZ17/wqGwYqIiChWHD4sRyeDj6dg5ekc2cJgRUREFCuOHJFjqINV9erOvX8F\nw2BFREQUK0IdrHSgYsUqYAxWREREsSJcwYoVq4AxWBEREcWKUASratXM1grJyXJkxSpgDFZERESx\nYN06YNw4CT86ADlBKTNI6YDFilXAGKyIiIhiwZVXAiUlQEGB8+/tHqxYsQoYgxUREVEsOHZMjobh\n/Hu7BylWrALGYEVERBQLUlND996sWDmGwYqIiCgW6IpVKLBi5RgGKyIiomhXUgLs2QP07g0sW+b8\n++uNmLl4PWgMVkRERNHuwAGguBi45Rage3fn39+9YsWpwIAxWBEREUW7zZvl2KxZaN5fBym9ML5q\n1dB8TgXAYEVERBTtFi+WY8+eoXn/GjWAypWlKgYAVaqE5nMqAAYrIiKiSDpwALjwQmDXLu/XLFoE\ndOwIpKWFZgyDBgF33AGMGSPP69YNzedUAAxWREREkfThhxKcJk/2/HpxMfDzzxK+QmXAAOC//wVG\njpTpQE4FBozBioiIKJJq1ZJjdrbn11euBPLzgT59wjcmClhipAdARERUoR09KscDB8xzpaXApEly\n1K0QGKxiAoMVERFRJB08KMeffgL69wcefxzYuBEYP17OX3UV0KYN0LBhxIZI9jFYERERRcr//gdM\nnSqP27QBVq+WtVTJyXJOKVl/de21ERsi+YdrrIiIiCLl738H9u4FOnQAMjPlzsBnnwUaNQKuvFIW\nkh85wmnAGMJgRUREFAlbtgBLl8pjvQ9g1arAfffJa/fea17LYBUzGKyIiIgiYdo08/Hu3WVf113W\nmzUDWrQIy5AoeAxWRERE4WYYwEcfAeeeK88rVy57TdOmcmS1KqZw8ToREVG4rVoFbNgAvP468Ne/\nAj16lL2malXghReAfv3CPz4KGIMVERFRuH38sVSprr0WSE31ft3o0eEbEzmCwYqIiChcCgqA224D\nvv0W6N3bd6iimMRgRUREFC5Ll5qL1tPTIzsWCgkuXiciIgqXvXvNx/quP4orDFZEREThsmmT+bh5\n88iNg0KGwYqIiCgcZs0Cxo0zn7NiFZdsBSul1GVKqY1Kqc1KqTFerumrlFqplMpQSv3k7DCJiIhi\n3Nixrs8bNIjMOCikyl28rpRKAPAKgEsA7AawTCn1pWEY6yzX1AbwXwCXGYaxUylVL1QDJiIiijn5\n+cDWrUDDhrJlzbJlwJlnRnpUFAJ27grsAWCzYRhbAUApNR3AYADrLNfcBGC2YRg7AcAwjANOD5SI\niChm/fADUFQk3dbZ8DOu2ZkKbAxgl+X57lPnrNoCqKOUWqiUWqGUGubpjZRSdyqlliullmdnZwc2\nYiIiomhTWAjk5Hh/fc4coGZN4PzzwzcmiginFq8nAugGYBCASwGMU0q1db/IMIw3DMPobhhG97p1\n6zr00URERBF2881AWprsAQgAb78te/0dPSrn5s4FBgzwvCcgxRU7U4F7ADS1PG9y6pzVbgA5hmEc\nA3BMKbUIwFkAMh0ZJRERUTSbNUuOBw4Aq1cDI0YAJSWyrkopICsLGDQosmOksLBTsVoGoI1SKl0p\nVQXAjQC+dLvmCwDnK6USlVLVAPQEsN7ZoRIREUUpvTXNt98C110n034AkJ0t1SoAuPzyyIyNwqrc\nYGUYRjGAewDMg4SlmYZhZCil7lJK3XXqmvUAvgWwGsDvAN4yDGNt6IZNREQURRo2lOMTTwB5ecDM\nmfL8wAHg99/lDsD69SM3PgobW3sFGoYxF8Bct3OvuT1/GsDTzg2NiIgoRtSvD6xdC2zZAjRqBJxz\njpzPzgbWrwc6d47s+Chs2HmdiIgoWCUl5uOmTYHatYHERGDPHmDzZqB9+8iNjcLKVsWKiIiIfDh2\nzHzctClQqZLcJfjss3KuQ4fIjIvCjhUrIiKiYB0/bj7W7YQOWHpl9+oV3vFQxLBiRUREFCxrsMrN\nlWNpqRz37ePC9QqEFSsiIqJgHT8OXHyxPL7hBjm+9BIwciRDVQXDihUREVGwjh+XO/+++04aggLA\nqFGRHRNFBCtWREREwTAMCVbVqpmhiiosBisiIqJgFBZKu4Vq1SI9EooCDFZERETB0AvXGawIDFZE\nRETBYbAiCwYrIiKiYOhgVb16ZMdBUYHBioiIKBisWJEFgxUREVEwGKzIgsGKiIgoGBs2yLFRo8iO\ng6ICgxUREVEwfvgBqFcP6NQp0iOhKMBgRUREFKjSUglWF1/M5qAEgMGKiIjIP7rTOgCsWQMcOABc\ncklkx0QT/kGOAAAgAElEQVRRg8GKiIjIH88+K60VDh0Cvv9ezjFY0SkMVkRERP546ik5rl0r04Dt\n2wONG0d2TBQ1GKyIiIj8kZsrxz/+ABYtYrWKXDBYERER2bV/P1BcLI9ffBE4cYLBilwwWBEREdm1\nerX5ePt24MorGazIRWKkB0BERBQzMjLkOH++3B3Yrx/bLJALBisiIiK71q4F6taVQEXkAacCiYiI\n7MrIADp2jPQoKIoxWBEREdlhGBKsuHUN+cBgRUREZMeuXUBeHitW5BODFRERkR1r18qRFSvygcGK\niIjIDn1HICtW5AODFRERkR1r1wKNGgF16kR6JBTFGKyIiIjsWLuW04BULgYrIiKi8pSUAOvXcxqQ\nysVgRUREVJ5PP5V9AVmxonIwWBEREZXn4YeBmjVlb0AiHxisiIiIypObC/zlL7KdDZEPDFZERETl\nKSoCKleO9CgoBjBYERERlae4mMGKbGGwIiIiKg8rVmQTgxUREZEvhsGKFdnGYEVERORLcbEcGazI\nBgYrIiIiX4qK5MhgRTYwWBEREfnCYEV+YLAiIiLyhcGK/MBgRURE5AuDFfmBwYqIiMgXBivyA4MV\nERGRLwxW5AcGKyIiIl8YrMgPDFZERES+MFiRHxisiIiIfNENQhMTIzsOigkMVkRERL6wYkV+YLAi\nIiLyhcGK/MBgRURE5AuDFfmBwYqIiMgXBivyA4MVERGRLwxW5AcGKyIiIl8YrMgPDFZERES+MFiR\nHxisiIiIfGGwIj8wWBEREfnCYEV+YLAiIqKK6dAh4PHHgZIS39cxWJEfGKyIiCj+PfAAMHt22XP/\n+hcwZ47vr9Vb2jBYkQ0MVkREFN+KioBnnwWuvdb1fG6uHI8fN8+Vlkoly/3rAe4VSLYwWBERUXzb\nvt3z+YQEOX7wAfD99/J4xAggNRUoKDCv41Qg+YHBioicd9tt8g2KKBpkZpqPn38e+PhjeayD1dy5\nwIABwODBwFtvybmDB82vYbAiPzBYEZHzpk4F3njDrALEgtdeA/r0AQ4ciPRIyGnWYPXww8ATT8hj\nvXYKAC65BJg3z3yek2M+ZrAiPzBYEVHwCgqAN9+Un/L1uhUAePfdyI3JX19+CSxeDPTtC+zdW/Z1\n/c2VYsuxY8DMmebzoiJgwwZgzx7g8GHz/Jgxcu6BB+Q5K1YUIAYrIgrel18Cd94JtGljfmOqUwf4\n4gtg82bzuvx8YNu2sl+fnw889xzw8svhGa+74mJg3TrgrLOAnTuBSy8FDMN8/fhxoEoVYMKEyIyP\nApOXB1x+OfD778CoUa6vzZ/vWpVq2VLWVg0bJs/dK1YJCYBSoR8zxTwGKyIKng5LZ50llStAQlK1\nasCFFwIbN0pQeewxoGNHYMsW4MQJ4KKLgEGDgBYtgPvvBx59NPxjf+YZqUTs2AFcd52MYc0a+aas\nvf22HN97L/zjo8A9+CDwyy/AtGnACy8At94KnH22BKgff3S9+69JEzmmpcnRPVixWkU28d5RIgre\njh1A7drAggVApVM/r11+OdCli4SnM88E+veXytCJE7Kw/b77pGoAAFddJSFs+nSZVkxKCt/YP//c\nfNyqlTnts38/ULOmPNZrxTp2DN+4KHgbNwK9ewNDh8pzPTV9/fUSrI4cMa/VrRRSU+XoPhXIYEU2\nsWJFRIHbtw948kn5Bta8uUyVzJ4tlZ86dYBOnYBFi+Ta+fOBpUuBhg3lm9qVVwJVq0rQ+uILoF8/\nuS47O7y/h+3bgZ49ZeH6RRcB9evLeesidl2Rc+9vRNHt8GHgjDPKnr/oImD3bpmCfvhh1z/rKlWA\nGjVYsaKAMVgRUeAef1ymzn78UYIVAFxzDfDJJ+Y17doBn34qjwsK5PrzzpNGjFdeCSQny2v16smx\naVPgkUfK74YdjCNHgB9+kPFkZUl17aefZAx6HPqbrWEAW7fKY+s3W4p+hw5JwHd30UXm40suAerW\ndX09Lc21YrVrF1C9emjGSHHHVrBSSl2mlNqolNqslBrj47pzlVLFSqnrnBsiEUWlkhKpNGm+Fvae\nfbb5uFcvYMYMWYs1dap5XgcaAJg8GbjiCuDf/5bAc/SoY8MGALz+unxDHTFCglOLFmXHsX+/HLOz\nzc7crFjFFm8Vq9atzcd9+pR9PS3NrFJu3Ah89RVwyy2hGSPFnXLXWCmlEgC8AuASALsBLFNKfWkY\nxjoP1/0bwHehGCgRRZmffpKpwGnTgN9+A2680fu1LVrIN7hevYBu3eTcHXe4XmMNVoCsZxpz6ue4\nf/0LGD/esaFj50456sXo1mClqxe6YqWrVZ07A+vXSxDj3WHRr6hIpvo8VayUkj/7kyc9T/Fde61U\nTceOlYpmQgJw772hHzPFBTuL13sA2GwYxlYAUEpNBzAYwDq360YB+BTAuY6OkIii08cfAykp0q3a\nV6gC5BvZrl3mtJ8n7sFq0SLg1Vflm5unFg3ByMoCOnSQasVbb8nieq1KFflmrIOV/uxu3eRuwfx8\n+X1TdNM9qjxVrACzrYIn//iH3E04aZI8v/pqc+0dUTnsTAU2BrDL8nz3qXOnKaUaA7gGwKu+3kgp\ndadSarlSanl2uBeoEpFzCgpk3dTVV8sCdDuqVTPvGPTEGlZWrJBviI8+CvToIaHMSVlZcnv9f/8r\nU33u3zTr1ZNqHGBWrHSljdOBsUH/OXmqWJUnOVl6s23ZItPSkyc7OzaKa04tXn8BwMOGYZT6usgw\njDcMw+huGEb3uu6LBYkodnz7rSwAv+km595TKakiPfQQcM455vnmzaWdg5OysoBGjeQza9cu+3r7\n9sDq1fJ42zagQQOzzxEXsEfOxx9LhdQOXbEKJFhpLVvKXYPt2gX+HlTh2JkK3AOgqeV5k1PnrLoD\nmK5k3UEagIFKqWLDMD4HEcWfadNkga/17ionZGSUPde8ubRwuOoqaePgawrHjtJS2bKmUSPv1/To\nIf2tDh2SilV6uhnArL2PKLRKS12rnDffLMfjx6UCahjSG83TOildsfI2FUgUInYqVssAtFFKpSul\nqgC4EcCX1gsMw0g3DKOFYRgtAMwCMJKhiihO5efLNMn114ent0/z5nIH4ldfAcOHyzRkMLKz5f18\nBauePeX4++9SsWrZUtZeAdwzMFwef1yqTTrIrlplvjZ7tjRzrVpVAq+nvR2DmQokCkK5wcowjGIA\n9wCYB2A9gJmGYWQope5SSt0V6gESUZRZtEiael57bXg+r18/aTTasqU8//DD4N4vK0uOvoLVuedK\naPz2W7mDMD1d7gwDJJRRaO3aJXeC5uZKu4P9+12nAKdMkTVwN9wg1asVK8q+x5YtMtXbtGnZ14hC\nyNYaK8Mw5hqG0dYwjFaGYUw6de41wzBe83DtrYZhzHJ6oEQUJRYtktDRu3d4Pq9jR7kbb/Nm6Yf1\n9NMyRRQoO8EqJUW24JkyRT6rZUtzy5Pi4sA/m+yxNofNzJSbJA4ckD8PQCqJvXoBL74oz9evL/se\nGRlmVYsojNh5nYj8s2iRrEGqVi28n6uULGzfuNG1Mam/7AQrQBqU6gCXnm4GK1asQm/OHHNt1Guv\nSZ+0t96SDvla794yDdiggfx/sXat+drTTwOzZkmlkyjMGKyIyD87d0buLqnrrpO78z74IPD30MGq\nQQPf151racnXsqU5FciKVWidPClbJP35z0CtWtJPCpBQZb2bvFcvOerF7WPHynHhQglagOxLSRRm\nDFZE5J9jx2ST2khITAQuuABYvjzw98jKkj5V5S2879zZfNy4MStW4bJwoazhGzTI7IifkiLVqZQU\nIClJzukbDF56SY6FhbLg/eKL5c/3+uuBv/89zIMnYrAiIn/l50d2Q9pu3WRxcyBNhktL5WvLmwYE\nXKc6ExJYsQqXOXNkXVTfvrJGCpA7Q5WSX3Xryl5/unp17bXAZZcB33wjC97//GdgwwZg5kzXcEwU\nJgxWRGRfYaEEi0gHK8DznWDlefpp+QZcq5a96994A3jhBXnMxeuhZxjA3Lly40DVqlKdBFz3Zuzf\nX+4GtNJBuUsXmSZmiwWKIDsNQomIxLFjcoxksNJVjEC2ufnsMzmOGGHv+r/9zXzMdguht327NGS9\n7z55rhvQ7t9vXqM3zrbSwYod0ikKsGJFRPbpYBWpNVaAOQXk71RgYSGwciVw//0yXeQvVqxCT298\nnZ4ux44dgb/8BfjoI99fp7vip6aGbmxENrFiRUT2RUPFKjlZgp2/wep//5Ou7X/6U2Cfy8XroZeX\nJ0e9IXelSvbuANVThbwLkKIAgxUR2RcNwQqQu750dcMufdt+oI1NuXg99NyDlV133gns2QP885/O\nj4nITwxWRGRffr4cIx2s6tb1v2L1yy8yxRRoVYMVq9ALNFjVqAE8+6zz4yEKANdYEZF90VSx2rlT\nmknaYRgSrAKdBgRYsQqHQIMVURRhsCIi+6IlWNWtK1vb9O1r7/odO6QxaDDBiovXQ4/BiuIAgxUR\n2RctwWr7djkuXQosXixtEX7+2fv1en2VExUrTgWGTl6e/HdOTo70SIgCxjVWRGRfNLRbAIB77wXm\nz5fHffrIccsW85y7X36RMQezKS+nAkMvL0+qVdaGoEQxhsGKiOyLlorV4MHSOmH4cAlLubnAf/4j\n666aNSt7/S+/yKa9iUH8k1epkvxixSp0dLAiimGcCiQi+/RdgVWrRnYcAFClCjBtGvDooxK0AGDt\n2rLX5eUBq1YFNw2oJSSwYhVKDFYUB1ixIiL7jh2TzYkrRdnPZE2bytF9m5uCAqBmTXnsRLBKTGTF\nKpQYrCgORNm/jkQU1Y4eNYNKNGnYUMKee7DKyJBjkyb27yD0hRWr0MrNZbCimMdgRUT2HT4M1KkT\n6VGUlZgoG/G6B6uVK+X4449AUpIzn8NgFTqsWFEcYLAiIvuiNVgBMh1oDVa//w7cfrs8bt3amc9I\nSOBUYKgUFgK7d0vzV6IYxmBFRPbFSrAqLgbuuEMeDxrk3JowVqxCZ8ECmWoeODDSIyEKCoMVEdl3\n+DBwxhmRHoVnrVpJ49DCQmDiRGDNGmDGDGD2bOc+g4vXQ+fTT6XX2IABkR4JUVAYrIjIvmiuWHXq\nJNWkd98FnngCGDYMGDpU2jI4hYvXnZebC9xzD/Dmm1JdZNd1inFst0BE9pSUyFRNNAcrALj7bqBF\nC+Dll53/DFasnLVuHTBkiOz7CADXXRfZ8RA5gMGKiOw5elSO0Rqs2rUzt535+OPQ3F3GipUzPv0U\n6N9ftiNKSADmzQNKS4FLL430yIiCxmBFRPYcOiTHaA1WSUmyGXOXLkDPnqH5DC5eD96yZVKZ6tMH\nyMkBPvyQ66oorjBYEZE9hw/LMVqDFQC8+mpo378it1u48EIJRKNGlX/t0aNyJ6anquHOnXJctEiO\nTrXCIIoSXLxORPbk5MgxWu8KDIeKWrE6flyC0Jo15V9rGEDt2t63ENq82fV5q1bBj48oijBYEZE9\ne/fKsVGjyI4jksK5eH3aNKBtWwk1kbZ1qxwLCsq/9o8/5Lh2rYQsd5s2mY9TUoDU1ODHRxRFGKyI\nyJ6sLDk2bBjZcURSuBav79gBjBghIWT37tB/Xnn8CVZffmk+3rGj7OuZmebj9HRAqeDGRhRlGKyI\nyJ69e2V9VUXuMxSOilVpKfDXv8q+eYB500CwiotlDVphof9fu2WLHO187fr15uMVK8q+vmkT0K0b\ncP31wL//7f9YiKIcgxUR2ZOVVbGnAYHwVKw++giYP9/ckufuu4FVq4J/3zffBEaODKy/lw5WdipW\nGzcCF18sIdQ9WOXmAvv2ySL4mTOByy7zfyxEUY7Biojsycqq2NOAQHgWry9ZIuuOHnxQnv/xB3D5\n5cG/7/btcgxkzZbdYFVSIlN9Z50lDVvdg5VeuN6mjf9jIIoRDFZEZA8rVvbbLRQXAw89ZAaS8mzd\nClxwgUy3bt4swcO6qNuJTaR1g9dApnLtTgXu3AmcPCnNWrt1A5Yvd13Arheut23r/xiIYgSDFRGV\n79gxCVbNm0d6JJFlt2K1YgXw9NPAI4/Ye9+JE6VS9f77Eqxat5aWBVrNmoGN1xpqdLsM3Y/MrpIS\ns9pVXsVq3z45NmkCdO8u68OsC9j1wnW2WKA4xmBF4WEYcvv1zJmyOJdiy/Ll8g02VB3NY4Xdxeu/\n/y7HTz8t27fJE71X3vLlwK5dEqz09jxAYMHqm2+AatWApUslDOqqkw5Ydu3aBRQVyePygpW1iWy3\nbvJ46VLgq6/k34BNm4CmTWVcRHGKwYpCyzCAhx+W26o7dwZuuAGYMSPSoyJ//fabHHv1iuw4Is3u\n4vWlS6WRamIi8Mwzvq89fNj87ztrlvydcV+DVKOG/2P9+WeZluvVS4LZ//4n570Fq+xs4LnnZEG5\nbq8AmIEsNbX8qUBrsOrcWX7/I0YAV10lW9dkZnJ9FcU9BisKrW3bgP/8R35KfeMN+Un8lVciPSry\n16+/ll33UxG5V6yOHPFcgV21SjqPDx8OTJ1qTpF58v338h4ffyx3Aj78MHDlla7XnDzp/1jz8+X4\n979LuHnvPVlQ7ilYbd4MtGgB3H+/bIj8+efma0uWSK+pc8/1r2KVnCyfp9d2ffedVKy4voriHIMV\nhZb+yfeJJ2SD3MGDyy5o9Ud2tkwpUvgYhgSr3r0jPZLIs1asDh6UADF5sus1hiFrklq2lDv7Cgpk\n7ZQ3334r73P99dISYfLksnvs6Z5W/jhwQNYy/fe/wPPPA8OGyZg8BavFi+VuwYULZX2U9W6+776T\n9VKNGvkXrAD5Om3OHFlzxYoVxTkGKwotPY2gF6s2aSL/ONtpeujpp/THH5d1PtnZjg3RL6+/Dpx3\nXmQ+O1K2bZNv0gxWrovX9dTa9Onm6ydOABMmSLWoWTMJEc2bAytXur7PvHlSucnPl2B1ySXy3u6y\nsoA+fQILVvv3A/XquZ5LTfUcrDZtks8/7zwJQ8uXy/kjR2Ra89JLgaSk8oPVoUNA9epA5cryXK+z\nAszQxYoVxTkGKwqtLVuAKlXM2/QbN5bjnj2+v27zZqBqVddvWoB8kz9+PHLTiXfdBfzyi4yjovj1\nVzkyWLm2W9BVHeum1PfeK+EfkGAFAB07SpV13z6zUjtpkoSZp56SFgve+lQ1bAh06BB4xap+fddz\ndevKDyW6l9Uff0jPqd9+k3WQiYny55yZKVWsH3+U3++AAfL32M4aK12tAlyDlcaKFcU5BisKjZ9+\nAjIyZCowPd28w8lusNIbubovdNf71U2ZIl2c//lP+72CnKDXGP34Y/g+M9J+/VWqEJ06RXokkWet\nWOlgpdcQffAB8NZb5rXWYLVmjYSkJ56Qc+3by/HJJ2Utkq8GoCkpgQcr94rVZZdJONJrqJ54Ali9\nGliwQNY/AtKdvXFjaRfx3Xfy+b162atYuQerLl3k+RVXyPOEBPn3gCiOMViR83JygL595Rvxl18C\nZ55pvtakiRzL21g2N1eO7tMje/bIT9iHDgGjRwMvvCB3HIWLvu196dLwfWak/for0KOH6+3/FZVe\nvG4YwKJFcm77dmDdOqlm9uljXquDVbt25jnrFBsgweeFF8pWlqxSUiTQ6JYHdpSUSGXKPVhdeKGs\ns3rqKXlPPWUHmFN0NWpI8Dt4UKYs+/eX63Sw8rY+8ptv5O+7NVglJUn1+emn5XmLFlL5IopjDFbk\nvNdfNx8XFQHnn28+b9hQ7jAqr2JlbSoIyFRK377yj/2QIbIWZOpUeW3dOll0u2hR+e8bDMOQaRtA\n1tJUBI8/LtXDirauzBu9eH3NGqkIdeggIemSS6SqN22aLD6vXNkMS3/5C/DZZ8DVV5udx3Ny5K7B\n/fvljj1f9EJ2u1WrI0eAa66R/1/dtyCqVAl46SX5+3Teea4/INxzj/m4Zk0JgTt2yPoqQEIS4Lnd\nxIwZ5p2My5a5vnbGGWaViuurqAJgsCJnFRTINJ3+xxiQn5K1ypVlnUdWltxiPnGiGVasdLDSt6mP\nHy/Ti4BMU9x2m+v1/frJ5wwY4Nzvxd3hw+aCen+qB7Ho2DGZvvnXv4AGDWR7FjIrVt9/L88nTJBj\nVpZMAzZqJO0SCgvNbWiSkiRUtW0rU+MlJfIDgt3WFf4EqzVrZPH5N98ADzwA3HRT2WsGDZLGpTt2\nyBY0I0fK3zM9FQhIsNL/j190kRx1pcl9OvCzz+RzzjtPvu7++8t+ZlKSTCda/y0gilMebkMhCsKM\nGfKP9HvvAffdJ9McZ5/tek2dOjLVt2oVMG6cbN1h/WkZMIOVrkDptVWATFWcdZb5fMsWmY557TX5\nhhIKRUXyTUsrbxFvrPvuO7k9HpBeTO63/1dUumL1/fcyxX3ttTKttnNn2d5T7lq3lv9vdu+WipX7\n3wtv9PTzoUO+txQyDJkWLyiQCq6vKqOu+j77LHDnnWWnIq2d3ps2laOuWBUUuDYsfeUVuet37ly5\n4cTbvob6JgiiOMdgRc4pLZXOzR07ytSIUp4rSDVrSrDS/aj27y97jd6bbM8e+YaRnS3fiBITpTqV\nlmZe27Kl/PrtN+CTT6SqFMhGs55cdZV0sM7Lc61SxWuw0rfE61YCAKdvrGrUkDvqFi2SZp4AMGaM\nva/VC9Y/+kgqVtb/h33p0UOOP/wg1a7iYs8d8HfulL83L79sb+q2fn1p3uuJDlbJyRKWADNYuf+/\nn5Ehi+KrVy//M4kqAAYrcs7HH0sV6oMPJFR5U7Om3EmlK0B79gC1asnaj+HD5RvHnj3yk/KuXbIO\nJDtbQs7zz5vv8/rrrtMp+hvVwYPmIvlgffWV+fjFF+Wb4/jx8RmsDAPo2lW+QVsXGDNYmc46S8LN\niRPAxRf797V/+hMwdCgwdqz8t7Y7Fdi8uXyuno6tXFnWd1k3aQakDYj+nGBZg5VmrVgB8nfg4EGp\nUPOOUaLTuMaKnDNzpkwJeFrXYeVesVq1Sp7fd58837NHvnn94x+yWetrr0nFqG5d1/e5806ZitGs\nwcppVaoAo0ZJJa5KlfhcY7Vhg4Sq666Txc86XLHvkEn3ZUpIkJsp/FGpktxwoStQ/mwP9PjjwC23\nyLGoSKbd3C1aJFWjzp39G5cnOlhZ7xp0D1bNm5vtUzp2DP4zieIEgxX5JyPDdR8xq8xMqXh4W2Oh\nuQcr3bNK73iv11d16iTB6aOP5Ll7sHLndLCyLtJNSzOrcHYaJcYi3T7gqaekMevRo3KHl69WABVN\nixZyl5ve2NhfVasCX3whP3z072//666+WrbFGTdO/jy++ML1dcOQ6uqAAZ47uPtL/96s7+W+eN26\n/6G1pQRRBcepQPKPbnmwebO5TQ0g03dbtsii2PLUrCnrqtzvLnIPVs2by91/H3wgz8sLVvp1p7a7\nOXDAfGwNi1WqmGuR4smKFRIg9Z9rcrLrXm8k4frtt+VOyUDVr2/+sOCvSpVkkfyMGfL3R1eRli6V\nSu/gwYGPy8pXxcrTDxW6ckVErFiRHw4fNqtBr73m+tr27RKu7KzHqVnTDFXW5qF6kaxeuN6smdye\n3aKFPHdvdujO6YqVpzYQQPxOBR454lqZI8+uvtrz4vFwfn5enllhBKQBZ61a8poT7EwFatWrs+kn\nkQWDFZW1ezfw4Ydlm23q2++BsnvlrV8vRzvrcaxTKNYFwKWlMqXxxReyZkPfun3rrfK6e7NDd3Xq\nSChwKlhZpzqsYaNy5ficCszLY1uFWKDXeelmowcOSC+pkSMlXDlB/39gnQr0FqzYkZ/IBYMVlfXk\nk7JQtkkTaQ6o/yGdPVtK/hdc4DpNBkgQq1PHXm8eHaxSUoDbbzfPHzokTUBXrJBvEtqDD8q6rvL2\nGEtMlDulcnLKH4MdvipW8RCsDh0y97kDGKxiha4q6jYlX38tP5AMHercZ+ipb2vFSt8heOKE2SjX\nei0RAWCwIk8yM2Wrjv/7P2D+fAlaY8bIP+BXXy3rS6zB6sQJCV3Dh5vrpHzRwapTJ1nsPnasPM/J\nka7NTZqYVSpA3tPu2pGUFCA/3961vhgG8O67sm6rTx9ZOKzFS7AaMkS2G9LfJPPzGaxiQWKi3FF4\n4ICsrRo/Xn7osDbNDVa7dtK2wbo9lW4Kmp9v7uUJMFgRueHfiIqquFgqFp5s2iSVp4kTgXPPlW07\n/v1vWVc0ZIisdbIuEN+7V97P7j/s1mAFAE88IeHt5Em5Q3DyZHsBzZMaNezvqeZLRoZ803r8cami\nWW+tj5c1VqtXy52Z48fL87w8147aFL3q1ZN1jhdcIFWl2bOdXRuXlCSNca09saxb61grnWy1QOSC\nwaqiGjFCfup130z45ElpytmmjfxDfeedrq/36SP/qB86ZIYLPWVW3hooTQcra78d3dOnZ0/gz3/2\n7/di5VTFSq9f0T2HrOJhjVVOjtyMkJYGPPOM2V2eFavYoG/k6NBBps67dg39Z3oKVsOGyb6DRHQa\ng1VFVFQEvPOOPLbeWQTIonTDMDdkHTpUFstOmCD7xyUmmm0N9CJxHazs3oLevr1Ut6zb3bRpI4Hl\n+eeDm1qoUcOZYLVlixytLSW0eJgK3LxZji++KG0t7r2XwSqW6B9OrrpK+mqFg65m5uWZU4F//Wv5\nbVCIKhj2sapocnJcO6N/8w1w6aXy+PhxcxNjvQi9Zk1g+XLX99A/LWdnS5XK34pV3brAypWu5/r1\nkwpKsPuN1ahRdmF9ILZulcX4deqUfS2UwWrWLJkitbahCAVdkTv7bODuu+UGAYDBKlbohevhqFRp\niYlyp661YuXUXYhEcYQVq4okK0uqTwsXAm+8IRu16q7nM2ZIiLj/flnQ3KGD9/fRwUr/4753r/yj\na3dTWW+c2MQ10KlA3epB27LFc7UKMNdYWa93wqJFwPXXA5MmlX3NySBXXCx//tWqyaJn640BDFax\nQa9BdGL7Gn+kpLgGq0C6zxPFOQarisIwpNvzjh3AggXA3/4m02+bN0tIePRR85v3HXf4fq9mzeSo\nGxfO+YAAABpZSURBVHnu3SvdpKPh7qBAFq/PmydrvN57zzznK1jpW9CLiwMbozvDAI4dkz8TQBaV\nW02eLG0krH21gjF3LrB4MTBlitxCb+09xsXrseG994CXXzan7MNFByv9Q5Wnii5RBRcF3wkpLEaN\nAh56SLqY6zt92rSRUPTqq+aaIqD87s1NmkjVZvNm+Uf2t9+iZ0sLfytW774LDBokXcd//FHOFRdL\nAG3Z0vPX6C7TTlSRtm6VdSo1akibi969pdmqfu+33gIeeURuMsjICP7zAKlOpqZKrzJNr5NhxSo2\nNG0qU7jh7pKvg9WcOVItY7AiKoNrrCqKV16Ro+7aDJg/7Y4eLW0VJk+WQFHeuomEBAkda9dKCNu0\nyfvGzOFWo4bc2Vhc7Hsz2oMHpVJ1++2yGW5Bgbkp9M6d8vW+pgIBCT/BTF8eOOD6GbfcAlx+uayB\n27hRQteIEfJns2yZPL/oIrm2uFgqWE2a+P+5CxfK51ibPzZqJGvmGKzIl5QU+QEgM1NakRBRGaxY\nVRR6b7MpU8xz1imgCRMkYNx2m733a9VKppTmz5eqzxVXODfWYOhgUF7V6pprgL/8Rabhxo+XNg/r\n1wMlJb7vCATMYBVsLytrZ/d//lPu1DznHHk+fTpw442yCfL330tItG4jNH683F3p7xh0IHPvYq8r\njrF+tyOFVkqKhH7DAK67LtKjIYpKDFYVRV4ecO21rnfude4M/OMf0l1Z3xlol16o/txzrlNKkWbt\nDq1lZ7suNP/gA2DJEvN5+/ZyJ15BgYSrrVvlfHlrrIINIdYmq1dcIeGpbVu5OeDJJ2XMs2dLBbFZ\nM+Cpp6Sqdvy4NIfMz/d/+579+2WhfqNGruf1FkK+blog0j+4tG/P/1eIvOBUYEWRn192YXJiovSN\nCsSECcBllwE33BD82JxkbWIIyN1L9epJRei556Slw7Bhrl9Tty5wySWy+H76dAlMSUne1405tcZK\nB6s6daRiBsiamXbtZJrwnnvMMehGrmPHypqww4fl+cGD9vuHAXJnKFD29zZokASucK/Zodii/35d\nf31kx0EUxVixqig8BatgNGsmU1XR9o3YvWK1fr0cn39eKkBffOH56xo3loal774rX5Oe7v0uR6em\nAnWD1Y0bXddqTZwoDTufeso89/LLcszLk+ncpCTX9wAk7Pbr5/szdbByr1gB0fdnSdFHBytOAxJ5\nxYpVReF0sIpWuq+O3gdxwwbztQ0bpAGndv31wAMPmM/vu0/CVVaWVHC8cbJipVTZztl9+sgvqyFD\nZE3YjBkS6O69F3jpJdepwMcek2NRkevCdKs9e+QYLXdxUmy59lq5eUXv80lEZbBiVREUFcn6oYpw\nx5de97FmjRw3bjRfW7xYtuW57z7ZWHn6dNe9AC++2Ox47m19FeDsGqszzpBvVHa0bi1/lnXqSPsM\nwLVipVkXubvbtUs+j9uQUCDOOw94+mlWN4l8YLCqCPS0WEWoWKWlyd53K1ZI36fvv5fWEAkJ0tG8\nqEgqVX36lJ3qU8rcIqRpU++f4WTFyp+Ao9tj3HGHOb5Zs+ROQuu0od6uxt2RI3Ln4Z/+ZD/MERGR\nX2wFK6XUZUqpjUqpzUqpMR5ev1kptVoptUYp9YtS6iznh0oBq0jBCpBeXdOny3TF6tVS3TnzTOlP\n1aSJa5XKnZ6C09v2eKKDVb9+ElYCtWePf8Gqf3+Zohw9WtZYpaQAP/wgAfL//s+8LjPT89ePHy9h\n7oUXAh8zERH5VO4aK6VUAoBXAFwCYDeAZUqpLw3DWGe5bBuACw3DOKyUuhzAGwB6hmLAFAB9h1xF\nCVbDh8tdcwMHyh2A9epJC4WMDFlw72vrnREjpCXFVVd5v0YHq4ICef/PPzff0zDsTZP85z/SsX7c\nOPu/r4YNga+/Np+npsqf7XvvyZ2BP/wgi9zdN7gG5NwrrwB//7vZK4uIiBxnZ/F6DwCbDcPYCgBK\nqekABgM4HawMw/jFcv1vAAJoB00hoytWFWGNFSChyD0Yvfii9Gpq0cL311aqVP6WPtZg9tVX0nNq\n7FjpRP3VV8Avv5h37XnyzjvAww9Lqwq94DwQzZvLequhQ2VMffvK+qr335fK3H33SfgCgCeekGsn\nTgz884iIqFx2pgIbA9hleb771DlvbgfwjacXlFJ3KqWWK6WWZ1ubI1JoVbSpQE+UkunA5OTg30u3\nRpg4UbafGT9euqjPmwf88Yf0y/KmoEB6avXvLwEomLVOM2dKlcoa9HQofPJJaTb60kvSZX72bLmj\ni3u7ERGFlKOL15VS/SDB6mFPrxuG8YZhGN0Nw+hel3clhQ+DlbPOOks6oP/f/0kVzDCA33+Xc5Uq\nSXVo507PXzt1KpCbK9UkPaUYqHr1yrZqGDJEOssvXgx06SLrsfQ+jpdfHtznERFRuewEqz0ArLdI\nNTl1zoVSqguAtwAMNgzDz302KGSsTTEZrJzTsaN5F2GlSnJ3Xn4+8Oij8vp995X9mt9/B+66Sx7r\nzZSdlpAg/a7OP1/2cfz8c1lbdfvt/m9bREREfrOzxmoZgDZKqXRIoLoRwE3WC5RSzQDMBnCLYRhe\nbkmisCspkb0A33lHGkJ66rZNwaleXfZN+/RTeX7FFRK4JkyQzZyt/bB2nZpRf/99Z6Yky6MUMHhw\n6D+HiIhOK7diZRhGMYB7AMwDsB7ATMMwMpRSdymlTv34jfEAUgH8Vym1Uim1PGQjJnt0v6aXXwbu\nvx/Yvp0Vq1Dp1k328lNKKll33ilVrFtvNTvAAzIFCJTtqk5ERHHD1horwzDmGobR1jCMVoZhTDp1\n7jXDMF479fgOwzDqGIbR9dSv7qEcNJXDMGTx8mefyULqZ56RDZcpNLqf+t+9VSupYDVuLGuvliwx\n1zcBZrDS2+4QEVHcYef1ePTMM3Lr/4ABMhVIodWtmxy7dDHPPf20HPftM8/pYFVR2l4QEVVADFbx\naNYsID1dbrHnnl6h17WrTLP26mWeS04GatUqG6yqVWP1kIgojvFf+HiTny/75I0ZY/ZbotCqVg3Y\nsKHs9jT16wP795vPc3M5DUhEFOcYrOLN0qVyNyAXSIdXYw89cxs0KFuxYrAiIoprnAqMN3/8IUe9\n7ocix1qxWrJENoZmsCIiimsMVvFm5UqgaVNzjziKHB2sDENaMAAMVkREcY7BKt7873+ymJoir0ED\n4MgR2ZR5/Xo5V1gY2TEREVFIMVjFk/nz5Rv4hRdGeiQEAK1by3HsWPPcrl2eryUiorjAYBUvCguB\ne+6RNgsjR0Z6NARIF3YAWLjQbMXAYEVEFNcYrOLFiy9KtWrKFKBq1UiPhgCgbVuzZ9Ujj0hbBt04\nlIiI4hLbLUSzwkIgO9vzrfzu102YAFx5JTBoUHjGRuWrUgVo104WsF92GXDsWKRHREREIcZgFc1u\nvRWYNk2CU+XK3q/LyZHGoAMHhm1oZNPEiUBpqYQsIiKKe5wKDIe8POlhtHu3PH/pJVkPdfiw76+b\nNk2OBQW+r9PvU6dOcOMk5119NTBkSKRHQUREYcKKVTjcfjvwySfyuHJloKhIHrdrB4waVf7Xl3eL\nvg5WtWsHPkYiIiIKGitWoTR7tqx7+uQT4O67gX//W5pFJiQADRsCn39u7310EPOGFSsiIqKowGAV\nKu+8A1x3HfD11/J8+HDgoYdk77iNG6WKtXChbN5bnvIqVkeOyJHBioiIKKI4Feik0lKpSL36qkzx\ndesGrFghr3XqJMfUVPk1ahTwwgvAo4/K1ifVqwPnny+h66GHXN/X7lQggxUREVFEMVg56YEHgOef\nl8eDB8uCdd1Tyr23VL16wIMPAo89Zp778EOpaKWnA9dfb563G6xq1Qpu/ERERBQUTgU6aelSOU6Y\nIOuqkpOlIeTrr3u+/r77JGBp+/ZJ9ervf5fHmp1gVaOG75YMREREFHIMVk4qLAQuvxwYN84MOQ88\nANx5p+fra9QA/vMf13Nz5khPqptvdn1fq5deAhYvNp8fOcJpQCIioijAYOWkw4f9DzjDh8uUodat\nm6y9mj/fPGcNVgcOAKNHA336mOe2b2ewIiIiigIMVk46dAg44wz/v65JE9fnd90lvzRrsPrqK/Nx\nbq7cdbhwIdChg/+fS0RERI5isFqwoPzO5naUlMiUXCDBSu8FWL26eW7KFGDkSHlsDVY//2w+/vFH\nYNkyefzmm/5/LhERETmqYger9euB/v3tN+r05ehRabUQaMWqZ0/XKcHEROC22+SxNVhlZwMdO8od\ngHPmyNRg3bqyXouIiIgiqmK3W1i1So4HDwb/XsH0kkpMBH77rex5vXGvNVjl5Midgx06AHPnSiCz\n3llIREREEVOxK1br1skxLy/49zp0SI6BVKy88RasUlOBgQOBvXuBn35isCIiIooSFStYbdgAHD9u\nPs/IkGMsBqvLLpPnhw9LBYuIiIgiruIEq4MHgfbtZTNkTVes8vODf/9wBKvSUglSqalAgwbSmgFg\nxYqIiChKVJxgpTdDXrBAjgUFwKZN8tiJipVeYxXKYHXkiISr1FR5PnCgHBmsiIiIokLFCVa6/5MO\nK5s2SYsEwNmpQCcbdboHq5wcOepgNWiQHBs1cu4ziYiIKGDxG6wOHABefhnYtk2qPD/9JOe3bAFO\nnjTXV1Wr5lywcnq/Ph2siork6B6sevSQVhFDhzr3mURERBSw+A1W+/cDo0ZJA8316yWUXHGFGbLe\new9ISADOPtu5YOXkNCBQfsVKKWDwYNfGokRERBQx8RusGjaU4969wPffy+NJk6RCdcUVwLx5wDPP\nAGlpzq2xcjpYJSRIeNLBSk836mBFREREUSV+g9UZZ8i03N690tG8a1egSxfpZl67tgSrf/wDSElx\n7q5ApzdCVkqqVt4qVkRERBRV4jdYVaok/Z2WLAGWLgVuuknOv/gikJUFXHyxPE9Jid6pQKBssKpU\nSYIhERERRZ343tKmYUNz0+Ibb5RjQoL80mItWNWpI+GKiIiIok78BysA6NMHaNrU8zUpKdLTqqjI\n/zv6ioqAf/5TNlEOVbCqXNk1WHEakIiIKGpVjGClpwE9qVlTjkePykJ2f6xfD7zyivk8FI063StW\nDFb0/+3dfYwd1XmA8efFNhW1HeGwS2JsbFDBqdxEGGNR0lh1E0KEU4mP1KmwhGKaVggSVFBVKURF\nVZK2SkqrKIkchZAPRa1a0iJQayRC1aCQClraEkI3xYkdQ0HmKzY1juVsAik+/eOcqx1f7de9O/fu\nzO7zk0Yzd+bMzHl37tl995y5cyVJjbWwE6sNG/KnAHfsmLpMp5fpyJHeE6tDh/L8rrvy8OJll/VX\nz+l0J1Zr19Z/DkmSVIuFfbPOTTfB/v3T9/J0tnUeZdCLw4fz/IIL4P3vH8xN5fZYSZLUGgs7sTr1\nVFizZvoynR6rzqMMetHpsRrkd/WZWEmS1BoLO7Gajbn0WB06lIcA635+VdVpp+X7v372MxgfN7GS\nJKnBTKzm2mM1OjrYxx9s2gSPPz7ROzaITx5KkqRamFidfnp+wnm/PVaDHAYE2Lo1Pxn+oYfya3us\nJElqLBOrU07JQ3lz6bEapHe8I8/37MlzEytJkhrLxApystJPj9WRI4NPdNaty49YeOCB/NrESpKk\nxjKxgpysHDgAJ070tt/4OCxfPpg6dUTkXquf/CS/NrGSJKmxTKwArr0WHnsMbrutt/3Gx/MDSAet\nMxwIJlaSJDXYwn7y+mx96EMwNgaf/CS85S2wa9fs9htWYrV1a56fdlqeJElSI9ljBXm4bfduuPhi\n+NSnZrfP66/nZ0sNeigQ4G1vgxUr7K2SJKnhTKw6li2DzZvh5ZdnV/6nP83zYfRYLV0K27blG9kl\nSVJjORRYtWoVvPIKpJR7saYzPp7nw0isAL72NXj11eGcS5Ik9cXEqmrVqjzEd/w4rFw5fdlhJ1Yj\nI8M5jyRJ6ptDgVWd7/x75ZWZy3YefzCMe6wkSVIrmFhV9ZJYDbvHSpIkNZ6JVVXnC467E6t77oFj\nx05eZ2IlSZK6mFhVTdZjdeAA7NgB1103se6FF+Dw4bxsYiVJkgpvXq/qJFbV7w3s3Es1NpbnJ07A\nRRdNJFTeYyVJkgoTq6rJeqyOHs3zznOr9u6Fl16a2G6PlSRJKhwKrFq5EpYsOTmx6ix37ql6+OGT\n9zGxkiRJhYlVVUT+2pjq09e7E6tHHjl5HxMrSZJUmFh1GxmZuDEdJoYCX3stz+2xkiRJUzCx6jY6\nOnmPFcCjj8Izz8DGjRPrli0bWtUkSVKzmVh1Gx09uceqk1gtXw5vf3tevuqqPH/rW4dbN0mS1Ggm\nVt0mS6zOPRduuWVi3c03w9VXw333Db9+kiSpsUysuo2O5udYPfss7NuXl1etgm3bJsqceSbcey+c\nc868VVOSJDWPz7HqNjICKcGmTRM3rl966cQw4Ac/OH91kyRJjWZi1W10NM+PHoXLL4cNG+A974EV\nK+DgwdxbJUmSNAkTq25vfvPE8hVXwI03Trxeu3b49ZEkSa3hPVbdzjtvYnnNmvmrhyRJap1ZJVYR\ncXlE7IuIAxFx6yTbIyI+V7aPRcTm+qs6JGedNbFsYiVJknowY2IVEUuAzwPbgY3AzojY2FVsO3B+\nma4HvlBzPYfnlMqPxMRKkiT1YDY9VhcDB1JKT6eUXgO+DlzZVeZK4K9S9ihwekSsrrmuw+eN6pIk\nqQezuXl9DXCw8vo54FdnUWYN8GK1UERcT+7RYt26db3WdXjuvx++/e2Te68kSZJmMNRPBaaU7gTu\nBNiyZUsa5rl7sn17niRJknowmy6Z54GzK6/XlnW9lpEkSVrQZpNY/SdwfkScGxGnAtcAe7rK7AE+\nUD4deAnw45TSi90HkiRJWshmHApMKf1fRNwE/BOwBPhqSunJiLihbL8DuB94L3AAGAd+Z3BVliRJ\naqZZ3WOVUrqfnDxV191RWU7Ah+utmiRJUrv4sTdJkqSamFhJkiTVxMRKkiSpJiZWkiRJNTGxkiRJ\nqomJlSRJUk1MrCRJkmpiYiVJklQTEytJkqSamFhJkiTVxMRKkiSpJiZWkiRJNTGxkiRJqomJlSRJ\nUk0ipTQ/J444DDw7LycfvhHg5fmuxDxazPEb++Jl/Is3fmNfmNanlEZnKjRvidViEhGPpZS2zHc9\n5stijt/YF2fsYPyLOX5jX5yxdzgUKEmSVBMTK0mSpJqYWA3HnfNdgXm2mOM39sXL+BcvY1/EvMdK\nkiSpJvZYSZIk1cTESpIkqSYmVpOIiLMj4lsRsTcinoyIm8v6N0bEP0fED8t8VVl/Ril/PCJ2dx3r\n1Ii4MyL2R8QPIuK3pjjnRRHxvYg4EBGfi4iobPvtSl3+dpCxl/M1Jv6IWB8RD0bEWEQ8FBFrF2Ds\nfxYRByPieNf6Pyj1GCs/g/WDirucr5bYI2JlRDxRmV6OiM9Mcc6prvsNZf0TEfFwRGwcZOxNi79s\na2W7r+n6t7Ld9xj7gmr3ZdvOcj3HIuKBiBiZ4pyNafcDkVJy6pqA1cDmsrwS2A9sBG4Hbi3rbwX+\nvCwvB7YCNwC7u471ceBPy/IpwMgU5/wP4BIggG8A28v684HvAqvK6zMXWfx3A7vK8ruAv16AsV9S\nznu8a/07gV8syzcCf9eW2LuO+x3g13u87m+olLkCeKBN7/sa4m91u68h/ta2+x5iX1DtHlgKHKL8\nniv7f6zH6z70dj+Qn+l8V6ANE/CPwGXAPmB1Wbca2NdV7rruRgYcBJbPcPzVwA8qr3cCXyzLtwO/\nt4jjfxI4uywHcGwhxd5V/vg02y4EHmlL7JVtG8rPIXq57l3ldgLfGGbs8x1/29t9DfG3tt3PJvau\ncgui3QPLgMPA+nLN7gCu7+W6d5Wbl3Zfx+RQ4Awi4hzym/vfgTellF4sm14C3jTDvqeXxT+JiMcj\n4u6ImGyfNcBzldfPlXWQG+eGiHgkIh6NiMv7i6Q/DYj/v4D3leWrgZURcUavcfRjSLHP1u+S/7Mb\nirnE3uUa8n/caZJt0113IuLDEfEUOcn4/R7OOWcNiL+17b5Lv/G3st13mS722WpNu08p/Zzcw/Y9\n4AVyr9dXJina2HZfFxOraUTECuAe4JaU0rHqttJYZmowS4G1wL+mlDYD/wb8ZY/VWEoeFvgNcgb/\npcof7YFqSPx/CGyLiO8C24Dngdd7PEbPGhJ7py7XAluAv+hn/z7ON9fYq64B7uqnHimlz6eUfgn4\nCHBbP8foR0Pib3O7r+o3/ra2+6q+3/ulLq1q9xGxjJxYXQicBYwBH+21HvPV7utkYjWF8ia5B/ib\nlNK9ZfWPImJ12b6aPJ48nf8FxoHO/ncDmyNiSeXmxk+Qf2lUb85cW9ZBzub3pJR+nlL6H/L49/lz\nDG9GTYk/pfRCSul9KaULgT8q647OPcKpDTn2merybnLcV6SUXu0jnJ7UFHvnWBcAS1NK3ymve3nf\nV30duKqvgHrUoPjb3O47x+o7/ha3+86xZop9pv3b2O43AaSUniqJ2N8Dv9aGdl83E6tJlE8ofAX4\nfkrp05VNe4BdZXkXeSx6SuXNdR/5v06AS4G9KaXXU0qbyvTHpbv1WERcUs79gcqx/6Gzf/mExQbg\n6TmGOK0mxR8RIxHReZ9+FPjq3COc2rBjn6EuFwJfJP9yndUv9LmoK/aKnVT+Y+/xuleTiN8EfthX\nUD1oUvy0uN1XzOX6t7LdV0wb+wx1aWu7fx7YGBGj5fVl5ZiNbvcDkRpwo1fTJvInHhK5K/OJMr0X\nOAN4kHyxvwm8sbLPM8AR4Dj5v82NZf164F/KsR4E1k1xzi3AfwNPAbuZeCp+AJ8G9pLHrq9ZZPHv\nKOfbD3wZ+IUFGPvtZb8TZf6xsv6bwI8q9djTltjLtqeBX57hnFNd98+Sb2B+AvgW8Cttet/XEH+r\n230N8be23fcQ+4Jr9+RPCn6/HOs+4Iwer/vQ2/0gJr/SRpIkqSYOBUqSJNXExEqSJKkmJlaSJEk1\nMbGSJEmqiYmVJElSTUysJEmSamJiJUmSVJP/BzX7ty1+ALpjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8d811fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_prices = get_stock_price(file_csv_name, normalize=True)\n",
    "\n",
    "plt.plot(stock_prices.Close, color='red', label='Close_'+file_csv_name[:-3])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set last day Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock_prices, window_size):\n",
    "    \n",
    "    amount_of_features = len(stock_prices.columns)\n",
    "    data = stock_prices.as_matrix()\n",
    "    sequence_length = window_size + 1\n",
    "    result = []\n",
    "\n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "            result.append(data[index: index + sequence_length]) # index : index + 22 days as window_size \n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day y\n",
    "    y_train = train[:, -1][:,-1] # day y + 1 close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(stock_prices, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433, 22, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buidling LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(shape, neurons, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 389 samples, validate on 44 samples\n",
      "Epoch 1/70\n",
      "1s - loss: 0.0749 - acc: 0.0000e+00 - val_loss: 0.2071 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "1s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1136 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "1s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1570 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "2s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0972 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "2s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.1255 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "2s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0920 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "3s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0818 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "3s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0545 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "3s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0563 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "4s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "4s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "5s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0275 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "5s - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "5s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "6s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0264 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "7s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "7s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "7s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "8s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "9s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "9s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "10s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "11s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "11s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "13s - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "13s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "14s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "14s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "15s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "16s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "17s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "18s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "23s - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "20s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "21s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "23s - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0103 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "25s - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "29s - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "29s - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "36s - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "33s - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "39s - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "46s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "49s - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "42s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "42s - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0063 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "44s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0106 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "53s - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "61s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "48s - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "64s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "58s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0085 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "54s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "67s - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "65s - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "65s - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "67s - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "68s - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "74s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "83s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 8.2381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "63s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "63s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 5.9476e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "66s - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "68s - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "70s - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "71s - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "74s - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 7.0761e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "77s - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "78s - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "81s - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8dd459a58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00107 MSE (0.03 RMSE)\n",
      "Test Score: 0.00723 MSE (0.09 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction vs Actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = percentage_difference(model, X_test, y_test)\n",
    "p_train = percentage_difference(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denormalize(file_csv_name, normalized_value):\n",
    "    df_to_plot = pd.read_csv(file_csv_name)\n",
    "    df_to_plot = df_to_plot.replace('null', 0)\n",
    "    df_to_plot = df_to_plot['Close'].values.reshape(-1,1)\n",
    "    \n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df_to_plot)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(file_csv_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(file_csv_name, normalized_value_p)\n",
    "    newy_test = denormalize(file_csv_name, normalized_value_y_test)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(file_csv_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Adjusted Close')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHwCAYAAADuJ7gwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8a7229e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(file_csv_name, p_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAHwCAYAAAD0G1i+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl81NW9//HXyT5ZyE7YwyqLyO6CIGIriopYa61abatt\ntV5tq7c/a7W3XpfWW9tqa71arbYurV6s2rrWBRUpiigGBGWHAIEQIPs62SZzfn/Md8aZZBISyJ73\n8/HIg5nvcuZMsJf3/ZxzvsdYaxERERGR3i2ipzsgIiIiIkem0CYiIiLSByi0iYiIiPQBCm0iIiIi\nfYBCm4iIiEgfoNAmIiIi0gcotImIiIj0AQptIv2AMeYOY8zTPd2P3soYM9oYY40xUR2450JjzH5j\nTLUxZmZX9k9EpD0U2kT6ACc4+H+8xpjaoPeXd/JnPWmM+WUntNPhoNRdjDErjTHfO8Jl9wI/sNYm\nWms/7aTPPdsYs8oYU2WMKTLG/NsYs9Q5F2OMuc8Yk+/8ve41xtzf7P5vGGNynPMHjTFvGGPmB52f\nYox5xRhT4XzGe8aYU9vRr+OMMS87fSo1xrxljJnonFtkjCk0xmQEXR9rjNlqjLm2M34vItI+Cm0i\nfYATHBKttYnAPuD8oGPP9HT/OksvC3jZwOajudEYExnm2NeA54G/AiOALOC/gfOdS24F5gAnAUnA\nQmB90P0/Bu4H/se5dxTwEOAPfeOA1cDnwBhgGPAisNwYM/cIXU4BXgEmOm2vBV4GsNa+DbwK/CHo\n+p8DB4E/HaFdEelM1lr96Ec/fegH2Auc2ezYHcBz+AJBFb6wMSfo/DDgH0ARsAf4USttXwM0Ag1A\nNfDqke7HFzJygErgMPA75/g+wDrtVANzw3zeHcALwNPO/d/D9/9M3gLkAiXO90pzro9zri0ByoFP\ngKxwvxen7aed16OdvkQBdwNNQJ3Trweb9SnWOW6BGiDXOT4ZWOl87mZgadA9TwIPA6879zT/+zHO\n7+Mnbfy9vgbc2Mq5ZKdPF7dx/9+A18McfxhY1cH/xtKc758e9PkHgPOAqUAZMLaN+68Dtjn/LX4O\nnOAcvw1f2KsEtgKnOX83NUBS0P1znesie/p/b/rRT2/66fEO6Ec/+unYT/Nw4hy7wwkh5wKRwK+A\nj5xzEcA6fFWdGGAssBs4u5X2nwR+GfS+zfuBNcA3ndeJwCnO60BQauO73IEvJH7F+RwXcAPwEb5q\nVCy+as4y5/rv46v6xDvfczYwKNzvhVZCm/N+JfC9I/yeLTDeeR0N7AJ+5vwOvuQEkolBv7MKYJ7z\nPeKatTXJaW9MG5/3c3zB7jrgBMAEnVsMeI7wuzwEXBXm+Bn4QqqrA/+NfQU42OzY+cB+fFW4sOHS\nue6bQB4wE19Ynej8XU53/rvJco6P9f8+gA/9/w057/8XuL+n/7emH/30th8Nj4r0Hx9Ya1+31jbh\nq7pMd46fCGRaa++y1jZYa3cDjwGXtrPdI93fCIw3xmRYa6uttR91sN9rrLUvWWu91tpa4Frgv6y1\n+dbaenzh62vO0GkjkI4vTDVZa9dZays7+HlH4xR8gfQe53ewAl9l7LKga1621q52vkdds/vTnT8P\ntvEZvwJ+DVyOr3J5wBjz7aD7i621njbuz2il/YP4gmRaG/cGGGNG4Bt2/XHwcWvtq/jCdATwQBtN\nfA/4H2vtp9Znu7U2H1/odAFT8FXQdltr9zj3/B/O79IZWv66c0xEgii0ifQfh4Jeu4E4J+hkA8OM\nMeX+H3wVo6x2tnuk+78LHAdsM8Z8YoxZ0sF+7w/zeS8GfdZWfJWiLHxh9C3gWWNMgTHmN8aY6A5+\n3tEYBuy31nqDjuUBw4PeN/8ewUqcP4e2doETQh+y1s7DN8fsbuBxY8xk5/6MI8z5K26l/aGAF9+Q\nZpuMMZnAcuCP1tplYS7ZDGxr9ntobiS+oe0Q1trN+Ia97wYKjTHPGGP8/w09B5zhLHY4E6i01q49\nUn9FBhqFNpH+bz+wx1qbEvSTZK09t5XrbUfut9butNZeBgzGVyl6wRiTEKad1oT7vHOafV6ctfaA\ntbbRWnuntXYKcCqwBPiWc18NvmFTvyEd+MwjKQBGGmOC/2/mKHzzvNrT5nZ83+ui9nyYtbbWWvsQ\nvqA1Bd8QdD2+YcvWvANcHOb41/FVM91tfaYxJhVfYHvFWnt3e/rZiv3AuHAnrLVPWWtPxTc0Ggf8\n0jleCKwCvgZ8A1XZRMJSaBPp/9YCVcaYnxpjXMaYSGPMVGPMia1cfxjfP6rtut8Yc4UxJtOpvpQ7\n93jxLVrwNmurPR4B7jbGZDvtZxpjLnBen2GMOcEZQqvEN1zqr/psAC41xkQbY+bgCwCtaf4dj+Rj\nfNXLm532F+Kb4/Vse2621lp8w423GWOuMsYMMsZEGGPmG2Medb7bjcaYhc7vOMoZGk0CPrXWVuCb\nU/iQMeYrxph4px/nGGN+43zMncCpxpi7jTFpxpgkY8wP8YXan7bVP2PMIHwVzNXW2ls68HsJ58/A\nLcaY6cbnOGPMCOdxJKcbY2KBWucnuGL3f8BV+IKpQptIGAptIv2cM8dtCTAD38rPYnz/sCa3cstf\ngCnO8ORL7bh/MbDZGFON77EQlzqVIje+obDVTluntLPLf8D3+InlxpgqfPOoTnbODcG32tS/+vDf\n+IZMwbcycRy+6tSdtP0P/x/wzZMrM8a0NT8LAGttA76Qdg6+7/9H4FvW2m3t/E5Ya18ALgG+g69y\ndxhfpell5xI3cB++Ye5i4HrgImcOIdba+/AFv5/jC8T7gR8ALznndwLz8c1l3ItvLttF+BaMrD5C\n9y7EN3fxKhP6TMBRR/pezjPbqv0h3lr7N+B3+P6eqpw/U/DNZ7vP+W4H8c0RvC2oqX8C04Bd1trt\nR/pckYHI+P4fQBERERHpzVRpExEREekDFNpERAYAY8zlzYY+/T9HteuDiHQ/DY+KiIiI9AGqtImI\niIj0Ab1pc+ZjlpGRYUePHt3T3RARERE5onXr1hVbazPbe32/Cm2jR48mJyenp7shIiIickTGmLyO\nXK/hUREREZE+QKFNREREpA9QaBMRERHpA/rVnLZwGhsbyc/Pp66urqe7Ip0kLi6OESNGEB0d3dNd\nERER6Tb9PrTl5+eTlJTE6NGjMcb0dHfkGFlrKSkpIT8/nzFjxvR0d0RERLpNvx8eraurIz09XYGt\nnzDGkJ6ersqpiIgMOP0+tAEKbP2M/j5FRGQgGhChTURERKSvU2jrJi+99BLGGLZt29bmdU8++SQF\nBQVH/TkrV65kyZIlR31/sFNPPbVT2hEREZFjp9DWTZYtW8b8+fNZtmxZm9cda2jrDB6PB4APP/yw\nR/shIiIiX+j3q0eD3fjmm2w4dKhT25wxZAj3L17c5jXV1dV88MEHvPfee5x//vnceeedAPz617/m\n6aefJiIignPOOYc5c+aQk5PD5ZdfjsvlYs2aNUyePJmcnBwyMjLIycnhpptuYuXKlaxdu5YbbriB\nuro6XC4XTzzxBBMnTjxif++44w5yc3PZtWsXxcXF3HzzzVx99dWsXLmS2267jdTUVLZt28aOHTtI\nTEykuro6bF/vuececnNzuf766ykqKiI+Pp7HHnuMSZMm8fzzz3PnnXcSGRlJcnIyq1atOvZftIiI\nyAA3oEJbT3n55ZdZvHgxxx13HOnp6axbt47CwkJefvllPv74Y+Lj4yktLSUtLY0HH3yQe++9lzlz\n5rTZ5qRJk3j//feJiorinXfe4Wc/+xn/+Mc/2tWfzz77jI8++oiamhpmzpzJeeedB8D69evZtGlT\ni0dpvPHGGy36CnDNNdfwyCOPMGHCBD7++GOuu+46VqxYwV133cVbb73F8OHDKS8vP4rfmIiIiDQ3\noELbkSpiXWXZsmXccMMNAFx66aUsW7YMay1XXXUV8fHxAKSlpXWozYqKCr797W+zc+dOjDE0Nja2\n+94LLrgAl8uFy+XijDPOYO3ataSkpHDSSSeFffbZO++806Kv1dXVfPjhh1x88cWB6+rr6wGYN28e\nV155JV//+tf56le/2qHvJSIiIuENqNDWE0pLS1mxYgWff/45xhiampowxoSEnbZERUXh9XoBQp5N\ndtttt3HGGWfw4osvsnfvXhYuXNjuPjV/ZIb/fUJCQrvb8Hq9pKSksGHDhhbnHnnkET7++GP+9a9/\nMXv2bNatW0d6enq72xYREZGWtBChi73wwgt885vfJC8vj71797J//37GjBlDcnIyTzzxBG63GyAw\n5JiUlERVVVXg/tGjR7Nu3TqAkOHPiooKhg8fDvgWL3TEyy+/TF1dHSUlJaxcuZITTzyxzesXLVrU\noq+DBg1izJgxPP/884Bvp4KNGzcCkJuby8knn8xdd91FZmYm+/fv71D/REREpCWFti62bNkyLrzw\nwpBjF110EQcPHmTp0qXMmTOHGTNmcO+99wJw5ZVXcu211zJjxgxqa2u5/fbbueGGG5gzZw6RkZGB\nNm6++WZuvfVWZs6cGVjt2V7Tpk3jjDPO4JRTTuG2225j2LBhbV6/ePHisH195pln+Mtf/sL06dM5\n/vjjefnllwH4yU9+wgknnMDUqVM59dRTmT59eof6JyIiIi0Za21P96HTzJkzx+bk5IQc27p1K5Mn\nT+6hHvU+d9xxB4mJidx000093ZVjor9XEZGBy1rbL3bHMcass9a2vfIwiCptIiIi0qdc8OyzXPva\naz3djW6nhQj91BNPPMEf/vCHkGPz5s3joYce6qEeiYiIHLuy2lr+tXMnUzIze7or3U6hrZ+66qqr\nuOqqq3q6GyIiIp3q3T178FrLnrKyfjNM2l4aHhUREZFeoT3z7N/ctQuAmsZGip2nGgwUCm0iIiLS\n4/65dSsRd93F7rKyNq/7cP9+4qOjAdhTXs6WoiLe3b27O7rY4xTaREREpMe9vH07wBEDWHldHdOy\nsgB4bvNmjv/jHznzb3/r8v71BgptIiIi0uOyk5MByKuoaPO6qoYGThg8GID71qwJHB8IQ6UKbd0g\nMjKSGTNmMHXqVC6++OLAzgJHY+XKlSxZsgSAV155hXvuuafVa8vLy/njH//Y4c+44447Ag/QPRY5\nOTn86Ec/OuZ2RESk/0twhjzbCm1ea6luaGBoYmLg2IWTJgGwtaioazvYC3R5aDPGRBpjPjXGvOa8\nv8MYc8AYs8H5ObeV+xYbY7YbY3YZY27p6n52JZfLxYYNG9i0aRMxMTE88sgjIeettYH9RTti6dKl\n3HJL67+aow1tncHj8TBnzhweeOCBHvl8ERHpW9yNjQDsLS8/4jWJMTGBY7844wwAthUXd2Hveofu\neOTHDcBWYFDQsd9ba1st5RhjIoGHgEVAPvCJMeYVa+2WY+rJjTdCmA3Oj8mMGXD//e2+/LTTTuOz\nzz5j7969nH322Zx88smsW7eO119/ne3bt3P77bdTX1/PuHHjeOKJJ0hMTOTNN9/kxhtvJD4+nvnz\n5wfaevLJJ8nJyeHBBx/k8OHDXHvttex25gI8/PDDPPDAA+Tm5jJjxgwWLVrEb3/7W37729/y3HPP\nUV9fz4UXXsidd94JwN13381TTz3F4MGDGTlyJLNnz271OyxcuJDp06fz73//G4/Hw+OPP85JJ53E\nHXfcQW5uLrt372bUqFF8//vf59577+W1116jurqaH/7wh+Tk5GCM4fbbb+eiiy5i+fLlYb/zLbfc\nwiuvvEJUVBRnnXVWp1T+RESk96p1tmTc3kb4qqqvByApNpblV1xBncfD5MxMXFFRbFVoOzbGmBHA\necDdwI87cOtJwC5r7W6nnWeBC4BjC209zOPx8MYbb7B48WIAdu7cyVNPPcUpp5xCcXExv/zlL3nn\nnXdISEjg17/+Nb/73e+4+eabufrqq1mxYgXjx4/nkksuCdv2j370I04//XRefPFFmpqaqK6u5p57\n7mHTpk1scILq8uXL2blzJ2vXrsVay9KlS1m1ahUJCQk8++yzbNiwAY/Hw6xZs9oMbQBut5sNGzaw\natUqvvOd77Bp0yYAtmzZwgcffIDL5WLlypWB63/xi1+QnJzM559/DkBZWVmr3/n666/nxRdfZNu2\nbRhjKG/j/+sSEZH+wV9FK3K7KXG7SY+Pb3FNdUMDAEkxMSwaNy5wfGJGhiptneB+4GYgqdnxHxpj\nvgXkAP/PWtt8fe9wYH/Q+3zg5GPvTfsrYp2ptraWGTNmAL5K23e/+10KCgrIzs7mlFNOAeCjjz5i\ny5YtzJs3D4CGhgbmzp3Ltm3bGDNmDBMmTADgiiuu4NFHH23xGStWrOCvf/0r4JtDl5ycTFmzZdPL\nly9n+fLlzJw5E4Dq6mp27txJVVUVF154IfHO/0CWLl16xO902WWXAbBgwQIqKysDwWrp0qW4XK4W\n17/zzjs8++yzgfepqam89tprYb9zcnIycXFxfPe732XJkiWBOXwiItJ/1TqhDWBzURELsrNbXFPl\nhLbg4VGASRkZfJSf37Ud7AW6LLQZY5YAhdbadcaYhUGnHgZ+AVjnz/uA7xzD51wDXAMwatSoo+5v\nV/LPaWsuISEh8Npay6JFi1i2bFnINeHuO1rWWm699Va+//3vhxy//yjCbPMnUPvfB3+n9vQn3HcG\nWLt2Le+++y4vvPACDz74ICtWrOhwH0VEpO9wezwkxcRQ1dDApwcPhg1tgUpbbGzI8ezkZP6xZUu/\n3yGhKxcizAOWGmP2As8CXzLGPG2tPWytbbLWeoHH8A2FNncAGBn0foRzrAVr7aPW2jnW2jmZfXgf\nslNOOYXVq1ezy/+k55oaduzYwaRJk9i7dy+5ubkAYQMOwJe//GUefvhhAJqamqioqCApKYmqqqrA\nNWeffTaPP/441dXVABw4cIDCwkIWLFjASy+9RG1tLVVVVbz66qtH7O/f//53AD744AOSk5NJdpZq\nt2bRokUh+56WlZW1+p2rq6upqKjg3HPP5fe//z0bN248Yn9ERKRvq21sZExqKkMTE1l/6FDYa/xz\n2ppX2lLj4mj0egNDrP1Vl4U2a+2t1toR1trRwKXACmvtFcaYoUGXXQhsCnP7J8AEY8wYY0yMc/8r\nXdXX3iAzM5Mnn3ySyy67jGnTpgWGRuPi4nj00Uc577zzmDVrFoOdZ9M094c//IH33nuPE044gdmz\nZ7NlyxbS09OZN28eU6dO5Sc/+QlnnXUW3/jGN5g7dy4nnHACX/va16iqqmLWrFlccsklTJ8+nXPO\nOYcTTzzxiP2Ni4tj5syZXHvttfzlL3854vU///nPKSsrY+rUqUyfPp333nuv1e9cVVXFkiVLmDZt\nGvPnz+d3v/tdh3+fIiLSt7gbG4mPjmbW0KGsP3gw7DXBc9qCpTnTckpra7u2kz3MtGefr2P+EN/w\n6E3W2iXGmL8BM/ANj+4Fvm+tPWiMGQb82Vp7rnPPufjmxEUCj1tr7z7S58yZM8fm5OSEHNu6dSuT\nJ0/uzK8z4C1cuJB7772XOXPm9Fgf9PcqItK/nPbEE0RHRDBv5Eh+9cEHVN16Ky7n2W1+f16/nqtf\nfZV9N97IyKARnhe2bOHi559n47XXBnZL6AuMMeuste3+x7Q7HvmBtXYlsNJ5/c1WrikAzg16/zrw\nejd0T0RERHqYu7GRoYmJzBo6lCZr2VRYyInDh4dc09qcNn+lrayfV9q6JbRJ33T99dezevXqkGM3\n3HBDyKM8REREOkNtYyOu6GiGD/I91rWwpqbFNW3NaYP+Pzyq0CatCl44ICIi0pX8c9r889X8j/cI\nVtXQQFxUFFERoVPyA5W2urqu72gPGhB7j3bHvD3pPvr7FBHpf2o9HlxRUQxyhj79VbVg1Q0NLaps\nAKkDZCFCvw9tcXFxlJSU6B/6fsJaS0lJCXFOKVxERPqHQKXNH9paqbQ1XzkKvtWkkcZoTltfN2LE\nCPLz8ykqKurprkgniYuLY8SIET3dDRER6STWWt+ctqioQCWt0qm0/W3jRs6ZMIGM+PhWK23GGFJd\nrn5faev3oS06OpoxY8b0dDdERESkFY1eL03WEh8dTYQxJERHU1VfT0FVFd966SX+e8EC7jzjDKrq\n61usHPVLc7k0p01ERESkK/n3HfU/ly0pNpaqhgb2VVQA8NEB36ZIrVXawLeCtL9X2hTaREREpEf5\nt5+Kd0LbICe05VdWAvBxfj5ea1ud0wa+SltBVRUf7NvXPZ3uAQptIiIi0q0+PXiQ13fuDLz3hzZX\nlG/WVlJMDJX19YHQVlFfz/bi4jaHR1NdLjYXFXHaE0/w2Lp1XfwNeoZCm4iIiHSrn61YwTdffDHw\nZIdajwf4otKWFBtLVVBoA/jrxo3sr6xkckZG2DbLnflsCdHRXPf66/1yJalCm4iIiHQbay05BQWU\n1tYGQlnz4dGkmJjA8Oi41FSOS0/nntWrMcBlU6eGbXfmkCEA3H766Xi8Xord7q7/Mt1MoU1ERES6\nzb6KikCg2nDoENDKQgSn0jYqOZlfn3kmAAtHjw7ZKD7Y7aefzuGbbmJ8WhoANU6b/YlCm4iIiHSb\nnIKCwGt/aGuxECGo0jZi0CAumDiRO04/nV9+6UutthsdGcnghIRAG+5+GNr6/XPaREREpPfIKSgg\nKiKCYUlJbDx8GPhiTltgIUJsLOV1dZRay4hBgzDGcPvChe1qP8FZXdofQ5sqbSIiItJtthQXMykj\ng5OHD2+10pYUE0NDUxMer5fhSUntatd6vVQVlgTaqGlooNjtZt7jj7Pfed5bX6fQJiIiIt3G3dhI\nUkwM2cnJFFRVAeHntPlNzsxsV7trz70Umz2apvwvguCWoiI+3L8/EA77OoU2ERER6TYNTU3EREaS\n6nJR6/FQ7/EEFg0EV9r8/KtC2/L5k//g5LeeZ1BdNbW33wP4FiLUOcOu/WWoVKFNREREuk29x0Ns\nVBSpcXEAlNXVUd3QABDYompQUKUt1eU6Ypve+3/P4eRMPjntPE5e/k9SamtxB4U2/5y5vk6hTURE\nRLpNcKUNoLS2luqGBmIiI4mJjAS+GB4dmph4xPaqi8uYtGkte05fTNKPf0Rsk4ezd+0KDW2qtImI\niIh0TH1TE7GRkaQ5oa3MCW3BG8H7d0qYkJ5+xPZ2PPUCsU2NJF36NSac9yVKE5JZsmMHNQ0N1Gt4\nVEREROToBCptzYZHg+exHT94MAC3zp9/xPY8r/2LclcSE7+6mMjoKHJnn8a5O3fhrq3T8KiIiIjI\n0QrMaQuqtFU1q7SNSk7G3n47i8ePP2J7g7dtJO+4aUTF+u6POO9c0upqiVm/UcOjIiIiIkeroamJ\nmIiIQKWtNMzwaHvVlJQz8vA+3NNnBY6Nufh8AEZ9kqPVoyIiIiJHq76pidioKFKaDY8eTWjLe2c1\nkdZL/NyTAsfSxoxg45BhTNu8ScOjIiIiIkfLP6ctMiKC5NjYwEKE4Afqtlfl+x8CMHzRgpDja46b\nyOy83dRVVQOqtImIiIh0WL3HQ6zzaI9Ul+uYKm1Rn67nUHImGeNGhRzfMmUKcU0eYjZuAlRpExER\nEemQJq+XJmsDz2NLjYujrK6Oqvp6Ep3dEDpiyPbPKZgwtcXxkrFjARi0MxfQQgQRERGRDmloagIg\nNioK8FXajnYhQsWBQkaUHKB+xqwW5xpHDqcmOpqsvL2AhkdFREREOsQf2vyVtjSXi2K3m1qPp8Oh\nbd/b7wOQOG9ui3PxcbFszxzMqPx8QMOjIiIiIh1S76+0BQ2P5ldWAoQsRPDUN/DR9T9jzTU/oeJA\nYdi2qlevAWDUotNanEuIjmbL4MGMO3wA6Fil7d4PP2TWn/7U7uu7U1RPd0BEREQGhuaVtnSXKxCo\ngittnz30V075468AWFNby9y/PdiirdgN68lPH86I4YNbnIuPjmZLZiZXbPiUlNraDs1pyysvJ6+i\nov1fqhup0iYiIiLdwr8XqH9OW3ZKSuBcyPDos8soSUhh88TZjHr9n1ivt0Vbw3Zu4vBxLRchgC+0\nbczMAGBKUVGHhkcrGxoYdBSPH+kOCm0iIiLSLZpX2salpgbO+UNbVWEJx69/n12nn4P7sisYXnqQ\nbf94M6Sd4tx9DKkoonHW7LCfkxATw860NADGl5Z2aHi0oq5OoU1EREQGtuZz2sY5wQoIbBi/7eG/\nEdvUSPJ3v8WUH1xJdUw8NX8IHR498PYqAAbNPyXs58RHR7M3JQWPiWB8aWmHhkcr6+sV2kRERGRg\na15pG5WcHDjnr7TFvvAcBalDmPiVs0hIT2HT2V9l+prlFG3fHbjWvWYtTSaC7DCLEMC3EKExKoq8\nlGRfaPN4sNa2q4+V9fUkK7SJiIjIQNZ8TltUxBcxJDEmhtI9+UzZ/DF5i87HOOdG/vfNRFjLviv/\nA6/HF/riN6xjf9YoEtJTCCfBCYC70tIYX1oKENiH9EhUaRMREZEBr3mlLVhiTAw77v49UdbLkP/4\nbuD48DknsPZ7P2b2R8v5+Js/oHjHHiZvWsuhk8JX2cD3/DfwhbYJJSVgbbsXI/Tm0KZHfoiIiEi3\nqG8jtEVUu5m47M9snD6P6QtPDjl3yiO/Zu2ePcx99hF2friSNG8TI/7rplY/JyM+HoCdaWmk1NeT\n7nbjbmwMhLm2VPTi0KZKm4iIiHSLhmYLEQDuO+ssAPZ/50ekuiuJveO/W9xnIiKY+eozfLJgCWP3\n72DjrNMZcdK0Vj/HH9p2OQsdJjiLEd7bs4fK+vo2+1fn8Si0iYiISP9xoLKSB9eu7dA94YZHfzx3\nLuvTsjll+Qus+frVTPrKWWHvjY6L5cR/v0p57l4mvftKm5+T7lTUtmRmAjC1sJDCmhrO/NvfeGrD\nhlbvq3ICnRYiiIiISL/xp3Xr+OEbb3Courrd9zRfiODX8PobVMXGc9LTfzxiG+ljRuJKSWrzGld0\nNLGRkexJTaU8No5ZBw9yuKYGr7VtVtr851RpExERkX7j80LfnqCFNTXtvqe1hQjpmzaQN3YKkdGd\nN9U+KTYWjGHz8JHMPHiQYrcb+GJeXTgKbSIiItLvbHJCW1EHQlvzh+sC1FVWk12QS9W0WZ3aP/9z\n33KzxzAR22ZFAAAgAElEQVT98GGKyn37ida3sYq0QqFNRERE+pOahgZyneefHWulbe87q4n2NhF3\navjdDY6WP7QVHjcRl8eD+/MtgCptbTLGRBpjPjXGvOa8/60xZpsx5jNjzIvGmLBPxjPG7DXGfG6M\n2WCMyenqfoqIiEj7bC0uxr+/QJEz7Nge4ea0la/6EIDhixZ0Wv/AtysCQP3M6QAkrf80pA/h+ENb\nclxcp/als3RHpe0GYGvQ+7eBqdbaacAO4NY27j3DWjvDWjunKzsoIiIi7ff54cOB18c8p23/fuqi\nYsicOKbT+gdfVNpSZkylIDGRKRt9q0ZVaWuFMWYEcB7wZ/8xa+1ya60/5n4EjOjKPoiIiEjn2lVa\nSqQxZMTHd3hOW4QxIdtXRRYVUp6YEti2qrMEQpvLxTtjxzJ313aM19tqaNtaVMTL27cDAzS0AfcD\nNwPeVs5/B3ijlXMWeMcYs84Yc01rH2CMucYYk2OMySkqKjq23oqIiMgRFbvdpMfHMyQxkcIODI82\nNDW1WDkaW1pM5aD0zu5iILR5vF7eHT+eTHcNMw4dClT7mvva88/z5q5dALiieueGUV0W2owxS4BC\na+26Vs7/F+ABnmmlifnW2hnAOcD1xpiwg93W2kettXOstXMynYfoiYiISNcpqa0lIz6ezI5W2jye\nkJWjAAllJbhTuy601TQ28vHEiQCctm9fq3Pa/HPgAIwxnd6fztCVlbZ5wFJjzF7gWeBLxpinAYwx\nVwJLgMuttTbczdbaA86fhcCLwEld2FcRERFpp2K3m3SXi8EJCR2e09a80pZcWUpDeucXXW485RSy\nEhK4YOJE3JmZHE5IYNrhw60OjyY5Q6K/OfPMTu9LZ+my0GatvdVaO8JaOxq4FFhhrb3CGLMY35Dp\nUmtt2JqqMSbBGJPkfw2cBWzqqr6KiIhI+4VU2jqyerSpKWTlqNfTREpNOU2DB3d6HydlZHDoppsY\nPmgQSbGxfJaV5QttrVTa3I2NLBo7lp/Mm9fpfeksPfGctgeBJOBt53EejwAYY4YZY153rskCPjDG\nbATWAv+y1r7ZA30VERGRZoIrbeV1da3OE2uueaWt4sBhoqwXk5XVVV0FICkmhs+ysphaWEhDQ0PY\na9yNjcQHDZH2Rt0y085auxJY6bwe38o1BcC5zuvdwPTu6JuIiIi0n7WWErfbV2lLSAB8uyIMHzTo\niPfWNzWFzGmr2JtPKhA1dEhXdRfwzW/7LCsLl8dDan5B2Gtq+0Bo044IIiIi0i6Hqqv5YN8+Gr1e\n0uPjyYiPB6C0trZd9zevtFXvOwBA3PChnd/ZIP7hUYDsvXvDXuNubOy1q0b9enfvREREpFdwNzYy\n9L77Au8z4uNJc7mA9oe2eo8nZE5bnVP1Sho1vBN72lJSTAxbMjNpiIjguH37w17TF4ZHVWkTERGR\nI/rlqlUh748mtDWvtHkO+XZWSB7dtc/ZT4yJoSEqis+zspi+f1/Ya2o9HlwKbSIiItLXrcnPD3mf\n7nIFQltJO0NbTbNqli0sxGMiGDSs81ePBktyntmWM2wYMw/kY72hz/z3Wkudx6NKm4iIiPR9+yoq\nWHLccYH3GfHxpHew0lZRV0dy0BZREeVlVLkSiYiKbOOuY+d/Btsnw4aRUl9H/iefh5yvbWwEUGgT\nERGRvs1rLfmVlUzJyAgcS4+PJz46mpjIyPaHtvr6kNAWWVWFOy6h0/vbnH93hE+G++bOHV7xfsj5\nWufZbb19IYJCm4iIiLSpqKaGhqYmRiYnB46lxMVhjCHN5epYpS0uLvA+qrqK2vjETu9vc/7h0S2Z\nmdRFRtK4NifkvLuPVNp6d6QUERGRHrevogKAUcnJvHn55SzPzSXC2Z8zzeVq15y2xqYmaj2ekEpb\nTE0V9fFJXdPpIP7hUU9kJNsyMojftT3kfF8ZHlVoExERkTbtr6wEYOSgQcwcOpSzx3/xnPz0dlba\nKurrAUIqbXHuaiqGdO3KUfii0hZhDJsHD+bL+3IBuPj55xmfmsrXjz8eQKtHRUREpG/b71TagodH\n/do7PFpRVwcQUmlzuavxJHZ9pc0/py0lLo7NmZkMqSiiqrCED/fvZ+Phw31meFShTURERNq0r6IC\nV1RUYLVosDSXi4KqKv74ySd4mj1KI1i4SltCXQ3epCNvf3Ws/MOjqXFxbHY2pz/w4TqKamqobmgI\nLERQaBMREZE+bX9lJSOTkzHOPLZgaS4XxW4317/+Oq/t2NFqG/5KW4oT2qzXS0K9G2+Y6l1n8w+P\nprlcbM7MBKB47XoavV6qGxoClTatHhUREZE+bU95OdmthKvg6pvX2lbbCFTanKpXTWkFkdaL6YbQ\nNio5mUunTuXscePYk5pKXVQMnk2+Z7UFhzZV2kRERKRPyy0tZVxqathzURFfRImyNua2Bea0OZW2\nmsJiACJSUjqrm62Kjoxk2UUXMWPIELwREewbPILEXN8K0uqGhj6zelShTURERFpVVltLWV0d49LS\nwp7PiI//4lonmIXTvNJWW1QGQGRq11fa/Pyb1R8eNYbh+XuB0EqbVo+KiIhIn7W7zBeuWqu0fWv6\ndHKuvpoIYyhvK7Q55wY5oa2upBSA6LTw7XaFWGez+srx4xlaWUJSXR3VDQ3UqNImIiIifV2uP7S1\nUmnb/ty/GH3c8dy3/G3KKqtabaeivp746GiineBUX+JrNya164dH/fyVtvqJvj1UpxQVYfli71Qt\nRBAREZE+K7fUVxEb20qlrfFX95BYV8ONqz9gyGtvtNpO883iG0vLAXBlhA+DXcFfafttjW8+3fFF\nRYBvm66YyEgiI3p3LOrdvRMREZEeU1Vfz7qDB8lKSAg8oDZY/trPOOGzNay/7BryUlKZ9/77YVrx\nqaivD3lGW5NTwXNldmNocyppH8dE446K4vjCQgAK3e5ePzQKCm0iIiISRmNTE3Mee4x/bN3aapVt\n/8OPAzD+5z/mzVknsWDnNopz94W9tqK+PqTSZst9uywkDE7v5J63LsaptNmICLZnZDCp2FdxK6qp\nUWgTERGRvumvGzeyo6SE6088kd8sWhT2mvT3lrNzzBQyJ45l45cWEmW97H7q+bDXVtTVhVTabEUF\nHhNBfGrX74jg5x8eBdiens7EkhIACmtqev18NlBoExERkTB+99FHnDhsGP97zjnMHzWqxfni3H0c\nl7eV0tN9ga5p6mSKXfGYVf8O215pbS2pQaEtorKCmth4TDfOI4sNCmbbMzIYXV5OjMdDkYZHRURE\npK/aU1bGguzssFtXAYGK2uDLvgpAakICq0ZnM/yzT8JeX+R2kxn0TLfIyipqXImd3Ou2BVfaZp+6\niEhrGVdaSnVDQ2B/0t5MoU1ERERCuBsbqfV4Qh6c21zUW29QmJTO2DPnAb7N2N8bPZphZYco2LA1\n5NrGpibK6+pC2ouqrqS2u0NbUKVt4rxTfX86Q6QjB3XfMO3RUmgTERGRECVuNxC6r2iwxrp6Jmxc\nw94TFwSGN1NdLlZlZwNQ8MaK0Pac56BlJiQEjsXUVFGf0HOVtiEnTgNgorMYQaFNRERE+hx/yGqt\n0rbzpeUk1buJPv+8wLHUuDh2Og/gbdyVG3J9sRMCg4dHY2uqaUhI6tR+H4l/9ejUwYNJGpxOUWJq\nYAXpyG7YuP5Y9f6lEiIiItKt/CErvZXQVvHPV2iMiGT8N74SOJbqclEbE0NRYgqReXkh1xfV1ACh\nIdBVW01ZYvdWt4wxfPid7zApIwOAA2OnMG+/byh3lBPaqgpLiE1MICY+rtV2eooqbSIiIhJQWV9P\nYZiQFSxr9XvsGD+dpKBnrPn3FD2UnkV8Qeiz2gKVtqDh0YTaapoGdW+lDWDuyJGkOsO+dWd8mQml\npYwpLQ0Mj276z9vwpKRSX+Pu9r4diUKbiIiIANDk9TLugQe469++x3aEC22Ht+xkbEEuVV8KfXab\nf75YyeAsUg8fCDlX5IQ2f3vW6yWh3o13UM8OSQ675AIAzsrNDQyPJn36CQVZI4lNaH0RRk9RaBMR\nERHAt2tBsdvNdmdFZVqYhQh5z/wTgKxLLgw57p8vVjFkKIPLC2lq9ATO+YdH/Qsb3GWVRFkvpofn\nkY08eQZ7k5NZvGsXmfHxNDV6GJ27hZKpM3u0X61RaBMREREAypwFCAApcXFEhXnwbcTKlRQlpTF6\nwYkhx/2hrXrEcKK9TRRt2x04V+x2kxIXR7RzTU2hLxSalJ4NbSYiglcnTuSs3FxqyyvZt3odiQ1u\nIk49tUf71RqFNhEREQGgrK4u8Lq1x30M3/Ip+yfParGTgT+01Y4YBkDp5u2Bc80frFtbVApAZCt7\nmnanf06eTLzHw/anXqBo+UoAhp59Rs92qhUKbSIiIgJAeVBoCzefrXBrLkPLD9Nw8iktzvlDW332\nCABqtu8KnCt2u0MWIdQW+0JbdFpK53T8GDz7+/spcw3C+9zzRL/1JqUJyQyfM7WnuxWWQpuIiIgA\nocOj4R73sf9f7wKQdlbLSpQ/tDUOGQyA50BB4FyR2x0SAhtKywCITe/5SltWagrbF13AjI/eZvr6\nf7P9/Eu7dT/UjuidvRIREZFuV3aESlvjqvepi4phjLN1VbBAaIuLoTomHnPo0Bft1taGLGpodEJb\nXHpap/X9WEx+5D7KElJojIxiwi9/1tPdaZUerisiIiLAF5W2NJeLsSkthy6Hrn2fHRNnMi2u5ebq\n/tDW0NRE2aA0oouLAueqGxpIiokJvPeUVQDgyuz5ShtA8tBMdr/0Kgf3H+CEcaN6ujutUmgTERER\nwFdpi46IYMt115EcF7ojwMHPtpF9OI+PLr4i7L3+laYNTU1UJ6cRVxIa2hKDQpstLwcgYXBGZ3+F\nozb2y71zxWgwDY+KiIgI4FuIkOpykZWYSFxUaF1n3//5ns829JKvhLsVYwwxkZE0NDVRm55JYrlv\nT896j4dGr7dZaKugyUSQkNb79/vsTRTaREREBPBV2lLjwu+5GfP22xxMyWLUqbNavd8f2hozMkmp\n9K0QrW5oAAgJbRGVFdTExvfaCf+9lX5bIiIiAvjmtKWECW0N7jqO2/Qx+05a0GbQ8oe2+oxMkuuq\n+eP7H4QPbVWV1MQltNaMtEKhTURERACn0hbmobo7X3yLhIZaYs47p837/aGtING3UOHdjz8OhLbg\nhQjRVZW447t/s/i+TqFNREREAGdOW5hKW+VLr9IYEcmEb4Sfz+YXExlJg9fL1mjf+2leE7bSFlNd\nSV2CQltHKbSJiIgI4BseDRfaMtauZufYqSRmtP2IjpjISOo8Hj70+J73FlNUEja0uWqqaEjSIoSO\n6vLQZoyJNMZ8aox5zXmfZox52xiz0/kz7H8BxpjFxpjtxphdxphburqfIiIiA5m1lvK6uhZz2txl\nFYzZv4OK2ScfsY2YyEjyysvZHu1beRpbXBw2tMW7q/AMGtSJvR8YuqPSdgOwNej9LcC71toJwLvO\n+xDGmEjgIeAcYApwmTFmSjf0VUREZEAqq6ujydqQnQsA9rz5b6KsF9fp84/YRkxkJOV1dRxOTMQL\nJBUWhg1tibXVNA1Spa2jujS0GWNGAOcBfw46fAHwlPP6KSDcAPlJwC5r7W5rbQPwrHOfiIiIdIEP\n9u0DYPawYSHHK997H4Dsc798xDb8oa0hKooDgwaRWXioRWjz1DeQ0FCLTekduyH0JV1dabsfuBnw\nBh3LstYedF4fArLC3Dcc2B/0Pt85JiIiIl3gnd27iY+OZu6IESHHXZ98zL7MkaRmD2vlzi/EREZS\nUV8PQG5qKsOLWlbaqg+XAGBSW26TJW3rstBmjFkCFFpr17V2jbXWAvYYP+caY0yOMSanqKjoyDeI\niIgIAJX19Vzz6quUuN28s3s3C7KziQ3aCcF6vWTv2Mih41t/oG6wmMhI3I2NAOSmpTGquCgQ2uKj\nfUtKqw/5/q2OTFWlraO6cu/RecBSY8y5QBwwyBjzNHDYGDPUWnvQGDMUKAxz7wFgZND7Ec6xFqy1\njwKPAsyZM+eYAqCIiMhAsiovj8fWr2d0Sgpbi4v5zsyZIefz125kpLsS5s5tV3v+TeMB9qSlkVVT\nRU15BfHR0UQ6D+WtLfJV2qIz0jvpWwwcXVZps9beaq0dYa0dDVwKrLDWXgG8AnzbuezbwMthbv8E\nmGCMGWOMiXHuf6Wr+ioiIjIQ7XU2bn/ok08AOD07O+T8wTfeA2Dw2We0q73g0HZwsG/2k83dE7II\noa7It71VTEbaUfZ64OqJ57TdAywyxuwEznTeY4wZZox5HcBa6wF+ALyFb+Xpc9bazT3QVxERkX4r\nzwltBVVVuKKimDFkSMh5u2YNlbEJjJo3u13tBYe2kqG+OXCuvftCQltDsa/S5hqs0NZRXTk8GmCt\nXQmsdF6XAC2WoFhrC4Bzg96/DrzeHf0TEREZiPZWVARenzR8ONFBoQtg8Ofr2DthGtOiIpvfGlZw\naHOP8i1oSDlwgMTjjwscbyotAyA+K+Oo+z1QaUcEERGRASqvvJwIYwCYN3JkyLmKg0VkH9pLzZyT\n2t1ecGiLHTyYEpeLIQcLQipt3jJfaEvMyjyWrg9ICm0iIiID1N7ycs4ZP55hSUksnTgx5Fze6yuI\nwJJ4+mntbi8m4otYkeZysT1zMKMPhYY2W1aGx0QQn6odETqqW4ZHRUREpHepaWigyO3m1JEjee0b\n32h5fuUHNJkIRp+7sN1t+ittBkiJjWVbZibnb90aEtoiKsqpciWRGqG6UUfpNyYiIjIA7XPms41O\nCf+Q28T1a9k7dCxJg9v/aA7/M97ioqJwRUezNSOdTHcN6dXuwDWRlZXUuBKPoecDl0KbiIjIAJTn\nhLZRyS33APXUNzBm1yaKp7Xvobp+/kpbfHQ0cVFRbMrwLTbI2pcfuCaurJiaQdoN4WgotImIiAxA\nB6uqABielNTi3J5315DY4CZy4cIOtekPbS4ntG11QtvQfV88Hz/j4D6qho8+uk4PcAptIiIiA9Ch\n6moAshJbDlWWvPE2AKOWntWhNgOhLSoKV1QU+5KTcUdFMWS/r9JWV1nN4PIiGseOO5auD1gKbSIi\nIgPQoepqBsXGBvYEDRa7ZjUH0oYyeHLHwlXz4VEbEcH2jAyGHvCFtoPrNxOBJXrKpGP/AgOQQpuI\niMgAdKimhiFhqmzW62X01vUUTDuxw202Hx4F2JqRwaiDvuHR8g2bAEg+YcrRdntAU2gTEREZgA5V\nV4cNbfs+WEequxLmt//5bH7Bw6OB0JaZyfDyItxlFdRt3Q7AkDlTj6HnA5dCm4iIyAB0qLqarISE\nlsdfWw7A0PM7Np8NQodHXc6wq38xQsHHG4jcuZPixNQOPUZEvqDQJiIiMoAUVFVxzwcfUFBVFbbS\nFvnhaoqS0hh+FNWwsMOjmb7tqsrXfUbazs0cHjr66Ds/wCm0iYiIDCB/3biRW999l+qGhhahzXq9\njNyUw74pszFHsWNBuOHRnWlpeEwEkS/+k/H5O6lafN6xf4kBSqFNRERkANlSVBR43Ty07VvzKVkV\nRXgWLDiqtpuvHgXwRkeTmz2JmetW4jERTPjPa46y56LQJiIiMoC0FdoOPvsiACO/cdFRtd38OW0A\nqS4Xg17+BwWpQ9h4yiLSx4w8qrZFG8aLiIgMGF5r2VpcHHifEhcXcj5hxdvkZWWTPWPyUbUfbk5b\nalwcQ6dNorFgLxlee5Q9F1BoExERGTD2VVTgbmzkroULqWls5KThwwPnasurmLh9PevPv5zso2w/\n3Jy2VJcLgOi42GPquyi0iYiIDBhbnaHRM8aMYf6oUSHndr30Jic0eYg/b/FRtx9uTltqs2qeHD3N\naRMRERkgtjlDo5OdZ6cFq37rXTwmgnEXHntocwU9p81faZNjp9AmIiIyQBRUVREXFUVamCCV+vFq\nckdPJiE95ajbDzs8qkpbp1FoExER6Uee3bSJD/btC3vuUE0NWQkJGGNCjteUlDMubyulJ807ps8e\nlpTE4IQEpmRmEhURQbrLRXZy8jG1KV/QnDYREZF+5MY332RSRgYrr7yyxbnD1dVkhdkFIfelt5jm\nbSLxrC8f02enuVwcvummwPsN115LRnz8MbUpX1BoExER6SfqPR4O19RQXldHvcdDbFToP/OHa2rC\nVr5qlr9LQ0QU4y48u1P7M2LQoE5tb6DT8KiIiEg/kV9ZCUB9UxM5BQWB4+sPHmTFnj2+SluYTeLT\n1q4md8wU4lM1lNmbqdImIiLST+x3QhvAWU8/zZ+WLOFQdTU/efttACKMabELQnVxGePytvHJJdpe\nqrdTaBMREekHyuvq2F1WBkC6y0VJbS3ffuklIoIWHXitbTGnLffFN5luvSSds6hb+ysdp9AmIiLS\nxzV5vUx88EG81rdN1Nqrr6a2sZGlzz5LRV0dz3z1qyx+5hmAFsOjtW+945vPtlShrbdTaBMREenj\ncsvKKKypASAjPp6xqakAfPTd7+JubGRwQgIGsNCi0pb2yRp2jT2eKSlJ3dxr6SgtRBAREenjNhUW\nBl6PClodmpmQQHZKCq7oaManpQGhlbbKw8WM27+dilPmd19n5agptImIiPRxnx8+HHg9spXHbJyQ\nlQWEVtpy//4qkdbLoMUaGu0LNDwqIiLSx31eWMi41FRiIiOZPXRo2Gu+NHo06woKSI6NDRxrfPFl\nKmMTOO6io99vVLqPQpuIiEgft6mwkGlZWTx/8cVERoQfRLvuxBO57sQTA1tYNTV6GP/JSnbOms/s\nuNiw90jvouFRERGRPmpbcTHuxkZ2lpYydfDgVgMbgDEmZM/Rbc/9i7SaCuz553dHV6UTqNImIiLS\nB+0sKWHKQw9x06mn4rWWEwYPbve91usl8r9vozgxlcnXXN6FvZTOpNAmIiLSB63evx8LPP7ppwBM\nbSW05a36hIO/f4jE7ZtpjE/E9bOfUvbia8zd/Tlrb72Hk9JTurHXciwU2kRERPqgtQcOAFBSW0tM\nZCQT0tNbXLPhwb8y8T+vYYj1kjdiPCP2biflonMA+GTBEmbf+f+6tc9ybBTaRERE+hBrLesOHuSj\n/PzAsckZGUQ1m89WnLuPsTddR/6wMaS++wbHjR9N+b6D5Cx7ieiUZOZcfSmmjTlw0vsotImIiPQh\nq/LyWPjUUwCMTklhb3l54BlswXZ/70fM8NQT++wzZIwfDUDKqKHM+el/dGd3pRMpYouIiPQhwbsf\n/OTUUwGY1mw+W8me/cxY9S8+Xfx1Rs2d1a39k66jSpuIiEgfsqu0lIToaDZfdx3ZKSkMT0rijDFj\nQq7Z8asHmOv1MOSW/+yhXkpXUGgTERHpQ3aWljI+LY3sFN+qzwsmTQo57/U0Meofz7B5wkyOnz+n\nJ7ooXUTDoyIiIr1ETkEBnx48GHhf29jI7rKykGt2lpaGXSnqt+2F1xleehD3N7/dZf2UnqHQJiIi\n0kuc+NhjzHr00cD7eY8/zrgHHsBrLQAer5fdZWVMSEtrtY3qP/2F6ph4jv/BlV3dXelmXTY8aoyJ\nA1YBsc7nvGCtvd0Y83dgonNZClBurZ0R5v69QBXQBHistarxiojIgFDv8VDd0MCnhw4BUFhTw5DE\nRPLKy/F4va2GtpqScqaufotNp53DSanJ3dll6QZdOaetHviStbbaGBMNfGCMecNae4n/AmPMfUBF\nG22cYa0t7sI+ioiI9DqbCgtZuXdv4H1eeTlDEhPZUVIC0Orw6JYHn+TExjqSrv1ed3RTulmXDY9a\nn2rnbbTzY/3njW/X2q8Dy7qqDyIiIn1RTkEBK/PyAu/3lpcDsCY/nwhjmB7muWwA8cueJj99GJMu\nWtwt/ZTu1aVz2owxkcaYDUAh8La19uOg06cBh621O1u53QLvGGPWGWOu6cp+ioiI9LTGpqbA63UH\nD7K5sJBzxo8HIK/CNyi1Ki+PWUOHkhQb2+L+gk+3cPz2dew//+va6aCf6tK/VWttkzNfbQRwkjFm\natDpy2i7yjbfufcc4HpjzIJwFxljrjHG5BhjcoqKijqt7yIiIt2por4+8HrFnj3sLS9n7ogRpLlc\n7C0vp97j4aP8fBaMGhX2/rz7H8GLYcyPteNBf9UtUdxaWw68BywGMMZEAV8F/t7GPQecPwuBF4GT\nWrnuUWvtHGvtnMzMzM7uuoiISLeoqKsD4PjMTHLLyrDA8YMHk52czLJNmzj9ySepb2ritOzsFvc2\nNXrIfvV5tkyezZATjuvmnkt36bLQZozJNMakOK9dwCJgm3P6TGCbtTa/lXsTjDFJ/tfAWcCmruqr\niIhIT/NX2i45/vjAseMzM8lOSaG8ro7NRUVMy8pi4ejRLe799N5HGFZ2iIbvfb+7uis9oCtXjw4F\nnjLGROILh89Za19zzl1Ks6FRY8ww4M/W2nOBLOBF31oFooD/s9a+2YV9FRER6VH+Stu8UaMYlpRE\nsdvNuLQ0qhsaALj/7LP57qyW+4g2NXrI/MO97Bkyhhk/uqpb+yzdq8tCm7X2M2BmK+euDHOsADjX\neb0bmN5VfRMREelt/JW2lLg4rpw+nS3FxURFRPDfCxYwJDGRb00P/8/iht8/xuzDeaz7zcOMiYrs\nzi5LN9PeoyIiIr1AuVNpS46N5e4vfzlw/LTs7LDz2MC3z2j6739DXlY2M27Us9n6O60JFhER6QX8\nw6PJcXHtvmfDA08w+tBeim74CZHRqsP0dwptIiIivYB/eDQ5zDPYwvF6mki57x72ZY5k5k1agDAQ\nKLSJiIj0AhV1dbiiooiODD8vzXq91JZXAb7A9sn5lzO2IJfC//ypqmwDhEKbiIhIL1BRX9/q0Ohn\nf/k7BZkjcKUOIi8rm/3Dx3Lym3/no69exeyf6mG6A4VCm4iISC9QUV9PSpjQtuvNfzP+P75NY3Qs\na77xH5QOH01Ncho5//O/nPz8n7Vl1QCieqqIiEgvUFFXF3Y+W/0NP8Ydm0Di6n8zd1z4LaxkYFA8\nFzKCQFMAACAASURBVBER6QXCDY9uff51jt+xnl1XXUeGAtuAp9AmIiLSCxTV1JDucoUca7zrF5Qk\npDD9rpt7qFfSmyi0iYiI9DCP18u+igrGpqYGju149V2mbfqInVdcgyslqQd7J72FQpuIiEgP219R\nQZO1IaGt4t77qYxLZOovb+nBnklvotAmIiLSw3aXlQEEQpvX08SY9avZMWs+iRmpbd0qA4hCm4iI\nSA9o8no555lneHHr1hahbc97H5FRXYb3zEU92UXpZdr1yA9jjAEuB8Zaa+8yxowChlhr13Zp70RE\nRPqpVXl5vLlrFzGRkUzJyCA6IoLhSb65a0X/fJVxwJhvfKVnOyn/n737Do+qzho4/r0z6b33QhJC\nTUKviwgIgggqWFCxdxfL6669rK69gA0Ve0FRF6VKUZGqdAKhhIQkJKT3ZNInyczc949MhhRKIA3w\nfJ6HZ5M7c+891yXMya+cc05p60jbR8Ao4Abz9xXAh50SkRBCCPE3sDg+HoDNx46RXFJCDzc3tOZC\nuQ5/beGYbw+8e4d3Z4jiHNPWpG2EqqpzAD2AqqqlgE2nRSWEEEJcwIwmE0sSEnCzs6OstpYlCQnN\n1rP1SDlIQfSQbo5SnGvamrTVK4qiBVQARVG8AVOnRSWEEEJcwAqqqiisrmbOsGGWY+N79AAgc8c+\nXPSVMGpUN0UnzlVtbWP1PrAM8FEU5RXgGuDZTotKCCGEuIAVVFUBMMjPj3+PGkWoqysPDB/e8Npv\nmwgFfCeP674AxTmpTUmbqqqLFEWJBS4BFOAqVVUTOjUyIYQQ4gJVWF0NgLejI3MvvbTZa+r27ZTZ\nORE8YmB3hCbOYW2aHlUUJQJIU1X1Q+AQMElRFLdOjUwIIYS4QDWOtPk4OrZ6zSd+L8cio9FYabs6\nLHGOa+uatiWAUVGUnsAnQDDwfadFJYQQQlzACs1Jm7eDQ7Pj5flFhOSlUz1keHeEJc5xbU3aTKqq\nGoCZwAeqqj4G+HdeWEIIIcSFq6CqCq2i4N6iQXz66g1oUHG6+KJuikycy85k9+gNwC3AKvMx684J\nSQghhLiwFVRV4eXggEZRmh2v3PwXJhR6TB3XPYGJc1pbk7bbaSiu+4qqqmmKooQB33ZeWEIIIcSF\nq7C6+oTr2Rz37ibdPwxnH89uiEqc69qUtKmqehh4FDioKEoUkKWq6hudGpkQQghxgSqoqmqVtJkM\nRnokH6QganA3RSXOdW3dPToOSKahddVHQJKiKGM7MS4hhBDiglVYXY13i6Qtc9teXGqrUKSorjiJ\nthbXnQdcqqrqEQBFUXoBPwDSY0MIIYQ4QwVVVfi02Dma//tGQgG/KeO6JSZx7mvrmjbrxoQNQFXV\nJGQjghBCCHHGag0GymtrW420sX0HOntnKaorTqqtI217FEX5HPjO/P1sYE/nhCSEEEJcmCpqa5mw\ncCEAgc7OzV7zTthPes8oBmjaOp4i/m7a+jfjfuAw8JD5z2HzMSGEEEK0UWppKXtycvjXyJHMjomx\nHK+r1hOUn051v+hujE6c69rae7QWeNv8RwghhBBnodZoBOCS8HBstMfbVOXsPkAPkxHrAZK0iZM7\nZdKmKMpBQD3Z66qqxpzsNSGEEEI0pzcYALDVNu8rWrwzlh6Ax3Ap9yFO7nQjbTMBXyCzxfFgIK9T\nIhJCCCEuULWNSZtV84/fuv0HMSgaAkfKJgRxcqdb0/YOUKaqanrTP0CZ+TUhhBBCtFHj9Khdi6TN\nLjGBbO8gbB0dTnSaEMDpkzZfVVUPtjxoPtajUyISQgghLlC1J5ke9U5Ppji0Z3eEJM4jp0va3E7x\nmn1HBiKEEEJc6PQnmB411hvwLc2jNiyiu8IS54nTJW17FEW5u+VBRVHuAmI7JyQhhBDiwtQ4Pdp0\npK3wSBrWJiOa8PDuCkucJ063EeH/gGWKoszmeJI2FLABZnRmYEIIIcSFpnF6tOmatpJDifgBDpGS\ntIlTO2XSpqpqPjBaUZTxQJT58GpVVTd0emRCCCHEBeZE06NVR44C4Na/d7fEJM4fbS2uuxHY2Mmx\nCCGEEBe0E02PGlLTMKHgExXZXWGJ84Q0OBNCCCG6yInqtFllHKPIxVPKfYjTkqRNCCGE6CJ6gwFr\njQaNoliOOeRkUewd0I1RifOFJG1CCCFEF6k1Glt1Q3AvzKHKL7CbIhLnE0nahBBCiC5SazA02zlq\nrDfgoyugPjikG6MS54tOS9oURbFTFGWXoij7FUWJVxTlv+bjLyiKkq0oSpz5z9STnD9FUZQjiqKk\nKIryZGfFKYQQQnQVvcHQbBNCSWoGVqoJTXBwN0Ylzhdt2j16lmqBCaqqViqKYg38pSjKWvNr76iq\nOvdkJyqKogU+BCYBWcBuRVFWqqp6uBPjFUIIITpVy+nRksSjeAO2YTLSJk6v00ba1AaV5m+tzX/U\nNp4+HEhRVTVVVdU64Efgyk4IUwghhOgytUZjs+nRqqPpADj3lMK64vQ6dU2boihaRVHigAJgnaqq\nO80vPagoygFFUb5UFMX9BKcGAplNvs8yHzvRPe5RFGWPoih7CgsLOzR+IYQQoiO1nB6tO9aQtHn2\nlqRNnF6nJm2qqhpVVR0IBAHDFUWJAhYA4cBAIBeY1857fKqq6lBVVYd6e3u3O2YhhBCis9QaDM13\nj2ZmUmNli2uQb/cFJc4bXbJ7VFVVHQ0dFaaoqppvTuZMwGc0TIW2lA00XZUZZD4mhBBCnLdaTo9a\n5+ZQ5O6DopFiDuL0OnP3qLeiKG7mr+1p2FSQqCiKf5O3zQAOneD03UCkoihhiqLYANcDKzsrViGE\nEKIr1LaYHnUsyKXM06cbIxLnk87cPeoPfGPeCaoBFququkpRlG8VRRlIw6aEY8C9AIqiBACfq6o6\nVVVVg6IoDwC/AVrgS1VV4zsxViGEEKLT6VtMj7qV5JM5cGQ3RiTOJ52WtKmqegAYdILjN5/k/TnA\n1CbfrwHWdFZ8QgghRFerNRotI22G2jq8yotJ85duCKJtZBJdCCGE6CJNOyIUHknFSjWhDe/RvUGJ\n84YkbUIIIUQ7HMjPJ6m4uE3vbVryoyQ+CQD7yJ6dFpu4sEjSJoQQQpyG0WTijb/+orCqqtVrt69Y\nwQNr2raap2lHhOqkowC494vsuEDFBU2SNiGEEOI0dmZn8+T69Xx74ECr19J1Oo6WlrbpOk2nRw2p\naZhQ8O4vI22ibSRpE0IIIU5jR1YW0DAV2lRNfT3FNTVklpWRV1nJMZ0OgK/27eOShQtR1ebdG5tO\nj1plpFPk4omto0MXPIG4EEjSJoToUvEFBbywaRO1BkN3hyJEm7VM2v5ITSW7vJycigoA6k0mLv76\na8Lee499ubncsXIlG9LSKK6psVzDaDJhVFXL9KhDThbF3gFd/CTifCZJmxCiS720ZQv/3byZa376\nCVOLUYjzxRd79/Jnenp3hyG60HZz0hZfWEheZSWTv/uON7duJau83PKexs0IQz/7zHKs6eu1RiOA\nZXrUvTCHKv+mzX+EODVJ2oQQna7OaKSqrg6AlJISAFYlJbE0IaE7wzorNfX1/HPNGm5dvpx684ew\nuLBllJWRVV7O0IAA6oxG5m3bhklVOVpaSrZ5pK3R3YMH88jIkUzr1QuAzLIyy2t68+iyrVaLobYO\nH10B9SEhXfcg4rwnSZsQotPdtXIlTq+9RmZZGQcLCvjXyJH08fLihU2b2jzadkyna/YB2F12ZWdT\nZzSSptPxzf79rV7fmJbGQ2vXtlrLJM5f7+3YgUZReGrMGAA+3L0bgDSdrtlIGsDs6GjmXnopn0yb\nBkBm05G2xqTNyoq8/YkNNdoiIrriEcQFQpI2IUSna9xx1+fDD6kzGhkWGMjzF19MfGEhP8Uf71BX\nUVvLP778kq0ZGQCsSExk4f79/HDwIGHvvcfVixd3S/wAn8bGctPSpczbvh0FCHF1ZVliYrP35FRU\nMGHhQubv2kVZbW33BCo6VEFVFR/t2cNNMTFc0bs3E8LCqDEY0CoKx8xJm4utLd4ODZsJBvr5AeDr\n6IiVRnPS6dHivQ0/Ey4D+3fxE4nzWWf2HhVCCKrq6lCAnh4eJJunRgf7+xPh7s5LW7bw5Pr1/JWR\nwWsTJ7IhLY1tmZl8HBtLsKsrMxcvbjYStzc3t1ueIaOsjHtXrUKjKJhUlRhfXyLc3UksKmr2vvk7\nd1q+zi4vx83OrqtDFR1sd3Y2eoOBewYPxkqjYd3NN7M+NZXdOTk8s2ED+/LyCHJxwdHaGlc7O1zN\n/59rNRoCnZ2bjbQ1nR6tOdSwNMBv6ICufyhx3pKRNiFEp6isq6PeaORQQQEq8MbEiYS6ugINCZxW\no+HVCRM4ptPxwe7dLEtI4I/UVKBhvVtjArT+llvYescdPDxiBAAGk6nLn2WfOVn8+PLLsdVqmdCj\nB2FubqTpdM2mQfOaFF5tudZJnJ8ad4eGmP/uahSFSRERDPD1BeCvjAwCnZ35z8UX88bEic3ODXZ1\nbTal33R6VElKotTBBbcQ/654DHGBkJE2IUSnmLhwIXtycpgzbBgAA/z8ODxnDnmVlWgUBYAr+/Sh\n9tlnCXr7bX47epTY3FxcbW3R6fW8s2MH03v1YkJYGAAJhYUYVZVlCQnYaLVM69WLGoMBJxubTn+W\nuLw8FODG6Ggmhofj7ejIN3Fx6A0G8ior8Xd2BkCn1+NkY0NlXR3ZLdY6ifNTY9Lm5+TU7HiYuztR\n+flMTE3FAxum3Xxzq3ODXVzYmZ1t+b7CvBnHVqvFKT2VfL9Q3DsxdnHhkaRNCNHhTKpq+bB6f9cu\nAHq4uaFRFMLdGz6m6qr17HthLtabN/GvPmG8VldHucHAS+PHszwxkWBXV96+9FLLNXu4uQENLYNM\nqsodgwaxOD6ehDlz8HTonOKkRdXVXLJwIVV1dfTy9MTRxoYwc5IYZn6ONJ3OkrSV1tTQz9ubXdnZ\nMtJ2gcitrMTH0RFrc0HcRlaHU9n12WfYGwzw228c+GkF1eGRqN7eYFLRZmYwJ/MI99RWsSO3mqEv\nPcpT69fjZGPDIH9/rHOPcWzImO55KHHekqRNCNHhcs0Jy+OjR/Pmtm1cHBpqGV0DKM8vIvPiKYw4\nEkupgytP7lrPwIgIHr/7Hu4ZMoRnx45tdc3GpK2qvh44voNvyqJFeNrbs+L66y1FSzvK9sxMSzHV\nWf2bLxgPc3OjT2EhWU+/zKHJlxF101Xo9Hp6uLnhYW8vI20XiJyKCgLMSXkj1WRCve0Oqqytue7J\np3k6OQ+/zesITz6IU101AMWObuR6eqPqq4l+/SmWxu3irxExLJwxA+eaOpwrSkiJ7NUdjyTOY5K0\nCSE6XJq5lc/4sDCeHTu22WYC1WTi6GVXE5Ucx+4X3mHocw+x7enXueSt/7Doi+9xuf+fJ7xmsKsr\nGiA6Nxd7g4GdgYGMDg1la2YmAIcKChgS0LHV5RtrysHxXYGNbFOz2P7557jV1mJY9DkH636gVK9n\noJ0dgc7OMtJ2gThR0ha/aAVRxw7z55Ov8PPzjzf7ZaG2qhrVaMLTxQkPVeWOFSsY+uqrzPl1Gc96\nOHNTTAyHvllKNOA4fEgXP40438lGBCFEh0s1N88Od3fH2dbWsqMOYPd/5jJo3xZi5zzFsOf/D0Wj\nYfTrT7P/5ffon3yAI2OnUFetb3XNgth4tn3+BXGffML2L74g+cOPmZdXzepZ1wOte0J2hKTiYhys\nrXls9GhuiIqyHDfWGzBedyMmRcO/X32TDP8w/B++j5ryctzt7Ah0cZGk7QKRU1FBQIv1bPXvvkep\ngyvDnn641eiuraMDdi4N71cUhU+nTyfpP/9hQ1g4zyz+gWObdlC1bgNGRUP4VZO77DnEhUGSNiFE\nh0srLUUBy27RRrVV1YR+MJfE8GhGvPvfZq8NfWoOu598lQH7t5Lfoxc7HnqO7D0HqSrWsfPRF/EY\nM4JeRUU8duUM9rz8PoqtMyOeeZDIS6+kV0VlpyRtR4qLifH15c1Jkwg1T88C7HniFSIzEnn1+tls\n8nSlZu7beFWWct2OHbiZR9pyJGk75xwqKKD8DOrnGUwm8quqLGsWAYrTMhmwdwuJ02dh5+x42mtY\na7W8N20a/X5dR6WdM9pZs/Daso7U4F44eck2BHFmJGkTQnS4NJ2OAGfnVqMQcS+9i29ZIfXPPY+i\naf3Pz4jXnmT/p99TZ2PHyPkvEzgsBkcvd0bMe5608P7sWf0bU96ey9BnHqRH+hH2vPw+vgVZbP7k\nE0q2bEVVVfbn5XVYN4IjxcX08vRsdqyqWEefBXM52HcYXnfcwt7cXJymXER8RDSPbtuGm40Ngc7O\n5FdWSpurc0hqaSnRCxbwyK+/tvmcgqoqTKrabHr06Nc/oUHF+7Ybz+j+fr3Cyf/4S0IKM+mZlUzx\nwGFndL4QIEmbEKITpOl0lt2VTTkv/Yk0vzCibplx0nMH3H0DEVlJZO85xM5HXmD7Xf8m/rvl9Du8\ni0kTLuKS8HAANFZahj7zIKW/b6TOyoYFb73Bq488wcBPPmHTsWPtfobKujpyKiro3SJpO/jSO7jq\nK7F++SVm9usHwIqkJHJn30SPsjKct+/Dw94eFc5oVEd0nMq6OpYlJDRL3l/esgWAFPPUfVF1Nd8d\nOMD+vLyTXqdxtLRp0ma1ZhUFLl5EXHrRGcfVf/aV7J33GeV2TrjedP0Zny+EJG1CiA6lqipJxcWE\nNZlOBCjLyqNXygHyxk0+4ShbS4FD+jPi7ecZ9dlc+s++8qTnhI4dxvKvv+agrw9PvfcW/7d9O5vS\n0tr9HEnFxQDNkjaTwUjIt5+REBFDn5mT6eXpST9vbxbHx2N/3XSqrK0JWrUGG3N5iPpuKAT8d1dU\nXU3wO+8wc/Fi1pmLNWeWlbHQ3Ce2zmhEVVUmLlzIzcuWcevy5Se9Vpo5wQtycQEapvd7H9hB2sjx\nbfo7fCKD/3UXzlVl9L126lmdL/7eJGkTQnSorZmZ5FVWMtE8ItYo+dslWKkm3GedfJTtbN06eSJb\nvvqaLQNH8s5vv+H55tvtvmZj0tZ0evTwDysJKMml6s57LMfuHDSIrZmZbC4pYnmfPozavhElLQuN\nyURxaka74xBnZkViIjp9w0aWOPMo2iexsajAxPBwjul0xBcWsj8/n1BXVw7k51ve39KW9HQcrK2J\n9vEBIOl/q3Csq8FuxhXtivFsEz4h5G+OEKJDfXfgAA7W1szs27fZcfXXXyl1cKXXtEs6/J6udnY8\nNmkiY3f/xYrhY3jot9Vsu/fxdl3zSFGRpWdqo5rPv6Tc1pGoObdajt09eDCutrY8s2EDz48bByhc\nefddlL32Gr0HRlOU1P5RP9F2ySUlWGs0+Ds5cSA/nzqjkU9jY5nWqxdjQ0LIq6zkuwMHUIDXJ05E\nBbZmtE6u64xGNqWnMyYkxFJYt3rJcmqsbOl941Vd+1BCmEnSJoToUEsTEriyd+9W7aX8DsdxrM8g\nNFbak5zZfhorLZUL3ufH/v0Z/elb7Hv/q7O+1pHiYkJcXbG3tgagRldB/+1/kHjRFEtJBwBnW1tu\nGdDQ9Puopyf7539Kll8QNdbWWKkmsjdub99DiTOSXFJCuLs7g/39OZCfz6GCAgqrq7kxKspSoPnT\n2FguCg3lyt69AZj2ww88tHat5RrPb9yI7csvc6iggPE9egAN9QVDtm3gSNTwZv//C9GVJGkTQnSY\neqORwupq+nl7Nzuuy8gluCgL/dDO3zE3rmcEN8+cSYp3AB6v/hdjveGsrpNUXExvLy/L94lfL8ah\nXo/DLbNbvXdqZKTl60G3Xkv6sh/o9eCDANTEHTir+4uzk1xcTKSnJzG+viQUFbEnJweAQf7+lrIt\npXo9V/fti721teXv6vxduzCpKj8eOsSL5k0LgKX3ber67fjr8qm77PIufiIhjpOkTQjRYRrXBrk1\nKaYLkL52IwAu4zq/12KgiwtRgYF8OP1yQvPT2ffmgjO+hqqqHCkubrYJwfjzEkodXOhzXesP7YtD\nQy1fO1hbY63RoLO3J9/ZA+3h+LN7EHHGTKpKSkkJkR4exPj6YjCZ+F98PPZWVkS4u1tG2gDL9P3S\n666zjJRuSU/nnl9+4R/BwRQ8+iirbriBYeYuGwVfLMSgaOh5d+ukXYiuIkmbEKLDlJqTNvcWSVvN\nlq0YFQ1hUy7ukjimRUYyPziANN9Q3D96H/UMdnEaTCZ+PHSISnOTeGhobt97z2aSR0zAytam1TmN\nU6jQUAW/cQ1UVmAP3NKS2/k0oq1yKyqoMRiI9PBgZFAQABvS0ojy8UGr0RDg7IyVRsNF3t641RlR\nTSbcC8u4atchbtu3j6e+W4TBZGLRzJl4Ozpyea9eKIrSMDW67hcS+g7FIyyom59S/J1J0iaE6DAn\nG2mzPxRHhl8PHNxdT3Rah7u8Vy+MisLOa2cRkZNC/HcnL+vQ0id79nDj0qXA8XIfSUvW4lxbjfWM\nK0963sH772fl9Q21txpLfhSHhROUewyTQYrsdoVkc6/Ynh4e9HBzY5x5PVqMry8Ahspq/vjfMjY8\n8CBO3h6YrKzx6deTGa89x1crVvDryy9xs66yWfcLaOg1GliSS82Mq7v0eYRoSZI2IUSHKa2pAcDd\n3r7Zcc/sdEqDw090SqeIMpdoSJ0+kVIHV+o++LDN5y48cIBIDw9eHDeOi80f+hUrV1OnsSLyhpMn\nbVE+Pkw3L2y3Npd0qOoVib2hlty4w2f5JOJMNPa8jTDv+L1z0CAABvj6oppMxE++movi49hz1S3s\nuP9Jds26hx0PPEPyqo38Y84D5Dg78867b3Pg8x8t1zTU1mH39BPkufkQ8/g/u/6hhGjC6vRvEUKI\nttGdYHq0Xl+LX0kumZOmd1kcTjY2eDk4kFFXy5HJMxiy4luKjmbgFRFyyvOOFBWxKzubuZMm8e/R\noy3HfbdtIjlyAP3b2CuycXrU0LMhUS05lEjg0OizexjRZkXV1QD4Ojb0BL22Xz8Si4q4rn9/dtz+\nCKN2/M6Oex9n5MdvtDq3PieZsQ72/PX9EmLuvoH98+ej9w0gOHYrPUty2fP6R/jJrlHRzWSkTQjR\nYUpPMD2aF3cYK9WEVZ/eXRpLqKsr6WVlBDz2INYmI8lvfnDacxor6F9jbk8FUHgklfCco1SMa3t9\nucbpUcIaksSapNQziFycraLqamy1WhzMawxtrax4ecIESn//i1EL32f32GmM+Oi1E57bx8uLchcX\nPHdtY/uN9+OZfYw+f/5KiW8gcR8sZOgT93flowhxQjLSJoToMCeaHi3ed4hgwDm670nO6hw93NyI\nLywkZNRgDkcOJHjJIkwfvnbKOnGZZWXYaLWEuB5fe5e2aBnegM+1J58abalxelQJ8qVeo8XYAW21\nxOkVV1fj6eCAoiiWY8Z6AzYPPkCeqzf9ln570m4Ez44dy6z+/fEK8MVr0UfARwB0zSpMIdpGRtqE\nEB1Gp9djq9ViZ3X890F9fCIAfkNjujSWUFdX0nU6VFWl+tY7CCrO4fAPK095TlZFBUEuLs0+9LW/\n/0ahswdh40e2+d6N06NGrZZCNx+sM6WdVVcorqnBs8V6ykNf/Eh4zlGynngeR0+3k5zZ0K7s8l69\nOjtEIdpFkjYhRIcp1etbbUJQUpLR2TvjFuLfpbH0cHOjxmAgLi+P3nNuRWfvTP1775/ynKzycktz\ncGhYhN5z/3bShlx0Rv0iG6dH64xGSr0DcMrNOruHEGekqLoaLweHZsdMn31BsaMbMQ/f2U1RCdFx\nJGkTQnQYnV7fqtyHY3oq+b7BXR5LYyHVwZ9+ylv7Ykm4cjYDYjeTsX3vSc9pmbSlrN6Iq74S7WWT\nz+jejdOj9SYTVf5BeBTmnMUTiDNVXFODZ5OkrTgtk+h9W0i67GpsHOxOcaYQ5wdJ2oQQHaZUr29V\nWNcrN4Py4LAuj6Vpra0Fe/aweMpo6rRW5D3z0gnfr6pqQ9Lm7Gw5VrrsF4yKhojrz6xBeOP0aL3R\niDEkFJ+KYmqrqs/iKcSZKK6ubjY9euzn1VipJjxumtWNUQnRcSRpE0J0GF2L6dEaXQV+ZYUYInp2\neSyRHh5E+fhw16BBFFVX8+6xVD4fPowhm1aecLStqLqaOqOR4CabEDz/3EBKj75nPLXbdHrUKrwh\nYS04eKQdTyNOx6SqlNTUNJseNWz+k2prW8Inj+3GyIToOLJ7VAjRYUpraiytnwByYw8SDtj07dpy\nH9DQWurg/fejqipWGg1eDg68VFnJbXtiKfq/JwjZua7Z+zPLywEs06O6jFx6piey85YHzvjeTadH\nHXtHNFzvcDLBIwe155HEKZTp9RhVtdlIm1fcblIjooiys+3GyIToODLSJoToMLoW06O6/Q2dAFwH\n9O+ukFAUhQXTpvHShAn07NuXL8dPYPCuP0hes7nZ+7JaJG3Jny1Cg4rntWc2NQrNp0c9ovoAUJ0s\ntdo6U7G53Ezjmrby/CLCslOoGDqqO8MSokNJ0iaE6BCqqrbaiFCbYC73MeTc6AYQ5e3NvNEjKbV3\noebxJyzHP9+7lyt/bGhd1Ji02S1ZTJZnAJFTx53xfRqnR+tNJrx7h1Gv0WKSWm2dqtjcDaFxejRt\n5To0qDhPGt+dYQnRoSRpE0J0iJKaGoyqineTNUVWKckUOnvg1Mb2T52tv48PGajsv+VeYuJ3csjc\nSP7bAwfwc3LiweHD8XV0pOhoBv0SY8mcdMUZlfpopDXXeaszGtFaW1Egtdo6XWMLq8bp0er1mzAo\nGsKnt72ThRDnOknahBAdIreyEgD/JrsvnTKPUeAX2l0htdLf2xuA2jm3ku/qjfVzz6CrrmZbZia3\nDxzI+5ddhqIopMz9CK1qIuD+O87qPoqiYK3RUG80AlDqI7XaOlvL6VHX2J2kBvfGwV16GogLhyRt\nQogOkVNRAUBAk6TNNy+dqpCuL/dxMv19fAA4Ul1J+gOP0vvYYd547CkMJhNTejbscDUZjAT9AaMF\negAAIABJREFU9B0JETGEjh121vey1mqpN5kAqPYPxrNIarV1pgP5+WgUBR9HR2qrqolIO0zJ4OHd\nHZYQHarTkjZFUewURdmlKMp+RVHiFUX5r/n4W4qiJCqKckBRlGWKopywr4iiKMcURTmoKEqcoih7\nOitOIUTHyDUnbf5OTgCU5RbiUVWGKTKyO8NqxtfREQ97e+ILCqi9/RoSPb249X+LCLCyYlRQEAAH\nv1pMUHE2Vbe1r4K+jVZLnXmkzRgaindFCfqKqnY/g2ittKaGT2Njua5/f1xsbTm6aiO2xnpsx1/c\n3aEJ0aE6c6StFpigquoAYCAwRVGUkcA6IEpV1RggCXjqFNcYr6rqQFVVh3ZinEKIDtByejQ/9iAA\ntt1Q7uNkFEVhkJ8fO7Oz+c9ff/LcjJn0KS7mhy37sFIU9BVVeDz9OFmeAUS1s+1R0+lRS622Q1Kr\nrTPM37WLiro6nh4zBoCy5b80rGe7dlo3RyZEx+q0pE1tUGn+1tr8R1VV9XdVVQ3m4zuAoM6KQQjR\ndXIqKnC1tcXB2hqA8v3xAHgMjOrOsFq5ODSU/fn5bM3IIPy6a9gx/SbG/r6EpIhoisJ6EVyURenc\nd7FzdmzXfZpOjzr2aph61R1Obnf84rhDBQV8HRfHuzt2cEXv3kT7+gLg8+cGknrG4Orv3c0RCtGx\nOnVNm6IoWkVR4oACYJ2qqjtbvOUOYO1JTleBPxRFiVUU5Z7OjFMI0X65lZXNNiHUJx7BhILfkO6r\n0XYi48MaRr3qTSYmhoczYvk37HjoOTRGIzpvf+I+WEj0bde2+z42TZI2z+heANQkH233dUWDqro6\npv/wA7evWEGpXs8zF10EQEHCUSKykykbP6mbIxSi43VqRwRVVY3AQPO6tWWKokSpqnoIQFGUZwAD\nsOgkp49RVTVbURQfYJ2iKImqqm5p+SZzQncPQEhISKc8hxDi9HIrKizr2QCsU4+S5+5LgKPDKc7q\nesMDA3GwtsZoMjEmJARFo2Hkey/Cey926H2sNRrLmjavyDDqNFaY0o516D3+bnR6PTZaLQ7W1ryx\ndSvHdDpeGj8eV1tbhgcGApD60Vf4AP43XtO9wQrRCbqkjZWqqjpFUTYCU4BDiqLcBkwDLlFVVT3J\nOdnm/y1QFGUZMBxolbSpqvop8CnA0KFDT3gtIUTny6moYHRwsOV718w0igJCCejGmE7ERqvl8shI\njKqKvXkqtzNYa7WWNW1aayty3X2wkVpt7TLhm2+I8vFh4YwZrElOZnyPHjw79nhfUWO9geDvvyQh\nIoa+F8vOUXHh6czdo96NO0MVRbEHJgGJiqJMAR4HrlBVtfok5zoqiuLc+DVwKXCos2IVQrSPqqoN\n06PmkTbVZMI/P5Oa0PBujuzEfrzmGn66tv1ToKfSdHoUQOcTgFPe369W29xt2/jlSPs3YBRWVbEv\nL48NaWnUGgwcyM+3jK41OrDgWwJLcqm+9/5230+Ic1FnrmnzBzYqinIA2E3DmrZVwAeAMw1TnnGK\nonwMoChKgKIoa8zn+gJ/KYqyH9gFrFZV9ddOjFUI0Q6lej16g8Gypq04LQunumrUc6jcR1MaRUFj\n7lrQWZpOj4K5Vlthbqfe81xjMJn4z8aNfHvgQJvPSS0tZeinnxJfUNDs+PashoQ3u6KCtSkp1JtM\nDA04Po5bV63H6+XnyfAOJubB2zvmAYQ4x3Ta9KiqqgeAQSc43vMk788Bppq/TgUGdFZsQoiOlVJS\nAkBPDw8ACvYcxAtwiOrbjVF1r6bTowDGkFC8NpSgL6/EzsXpFGeenQW7dzPAz6/ZFHV3SygspMZg\noLq+vs3n3LBkCbG5uaxPS7MUQwbYmnF8avmDXbsAmiVtex97kZGFmez/eBEhdrYdEL0Q5x7piCCE\naLek4mIAenl6AlB1KAEAz4Hn1s7RrtRyetQ6omHXav7Bjq/VlldZyT/XrGHSt992+LXbY09OQxeI\ntiZt8QUF7MrOBmiW8AJszcxksL8/tlot69PS8LC3J9S1oUVV3sEkYj57m7gBY4i5+/oOfAIhzi2S\ntAkh2i2puBiNohDu3tAY3nDkCHUaK3yjz53Cul2t5fSoY+/GWm1JHX6vpQkNSbJDB2+sqDMaWyVP\nZ6IxaasxGE7zzgbr09IsXzf2Em2UWFTE8IAAxoeFoVUUZkdHo5inuHNvvRsA34WfoWjkY01cuLpk\n96gQ4sKWVFxMmJsbNlotAHapKeR6BRBqa9PNkXWfltOj3oMbigzXmEchO9Li+IZCxm52duRUVGCt\n0eDt2L7iwABXL16Ms40N31999VmdH5vbsIavrSNtWzMzCXZxodZopLj6+D41k6pSqtfj5eDA/KlT\nqTcaLTt/9773JYP3bWHH/U8yMqbPWcUpxPlCfiURQrRbUnGxZWoUwC07ndKA0G6MqPu1nB71igih\n2NENzcGDHXqfyro6/jSv98ouL+fqxYu5b/Xqdl/XpKpsTEsjLi/vrM6vNxot57YlaVNVla0ZGfwj\nJARPe/tmI23ltbWYVBUPe3usNBpLwlZ4JJWwpx8hJSiSIW+/cFZxCnE+kaRNCNEuqqo2S9oMtXUE\nFmaiDz/hnqO/jaa9RxvlhETifjSx1Xt1ej2VdXVtuq6qqqw8csRy7b25uZhUlUvCwqgxGNiTk0NG\nWVm7408uLqaqvp6s8vKzOj++sJBaoxEXW9s2JW0ZZWVkV1Twj+BgPB0cmiVtjaNuHvb2lmNl2QXo\nLr0c2/parH/8AWvZfCD+BiRpE0K0S25lJVX19USad45m7zmIjdGANiammyPrXtZabbM1bQBVvfsR\nnJOKsb75Gq/x33zDHStWtOm6WzMzufLHH3n0998BLAv3r+rTMDWo1tdz2fKVbL/2LnIPtE4Q22qf\neZSsoq6O8traMz6/cT3b6ODgNiVtjaNywwICGkbamkyPlpgTOE+Hhu4aSb+sp2LAYEKzU0h+/3NC\n/zHkjOMT4nwkSZsQol0aR3XCzJsQinfsBcB9RKuKP38r1hpNs+lRAO3AgdgZ6sjec7xuWWJREXF5\neaxLTcV04gYxzRzT6QBYaK59tis7mx5ubgz088PKaGTlDz/w4ppVDF/yFTajR1GQ0PZ+p0sOH+Zg\nfj7QMILX6GxG22JzcnC1tSXax6dNSVtORQUAIa6uzaZHy/OLyH3seW7btw9nVWXfu18QMnMqWqOB\nlO+WMuC+m844NiHOV5K0iS6lqirzd+5kxOefW/6RFue3xg/0IBcXAGrjDmBCIXDk4O4Mq9vZtNiI\nAOAxqmFEqPCvXZZjyxMbRsN0ej2HWhSUPZHGmng6vZ5PY2P5KyOD4YGBBDo78+a6dUxNSeGfU6eS\nuv4v7OprKbzyWtQWyeOJVNXVcc1PPxHz8ce8sGkTa5KTsTbvxEzavY89r31A5s79rc4zqSoJhYWt\nju/JzWVIQABONjbUGY0YThNDTkUFGkXBx9ERTwcHcisqmDLvHSp79+WKH77gqxUrGBY9gEGP3EVG\nQAS2e/fQ7/rpp30uIS4kkrSJLrXo4EEe+vVXdmVn89Datd0djugALZM226QEcjwDsHdz7s6wul3L\nkh8AwWOGUWNlS/2fWy3HVhw5QrD5v92W9PTTXjelpAQbrRY/JyfuXbWK3MpKpvbsiX7bPh7esYMP\nhg1jwfDhOA+L5tCcJ+ifvI+En07/s5ZmHsED+O/mzRwtLeXJMWPoXVjIRddew9CnH8TnH8M4tHCp\n5X019fVc9eOP9PvoI/5ITbUcN5pMHMjPZ7Cfn6UMSc1pRttyKyvxcXREq9HgaW+Pqqo88MkCPCp1\nfPjW+0y6+Wb2XTyVHfc9QY+EvXiEBZ32mYS40EjSJrrU+rQ0vB0ceHHcOJYkJLRqVSPOP1nl5dhb\nWeFuZweA57FkCkPPzfZVXcm6ye7Rmvp6/rtpE3qtwtGeUXjt2wk07LDcm5vL9VFRBLu4WHaBnkpK\nSQkXhYRw7OGHOXDffRQ99hg39umL8tD/UeTgyDOXXAJAUXU1Mf/5F6UOLtS+8eZpr5tWWgrA9jvv\nJOuRR8j99795YvgIfl68GKNG4dA3S8jxDiLs7pvI3LEPgJ8OH+aXpCQUYFnC8VImWeXl1BmN9PL0\ntCRtp5sizamoIMDcBs3TwYHRmZlMS07mmUsm8KOnE39ERDD8158YueB1bBzsTvs8QlyIJGkTXWpP\nTg5DAwK40rxoen/aMfbN/5raqurTnHliR0tKuGnp0jNqk9MZSmtq2H+WpRHOd5nl5QS5uKAoCrqM\nXIIKs9BHRXd3WN2u6fTossREXti8mWUJCVQM/wfhWSmU5RaSXFJCndFItI8PF4WGsiU9HUNdPbFv\nfczOy65n+80PkHs0nZ1ZxxvNp5SU0NPDA1srK6J9ffGws2PftBvpk3qQhTffwSUDBwINOy7t3ZxJ\nnH49MXF/UZyWecp4G0fawt3dCXRxwc3OjoPPvEFUYSEf33kPUbfMxGH9b9Rrrai7Zha1VdXszMrC\n2caGab16sSo5GdW8Ju+oOQGM8PCwJG05CSnoMk7eezW3shJ/p4b2Xp729ty7Zw/lNjYsGjWKvzIy\ncLaxwdpcB1CIvytJ2kSXqa6v53BhIUMDAujl6UlESQljplzOoIduJ37CFa121DUVm5Nzwqmj5YmJ\nLDp4sE3TSp3plT//ZPjnn1NYVdWtcXSHLHPSBpCyaCla1YTHTFlr1HR6dFVSQxeE/fn5uFw6AQ0q\nz774Ik+tXw9AtK8vY0NCKNbpODB4DEMev5++G1cxfNECbKOjuPuVV9iYloZOr6e4psbS4xVg5z+f\nYvj6pWy//j4e/fh9/nPxxcDxjgI+d96CVjWR8umiU8abVlqKg7U13uYdmiVpWfT7eB4bevZm+5CG\nncC+/SJJev09IrKTiZt1N7vMv4Rd0bs3GWVllqbuR83r7sLd3XGwtuaD1asZMHwwuuGjMdSeuLRJ\nbkWFJWlzrNJz7eHDfDdgAOHmXqqNO0eF+DuTpE10mbi8PEyqytCAAGw1Gr74ZTWuNVXsmDabwbvW\ns+eZN0567r9+/527Vq5sdfyIuedlYzNptQ277zrDoYIC6oxGvj1w4PRvvsA0TdrUtWspdXCl59Tx\n3RxV97PWajGqKvVGI2tTUoCGn4GI6ZdQbW1L9K5trDxyBK2i0MfLi7Ghocxfu5bB8bv48ua72Lx3\nNxkbtqG3tuL3b79lwff/Y9OxYwBEmHfq7p33GSM/eZPYkZcy4tsPAPAyJzdF5pIZ4ZeMIsszEPtf\nlp8y3jSdjjA3N0trqOR7H8G+Xs+K++/jz8xMcs0bh9bEhPHOyFGMWP09/X79jeGBgczo04cQBweW\nzHmAff2HYfXyXFwMBoJdXDBt2sGc3bs5FNGfHvnH2PvK/Fb3NphMFFRVWaZH61f9gb3BwB8jRxPj\n6wtgmX4X4u9MkjbRqT7fu5dbly+npKbGUk9qiL8/e55/m4vTjvLa9CsYsWIhR3r0I/iLD0/4W7iq\nqhwqKOBoaSm1LXoYWpK2zEyO6XS4vfFGswXRXSWxqAiAL/bt6/J7dyejyURORQXBLi4YauuI2PsX\nRwePQWstHfIaW3pty8xEp9fj7+REXF4ets6ObB40nOvi47GrryfS0xM7KyvYtJt7Y2OZO2oUd0YE\nMWvZUoIuGsqLTz+DVlWZ9+ZrLHvqKaLc3bkkPJzE5b/T78kHSAyPpv/vS9FYNdzP01yAtrHOmaLR\nkDnhMvom7KEst/Uuz0ZpOp2lbEvhkVQGr1/G3inX8tBtt1BnNPL4H3+gqiq/Hz3K45Mmsi08kk9W\nLGfQyj8oWL2JNR99xbxfVuKVm87ty39k3dcLSfllPYNfeZlcJycKln5PUkhffD95v9W98ysrUQF/\nc9LmtWEzJfYOPP/E45akra39S4W4kEnSJjrVfzZuZOH+/Yz7+mt+PnyYPl5e2BSU0mvuf9kd1ov3\nY/pjAqr/9RgBJbnEzf2k1TXyq6ooqanBpKqWcgf15t6EjcnSzuxs/khNpby2lpe2bOnKR6Smvp6M\nsjIcrK05XFiI/m/04bIrOxuDyUSQiwv753+NR1UZVrOu6+6wzgmN5TIOmjfbzI6OplSvJ7O8nKUj\nhuBWW8uMhASifXzQl1di/+Acslzd+WbGDOZOmkSNwUB8YSEHvT145JFHqbV35Zvly/nllXnE3/Yw\nvrOvpdjFE58/1mDnfLzPqL21NfZWVs06CrhfNxNrk5GU75acMNZ0nY600lLC3NwASHlxLlqTiaD/\nPk2EhwdPjhnDdwcO8NDatezOycGg1TLtmhnsDApi1vw36Hvd5QTmZzHrmmv4dsUS7r/jbgbkZtP7\nqksJKsrl7unTqbe3o2TmdYQWZJC16/iI9LKEBILeeQcAfycnTAYjkXv/InXwxQwIDLAkbZkd0OVB\niPOdJG2i01TW1ZFfVUVvT08OFhSwNTOTm2NiyLj5Huzr9ex94QVqTCbSy8oYcP/NHPPtgdcHb2My\nNC+T0HSHaWJREdsyMwl591283nqLgqoqhgYEoKmsJP2V1/lm6VLmP/EE+c6uZLt5UuDiSeyoyeQd\nTOq050wuKUGF4yMC3bwpoitU19ezMyuL0V9+CUA/b28cPv6QHA9/ou+9sZujOzc0LppPLCpCqyhc\n0bs3AC9u3sxXHq4c8Qnknd/XM7tew8HpN9Aj/xjFcxdw4NFHLd0NdmVnk1lWhnboICJyjrH33S8w\nWtsy6sdPKPANxrR69QlLX3g6OFimRwEir5yEzt4Z0y+rWr13d3Y2kfPnU1FXxwBfX/TllfRZ/j37\nB48lcGjDhpL/jhvHfUOG8MHu3RhMJp74xz/o2bMnVn+sJ+6jhex5+X20x9JYM3gwhVVVfB8ZzjMf\nLGD3C++wa/lqVvfuTXV9PcE3XQtA1qKfgYZfvh5bt84SS4CzM0mr1uNRVQaXXQZAtI8PAFV/g58r\nIU5H5jBEp9mdnY1JVXl78mSe2bCBuLw8Ru9NYMiO39l+28P0HjsK0pLYn5fHjUuW8MKddzPl1WeI\n+3gRAx+4xXKd+CaFOxOKivj96FHK9HrLsfv0KlPnz8e/spI8J2fig3qwO8QWbX09PjZOjIvdTMqN\nt+J3cHunPOcR82jfQF9fdmRlXfAjbcsTE7lhyRJuGzAAgAP33Qdrt9D36AF2PPgsATI1ChyfHk0s\nKiLE1ZXRwcFMjojgi337iPT0pHbhIlymTebK2dcAsP36+xh11yygYQG/h70927OyLNPPikbD4Ifv\ngIfvoCwrj14BPiiaE//e7dWid6fW2oqUwWPoGfsXxnqDZfraYDJx9y+/4O3oyNrZs4n28WH3c28x\nvLqM7Ef+z3K+oii8f9llxObmciA/n+fGjuX1iRMbXuwZYXmft4MDSSUllNfW4h/Tn2GjRzeMju/+\ni+r6egKHDCDdNxSndWuBF/lffDxHS0t5afx4EouKiPLxYf+3z1GnsSLyjusBcLWz4+kxY5gUcfw+\nQvxdyb+uotPsMO8kGxkUxFdXXsmOw4n0mXkdx/x6MOTD10mubCjK+mdGBjuzs4mfPJ6YBX7Yv/0m\n6j9vsnwgxRcU4GFvj5ONDYlFRRRWV9Pfx4e00lJG7N/PrJ/fIN/JiRF3Xc/4667j9UmTAAh77z3+\nERyMx5L1DP9+ATlxCQQM7Nshz1ZaU4OiKChgWWQe7euLjcFA8op1+N80s0Pucy76ct8+9AYDn+/b\nR6SHB72dXcl98jGyPAMY+MqT3R3eOaNxejSxqIg+Xl5oNRqWX3897+7Ywaz+/Qlzdyfzz+3krlqH\n++gRjJp6seVcRVEYHhjIyiNHMKqqZaNHI9cgv1PeO8DZmfiCAowmE9rGxG76dNy3riXxl/X0mTkZ\ngN+PHmV/fj7fz5xJjK8vqsmE95efkBoQQf8br2j+PFota2bPJqWkBEcbmxPe18vBwdJDtDFmS3Fd\n8y8zORdfyrCfvqA0PYfVyckEODvzzEUXoSgKqslEyMa1JEQNZ4C/t+W6r5hrzwnxdyfTo6JT1BmN\nLE1MpLenJx729gz082PAZ//Dp6wI/YcfY+Ngh7djwzqcxpG0Y5WVZN7xT3qnxXPw++PNsw8VFtLf\n25u+Xl4kFBWRrtMRYW3Dst+3sfr77ynw9GfLV9+wKyiIEUHHp4pCXF3JKCsj9IkHAUif92GHPd+s\nn3/mlmXLeHDtWr6Ki2OQnx+e9va8tGEDY2++mr3zPuuwe50rdHo9i+Pj+e1oQy9Lg8nEyKAg9t50\nP6H56RS//EaztVV/d41JS3ZFBeHmBf52VlY8OWaMZcF/8MhBDH/5cSKbJGyNxgQHWxqlB7dI2k7n\ntgEDSNPp+Ndvv/GdeUdz5E1XY1A0lP60zPK+xfHxuNracnW/fgDsX7CIsNxUiu954ISjeF4ODowM\nOnknAm9HR0t7usbnt7dqGBtorKXoc+fNWKkmkhZ8w8a0NMb36GHZsXpk5R8ElOZRd8WMM3peIf4u\nJGkTneLxdevYk5PD8+aaURUFxfRf9i1xwydYfsv3tLdH4fiatVSdjv7PPkyeoxNVL78ANOxOjMvL\nY5CfH1E+PhwuLMQ9Pp7X//0Y/1i/kh1X30Fg8kFunjaZb2fMYLp53RAcT9r8Y/qQGDkA703r6Chx\neXnsz88nvrCQ8T16sOOuu7CuredO8+7R8Of+TXl+UYfd71zw3IYNzPr5Z+qMRoYFBAAwMjmDkcu+\nZueU66RxdwvjevSwJCyNSduZuNVcJBcg2NX1jM6d2bcv4e7uvL9rFzcvW0ad0YhroA9Heg7Ad8sf\nANQaDCxPTGRG377YaLWoJhMOb71GjrsfA5+4/4zjBSw13qD1SFtj0uY4cgDpXoHYLf0f+VVVjOvR\nw3JOzUuvUm7nRJ8Hbz+r+wtxoZOkTXQ4VVX5/uBBZvXvzw3RDQuZ4198G5faKpz/84zlfVqNBg97\ne7LNv5mnlpaSY6znnVEjGXXkAMlrNpNcUkJ1fT2D/f0Z6OfHkKNH+f3LL7E2mkj8YQUjf/4CW0cH\nrDQaboqJwarJ6ECIiwtZ5eUYTSbKRowhPCuZsuz2t80q0+sprK4ms6yMI0VF9Pf2bvjQ+2EZnjU1\nrLnhbtxqKkj6/Id23+tcUW808mN8PBeHhvLFFVfw+sSJ2KkqU95+h1w3X2J++Ly7Qzzn2Ftbc0l4\nOHB2SVuQi4sl6TvTkTatRsPia67huv79AdiX29CJoGLiZMJzjpIdG8/ShATKamuZZX7P3nmf0is9\ngaz7/w9rO9szjheaJ22N5TtstFo0imJJ2p7ZuJFFA/szIHk//QoKGG9O2lJ+3cygvZuJv+ZWnH08\nz+r+QlzoJGkTZ6y8tpY5q1dbpkFaStPpKKyutvxjrJpM+P38PYnh0URe3rzoauMUKTRUZI8vKGDB\n0KGU2tlT8/gTxJpruw3y9yfK3YPPV64k39GR7YsX0+/6U1fdD3F1xaiq5FZW4jJ1EhpUUpeuaceT\nN2hs0aPSsKOtsTp9n58Xk+Dlhfb5Ryl09kCzcsUprnJ++TUlhaLqav49ahR3DBrEhLAwfteZCM9P\nJ+/F1//2zeFPZnqvXgD08jy7JCRhzhw+nTYNd3PttTMxJCCAdyc3jGpvy2xoYdXjn3dgVDRkzPuA\nN7dto4+XF5dGRKAvryTw1ec5GtCTIS/8+6xiheM/z76OjpaNGIqi4GBtbUnaMsvLeXvQAKqsbXhq\n23bC3d2p0VVgfdutFDp70O+1Z8/6/kJc6CRpE2fsj9RUPtqzh6GfforR3BC7qaYbEACS12yiR/4x\nymfNbvVerya/mdcajaxPS6PCzo53Jk0mJn4n2V9/j61WS18vL/QfL6JPcTGPXXopvQf0P22cIeYp\npYyyMnpeMRG9lQ2169af1TM31VgrrlFPDw9Sft1M//QjLBg6lHoFUkdfQp99W9FXXBhtrbakp2Or\n1TKlZ0/APN39+fvE9x7CwDm3nObsv687Bg3i19mzGeh36o0DJxPq5sbdQ4ac9f39nZ0Jc3Njqzlp\n84uK5FD0SMJWL+ZgdjaPjR6NRlHY9/Cz+OkK0M+d167CyI0jbYEtRgabJm0FVVUUOzry5dCh3LA/\njvjvlnN4ytUE52eQ9/6np91kIcTfmSRt4ow1lrjIraxk6e8bWk05bs/MxNHamv7m+kqlH36K3sqG\nPg/f2epa3i36CTb2aJw/fAhHAyO5Z8H7TNXXUnY0gz7vvMKu4FCW9+lDqLkI6Kk0TdpsHR1I6RmN\nd+yOM3/gFk6UtJXMfZ9qa1sWDhhATX09djOuwqFeT/KS9o/snQuKamrwdnS01B479MhzeFSVYTPv\nrZOWnRBgpdEw2ZzodpfRwcGWnqAAZbfegl95CU8dPsJNMTGkbdrJwO8+Zu/wifS/4YpTXOn0Gkfa\nWu52bZm0ATw97mLSPH2IumUmQ3auY9e9jxF929Xtur8QFzr511acVGJREZ/FxrZqHZVYXMzAUh1f\nL1vGtZdNwjnIjx1zngYaNg6sT0tjWGAgVhoNtVXV9Nm0mkPDJ+Di69XqHo1JWz/vhu396eaq5zqj\nkT/nz0NvpeV/r72G9YAY7OprWfzvx/BzdsbF9vRrbhoXb6frdABUjLyIiJwUdBm5Z/lfpEFKSQl+\nTk642dmhAB419URvXk3smMmU2dujNxiIuPoyDIqGyt82tOte54ri6mpLe6TCI6kM+OlLYkdNbjXd\nLc49/b29yamooLKuoUXcT32CWR8WxmOrVhF7/5M4XDWdSntHgr7+uN33avx5DnJuPl3emLSZVJVC\nc9HfSltbfnnnPbbf8iCxb33CiI9ea/f9hbjQSZ02cVJPr1/PssRE3tu5k6133IGrnR2p67dx+6NP\nMO5oErVaLd9NnEpUYTnDFrzBoVHD2RjpT0JREc+OHQtA/MeLGKyvxPbOE+8Ga/zNfFRQEKGurqxN\nScHOygq9wcDtB/Yy9okneW3fUax0Ohzvvp1nrprEPU0qvZ+Ki60tzjY2lrV3blMnwddRsl8zAAAg\nAElEQVTvkrZ0DYP+r/WoX1uoqkp8YSE9PTyoqa+nqLqao3MXMNJQi+3DcyBuOzUGA05e7iSF9MZt\n97azuk93y6usRAF8nZwAKK6pwdPBAdVkImv2nfQzmfCbP697gxRt0lhe5JhOh7+TE1/GxaE+/Ag9\nX53LqM/mkuEdTN2339Gzb/uL13qdZHrUycaG8tpadHo9hiZLKvqMGMyom2a1+75C/F1I0iZOKi4v\nDxutlsSiIv65Zg13xSYx+o3ncLK14bOrruO7kUNwDg7if5Onkts7Cpd/PcS8xx5kQlgYN0RFAWD9\nxWcUuHjR7+YTF5tt/M3c19GReZdeyr2rVjHQz4+n1jesPZs1ZTKjnxvW7JwzWZTt6eBAibl7QsTl\nE6i2tqVu3Xo4y6Tt/Z072ZWdzeuXXIKngwNlNTUEvnoD/9/efcdnWd3/H3+d7EEG2YxAQsLee4qA\nyFQB9ygu6vi59euso7WOtlZxVG2l1jqwIlYUB8oqKMoKG0IChBVGBiGbDDLO74/ciQEZAe4s8n4+\nHnnkvq/7Otd1Lg94f/ictS26KzEXD4MNK6p2RDjcfwh953xAYXZeoxuof/Vnn1FUWsrq224DIKOg\ngJ7h4ax57g36r13KyjseZVDf048rlPpXuZ/o7qws/rVuHQUlJdx53TW0ufduUuOTiOwa67Qu7raB\ngdzYs2fVBIxKwd7epB05UtU1WqlzaCgiUnPqHm3CUvPzj/lXb3UZBQXszs7m+ZEjeXr4cJr9+9+M\nfPEJlrWJpOvdd3P0if8jKKotu7Oz8Q0OJOO5P9Hm0D6mLF7MmHbtMMawbe5CuifEsev6aScd3FyZ\naQv19SXAy4tZV17JVY6FPoGqge9nK8jbu2qBUg8fL3a270nYurMf1/byihWMio7mkaFD+W2fPozd\nmUbkoX3k3nIb3pUrvzvG7viMHoVHeSk7v150Ts9Q146WlbHqwAHiDh6sGr93uKCAThu30v35x4hv\n35t+rz9Xz7WUmqrMtM1JTOSN1au5s29fuoVVbIEV0b2DU8ckurm48MHkyXR37MNbKcTHh8MFBVVB\nW1RgID7u7lXjTkWkZhS0NVE7MzOJfv11nv/xRwDStu5g1cPPsfLep0ic8z2rkpMB6N+qFWN/XM/f\nv/mWb9q3Z+INN5Dh60uH4GCiAwPZk52NtZaed9zA2i79+MPSpbQtLqG0+CiljzxGtrcf3Z577KT1\nqMy0hVVb+qN618rZrG9VXbC3N4erdafmD7mA6NTdZO7ef4pSJ5dRUEDfFi1wcazgfvTNt8jy8af7\nA7/Fy7GmVmWmrd2UsZQZF/K/b1xB26a0NI6WlQHwWXw8+ZnZPDNnDn944VkOBYYRMX8ubp4n3sZI\nGp5QHx983N15f8MGvN3c+FPlnqF1KNjbm8OFhVVB29/Gj+fb66+v+nskIjWj7tEm6rFFiygqLeXt\nuDiuL3Mn4pJxDCzKr/jwTfBpHsLz3Trjs3AVA37+jvWx3XnlgTvZfv31fJeUxMjoaBIyMihwjOsK\n9fVly5OP0WPqNXT/7d1sCmtJnx3rWf3MKwwIOXng1TMigt4REQxs1arqWGXwc3nnc98nNNjHhz2O\niQgAzSeOgXdfYffn8wh6+PYzulZhSQlFpaU09/ICID1hJz3WLCXuilsY7Ni+ycPVtWqPRb+wYHZE\ndsC/kY1rW+1YGy86MJCPNm5k3F2Pct+WVcwdNZHRs2fiG3z6mbvScBhjaNe8OVvS0xkZHU2g489v\nXQr28SH/6FH2OSYa9WvZkgjHeEkRqTll2pqghTt38nlCAhe3a0d2bi7eV02hyN2TXYt/5vCuZJb/\n8VX2+TXjyWXL6Bq3lJWTbqTLhpUsuesu2gYGcme/fri5uBBVOVbGERQltwrlxilTiExJpsu6Zay8\n/i4GPPvQKesS5uvLujvuIMaxQG2lwiefZPaVV57zswZ5eXHY0T0KEDPuQo54eFO6+MzXa8tyjI2r\nHFO388VXcbGWNo8/WHVO5SSKSof7DSZ21xaKcvPP9hGAin0+39+wgZKyMrZ/vZjtUV3Y3rYLK6be\ny96f12JP0s19NuIOHiTM15dnR4xg+Pff03vLSu4dP57c6S8oYGukKse1jYs598kGZ6NygkKCY7mg\nkOOW+hGRmlHQ1sQUlZZy57ff0iE4mLnXXsttCYlEZh0idfqb3Ju6h/vWxzG7fycuuPVmViTtxCMn\ni0FffoCn76//JxtVbYAzwI7MTH4aOhT31BTK09MZ9PHZb9Du5eaGqxPG2gT7+JBVWEi5tQC4e3mS\n1Kk3kat+pLy07IyuleUI/pp7eZGzP5Wu//2ATb2G0aragHxvN7eqMW0AXqNH4lFWys6vz21R3/9u\n3cotc+cy84NPiJoyDv+cw5R6ejJ45pu0HdaPwwEhrJp4PRnbd5/TfT7fupVPt2zhgjZtmBLdjmd+\n+JEf20bx5oABBOuLttGqCtrqac24yuViEjIyCPb2Pma7ORGpOf3NaWLmJCSwKyuL18aOxcvVlft/\nWMbGiJZE3TCJ+UlJ/GfzZt6Oi+PW3r0ZHNPulHsQdggOxs3FhY1paUBF0NY+KAjvQD98mjeMAcZB\n3t5YINuRJQMouf43tMxKZcv7n520XMKhQ0S8/DI/JSez6d1ZxI24jMzLruPC3bsJ8vYm8Y4HaVZ0\nBP+X/3xMOS83N4rKfgkGoy8bSzmG3EVLzuk5vkhMxLWsjL5PP0meVzM81q2lY+Jaxj37R/59+33s\n7dafPt/PxqNnD+Kee/2sMm+p+fn85osv6BkRwZsTJrD1lX/SMj+PFy4YBsZUffFK4/PbPn34y+jR\nv8po15XKgH/roUPHjF8VkTOjoK2JSMrMpM2rr/LowoVEBwYyNjaWrZ9+Q4eMNF4ZNIBvk5Ioc2Sj\nAJ52rLN2Kl5ubvQID2f1gQOUlJWRmJFB+3r6UjiZykAjs1oXaY97b+GwbyD2zTdPWu7hhQs5lJfH\nwRtvo8dt19F+9VK6bFnD/JkzCZhyAwPnzSJuwrW0u2jIMeW83d2PybQFtApjb4tofOPObMaqtZaC\nkhJW7d9P+7/9jW+2b+f6zZvpcTCZ3U+/SFB0a5YlJzPfljN7+AD6rphP6vK1HGjVjv7PPMC6IeM4\nWlB0+htV85effqKkrIyPpkwh3MeHsBl/Y3uLtixwdKkp09Z4dQ8P59GhQ+vt/pXdoZmFhfUWOIqc\nDxS0NRFb0tPZl5vLgbw8pvXujYsxFPx9Brme3vy3SxdeWr4cdxcXXr74YqaPHVujbaIABrRsSdzB\ng8zasoXsoiIu7dixlp/kzAQ5grbqM0g9fLzYftVN9Nz4M/Ef/7Kpe7m1bExNZV1KCgsSE5k55wuu\n/mEBKyffhG/mIb788kv+Fx1Nm+SdrLz0N/T78sNf3e/4MW0A6T37E7NjE6XFR2tc79dXrcL3xRd5\naflykjIzKSou5plly9kQHk6zWyq2+pkdHw/AupQUrLVEDuxJ+61rWDHtIfquWkj8BeNrHLhZa3lv\nwwau7daN2KAgNv1zFlGpe0i77R5wzPBTpk3OVvU/O11Cfr0ziojUjIK2JqIy03TvgAHcO3AguWkZ\ndFuxiC3DJ1Do4cGG1FQGtm7N/w0Zwj0DBtT4ugNatSK3uJhbv/qKLqGhTGjfvrYe4awEV/sXfnW9\nX3+eg0Et8HvwXvLSDwPw6ZYt9HrnHf66dCmz/vtfrtuymcdGj6bDf/6Ou5cnec28mfCb3+CRksyg\nrz464dpz3m5uVbNHK7kOvwDfo4XsWbKixvWeu20bUNGdPbF9e+a4+hF7OJ3nhw/ny+3byS0u5r9b\nt+Lh6kr6kSOk5FdMdLjtu3m8MmYgqx78A73XLSV+eM0Ct4N5eeQWFzMkMhIAt1deJi0glEFP3Ft1\nTkA9zDqU80P1LG0XLagrctYUtDURlUHLC6NG4e/pScIr7+BdWkzo/XdzW58+jIuN5fcXXnjG1x3g\nWKqjtLycF0eNanDrLgWdoHsUwMu/GZmvv02LwwfZOWoCCxISWbx7N54lJdzw+99zRUICX9/2IC8N\nG8aiXbuAiokIBgg4xb6nJ8q0RU4aA0DG1/NrXG8/j1/WQbuic2diPplFamAYqRdfzMxNm7jr2285\nXFjI8yMr9v6csXYte7Oz+XHvXuIOHmTg9N+z8v5n6L12KVuHjqEgK+eU99t2uCJw7RgcTOKc+XTd\nsZ7dU2/H3cuTno6FUhta20rj4eXmhq9j8WkFbSJnT+u0NRGZhYW4ubjQzBEMBH06k10tY4gdfyEz\nzmEmV+fQUP44YgQXRkUxvG1bZ1XXaSq7Zaov+/H4okWsT03luxtuYE3CCwx48XHev+5akoYMYe6C\nBYzduZPpU2/l3r//leg33+T5H3/kis6dySoqIsDL65SzWr3d3cnNP3Z5j/Au7dkTHoXv0prPIE12\nrGfl7+lJn8wCuiWuZeW0h7ihVy/umjeP+EOH+OOIEdzZrx+PLlrEsz/8wLwdO9idlYWLMZSVlzPo\ntWdZ5eZGv+l/4EDH7hy84VZaTJlI2+H9f3W/bY6lGDqGhJDx3M1k+fjT44+PALB82jRyis5sfJzI\n8YJ9fDiSk0MndY+KnDVl2pqIwwUFBHl7Y4xh1+LltE9OJP2qG855CxsXY3j6wgsbZMAGEOjlheHY\nTNvs+HgW7NzJrC1bGPDCY/xz7CXcvHEjS//+d0bv2sUtkyYR8n/34e7qyl9Gj2Zzejp/X7OGzMLC\nqoV1T+ZEmTaA1EHD6bBtPYXZeTWq996cHO7u35/Djz5KwXsfU2ZciH38Xq7p1o1BrVszfcwYnho+\nHD9PT6b26AFUrK9mgTJrq7pLB778NIten0G+q2XQa8/S9sIBxHfqx5aPvjhmhmliRga+7u4cXbOF\nXhuWkXj1LVUzgH3c3Wnh17j2TpWGJ9jbm0h/f/xOkakWkVNT0NZEZBYVVXUVpr/2Nkdd3ej00J31\nXKva5+riQqCXV9X2OSl5eezOzsbFGP6wdCkAb02eSOe77+aqq67is6+/o2jqVC5xbHh9ZZcujIuN\n5ZGFC/l5377TblZ/ojFtAL6XTcSzrITtn3592jrnFheTXVREm4AAXIHWC74moVMfQmKjCPL2ZsW0\naTw4eDDG0V354ZQpvHPJJcdco3LleYC57SLoesc0ln73P1be9QRhB3fT7cbL2R7bg/Vv/JuyklK2\nHT5Mx+Bgsh9/miMe3nR5/nenrafImbgoOtopu5yINGUK2pqIzMJCgr29Kco7QqfFX7G5/ygC27So\n72rViV4RESxz7KX68759ANzQvTs7MjPZfvgwiRkZ9Bk1ir733cfVE8bwyRVXVAW4xhjenzQJLzc3\n9mRnn3Wmrf1VE8n19KXkw49OW9+9jh0m2gYEsHPBMiIz9lMw+dS7Qwyotg0Y/NK9CrBkzx4whsQQ\nPwa99SJ+B5JZ9cjz+GUfpvf9t5LSIorJb/+Dl197kz5x/2PT9bcT0CrstPUUORN/HTOG18aNq+9q\niDRqCtqaiMzCQoK8vdn82rsEFubhcceZ7bvZmF3SoQNb0tPZk53Nz8nJeLm58ciQivXV/rl2LcVl\nZYyOjubxYcNOONg+vFmzqi5I+6tPj+V9kqDNy8+XhIsuo8eqRWTtPXjKa+x1BFxtAwM59K+PKHFx\npcOdN56yTNfQULzd3KrWw9qXm8vqAwd4bOFCEh3j1Vbs319Vl4EvPUlYyl7W/vUdsoJCuDFuFd32\nJbPi1gcZ9K/pp3lKERGpDwramojKoC3g3XfYG9aGbjdOqe8q1ZnKrs7bv/6at+LiGBkVRbewMFr7\n+/PG6tVAxYSKU5nWuzcAOxyzLE/Gy82NzMJCXl2xgrLjdiUIe+R+PMpKSXzqT6e8RmWWrI2fH1GL\nvmFr1wGnzYq6u7oyul07xsfG4u/pyaJduxg7cyYvLa/YrL5NQAArHUFbJTdPD/o+fDvrZr+P71NP\nsW9HIoP/Nf2cxzmKiEjtqLX/OxtjvIwxq40xG40x8caYZx3Hg4wxC40xOxy/m5+k/DhjzDZjTJIx\n5vHaqmdTcbiggOjNCXRITiD1hlub1Bdzh+BgeoSHs3DXLsa3b8/Myy/HGMOYdu046thyqvNpZrT1\njIjgyQsu4IPJk095nrdjWYOHFizg2x07jvksesRA1g68mD6fvMO+letPeo2f9+3Dz8ODnP+tpEV2\nGkevuKomj8kX11zD+5MnE+Ljw/ydO3FzceHhwYMZFR3NbX36sP3wYVLzf71x/Tc7dtDSz4/eERE1\nuo+IiNSP2lzyoxgYZa3NN8a4Az8ZY74DLgcWW2v/7AjGHgceq17QGOMKvAVcDOwH4owxX1lrt9Zi\nfc9bxaWlHCkpYfxHs8lo1pzuTz9Q31Wqc8tvvZUya/GvNnPtxYsuont4OD7u7jVaOPb5UaNOe45r\nte7Vd9et47KOHSkpK+OJxYt5YNAg2s6cQVH37rhOnMjBxYtp2evYgdl7s7P5dMsW7h84kLyX/0mh\nmycd75xao2esXIrkyNGKnRc+mjKlaoPwbRkZPLNkCW+uXs0N3btXZRaLSkuZn5TEtd26VU1sEBGR\nhqnWgjZrrQUq/1nv7vixwCRghOP4B8BSjgvagAFAkrV2F4AxZpajnIK2s5BVVMTFSUkM2LmVlfc/\nw6AGspl7XfKttlhtpfBmzXhg0CCn3qeya7NTSAjzduwgJS+PA3l5vLJiBV5ubjw/ahTbP5tL+FWT\nKBpxIfvmLyRyYM+q8q+tXIkxhttiOtDmh2/YPHwCA8LPbF2rWVdeye6srKqADSrWX7usY0deWLaM\nF5Yt491LLyXQy4uU/Hzyjh7l2m7dnPMfQEREak2tLq7ryJitBWKBt6y1q4wx4dbaFMcpqUD4CYq2\nAvZVe78fGFibdT2fpWVl8/r337M3KJzeLxwfH4sz/e6CC2jp58fVXbvS4x//4MvExKqZqN9s387z\no0bR4ZJR7Pzqe5pPmUjJxInkxm/APzyErMJC/rluHdd160b29Bl0Kikm6JH7z7gOI6KiGBEV9avj\nz44YQfqRI+QWF/Pbr39ZeqRzSAgjT3C+iIg0LLU6sMlaW2at7QW0BgYYY7od97nl9BPyTskYc7sx\nZo0xZs2hQ4fO5VLnrb3T/0HnjAziHnoUT1+f0xeQsxYTFMRzo0bRLSyM9kFBfJGYyK6sLAA2pqVV\nrZ8Wc/FQUv/1MRGZKSRdcjUzN25k+Pvvc6SkhP/XNoYu7/2NTd0GEzvuzLcWO5meEREsnzaNz666\nit4RETw4aBCerq48OnSoukZFRBqBOhmNbq3NBpYA44A0Y0wLAMfv9BMUOQBEVnvf2nHsRNeeYa3t\nZ63tF6o97X5lya5dRH/yLhvDw4m95dr6rk6TYYxhSqdOLNmzh7UpKbg7xpt9vHlz1TldrpnImrse\np8+aJRz8/R/Zl5PDY0OG4HHPo/gcLSTgrddrpW6dQ0NZd8cdTB87luzHH+fmXr1q5T4iIuJctTl7\nNNQYE+h47U3FpIJE4CvgJsdpNwFzT1A8DmhvjIk2xngA1zrKyRkoLS/nnemv0T0tjS1XTqVni6ax\nmG5DMblTJ0rLy5m7bRt9W7ZkfGwsf/7pp6rMG8DAN55j3YCLeHjuF9y7YzeTZn5D3xXzibvl/hPu\nEepsXm7aflhEpLGozUxbC2CJMWYTFUHYQmvtN8CfgYuNMTuA0Y73GGNaGmPmAVhrS4F7gPlAAjDb\nWhtfi3U9L/3lp58YteJn8j28uPz5Z9QFVscGtGpFcy8vSsvLade8OS+PGcORkhJi3niDt+PiADAu\nLnSe/wVrW7XiufdmMPjDN1gzdDwD3/lrPddeREQamtqcPboJ6H2C44eBi05w/CAwodr7ecC82qrf\n+SwpM5PfL13K7A0bSE3YRmLfC+kXqA2/65qriwtjYmL4ND6edoGBdAkNZcMddzD6o4/4ed8+7upf\nkUnzCmjGqFtv4blDuQxrHUPfJ+5uUuvoiYhIzeib4Tz04Pz5fJmYyKOFpQQXHsH12mvqu0pNVuWy\nG+2aV6wh3TUsjC6hoezMzKw6J7e4mHxXV8qmXk2/J+9VwCYiIiekb4fzzKEjR/g+KYl7+vfn0sRk\nCt086XxzzVbUF+eb3KkT13TtypiYmKpjMc2bs7PauLZDBQUAhPr61nn9RESk8dAo5PPMp/HxlJaX\nM7VnT/xWL2NHpz708G9W39VqsgK9vJh15ZXHHItp3pyMggJyi4vx9/Tk0JEjAIT6aDkWERE5OWXa\nzjNfJCbSNTSUkLRs2qYnU3DhyPqukhwnJigIoGoWqTJtIiJSE8q01YPctAzyUw4R0iEaD5/T73lZ\n4+sWF7Ns714eGjyY5E+/JAIIv/Iyp11fnCPGMb5tZ2Ymb61ezdfbtwPKtImIyKkp01aHdi5YxoZe\nF+DTIpyWvbuQHxbBijsepSjviFOuv3jXLkrKyxkfG4vrokUc8gsiqg7W+pIzU5lpS8rMZPbWraQ5\nukdDFLSJiMgpKGirI1tmfknrCRcRtX0jcVdNY/WTf2F/dBcGz/grabFd2Ldy/TnfY96OHfh7ejIw\nogUxm1ayp89QzURsgPw9PQnz9eW7pCRyi4urjp9oU3sREZFK+kavAwc3JND2tzeQEtIKm5DI4E9n\nMOD5R+mxeTmb3p2Ff34OvqNHkrZ1x1nfw1rLvKQkxsTEkLzoZwIL8zBjxzjxKcSZhkZG8sPevfVd\nDRERaUQUtNUyW15O1jW/wVjw+O5bmrdtecznPaZdQ+78RXiWHCV74mSKjxSc1X02paVxMC+PCbGx\nHP7iGwCir510zvWX2jEyKqq+qyAiIo2MgrZatuGtD+m6fR3xDz1Ny95dTnhO22H92P6Xv9Fxz1Y2\nTppKeWlZ1WclRcVs+XAOJUXFJyxbad6OiizduNhYApYtIal1e4KjI533IOJUI6OjgYpJCR9MnswH\nkyfXc41ERKSh0+zRWmTLy/H/y4vsC2lN3z88dMpzez8wjRWr4hg86x3iu/Qnt+8ATGYmwZtW0i11\nH+tfv5Cuy74/6WzTH5OT6R4Whl9xKSE7N7PmiluIrY2HEqfoGhpKuK8vvSIiuLFnz/qujoiINALK\ntNWirZ98TcyBHaTc9SBunqcfZD7o47dZ9chzBB06yMBZM2j/03fkUcJ3oy6l97of2PD86ycteyA3\nl+jmzUn69Bvcy8vwu3TCSc+V+meMYcHUqbw6dmx9V0VERBoJBW21qODd98j38GbBoC78nJxMWXk5\nd3z9NX1nzOCdNWt+db5xcWHgS0/RIisVU1bKfR99QN877yT95WfZH9wSr6++OOm9UvLzadGsGcXf\nfEuhmyftL1cw0ND1CA8nMiCgvqshIiKNhLpHa0lBVg5dli/km169eXr1Kli9ir4tWrA2JYVWfn48\ntmgR13fvjp+n5wnLGxcXMgsLAQjw9mbfyPH0m/M+OQfSCWgVdsy5R8vKyCgooIW3D9HLF5PYYzC9\nfbXml4iIyPlEmbZasOPwYR545FF8jxYyo1MsL4waxe+GDWNzejrXd+/O51dfTU5xMf9ct+6U18ly\nBG0lZWUE33gd7uVl7Pj3rF+dl5afD0B4/A7CcjMo1aB2ERGR844ybbXg7bg4emxaT767ByujopjT\nvz8BXl48PGQI/p6euLq4MLh1az7atImHBg8+6XWyiooAKCwtJXb8CDJ9A2D+AnjqvmPOS3EEba0X\nLqHExZUOt15Xew8nIiIi9UKZNicpKy/nQG4uxaWlfLRpE2N27mRpVFv6RUcT4FUx47O5tzeujh0K\nJrRvz4bUVK7+7DMmzarIns1JSODqzz6j3Frgl0xbYUkJLm6u7Oo1hKgNy49ZEgQgNT8fj9JSBi35\nnvgeQ37VfSoiIiKNn4I2J/lkyxZav/oqQS+9RLODB+mQmcmCmBjGxcSc8PzR7doB8NnWrSzatYu0\n/Hx++9VXfLZ1Kyv27QOOzbQBlF80mpD8LHYvWXnMtVLy8rg6Pp6QIzm43n9vbT2iiIiI1CMFbU4S\nn54OwKSOHfmLSzMA3EZdzE29ep3w/H4tW+LvmIRQUFLCvd99R97Ro3i6ujI7Pp6jZWWUlpcDFZk2\ngCjHDgfps4+dRXowM4vfLVvGnrC2dLvxCuc/nIiIiNQ7BW1Osi83l6jAQP5zxRVEJewg38OHvz5w\nHy39/E54vpuLCzf37EmviAigomt0aGQkE9q359P4eH5OTq46t8iRaQvrHMO26K6EzZvLCz/+yE+O\nc8Le/YDOGRlkP/OcNogXERE5T+kb3kn25ebS2t8fgOAt69kd0xVX91PP83h9/HgWTZ0KQJm1DI2M\n5IFBgzhSUsKoDz+sOq+yexQg67IriDmYxMzZs7ll7lwSvlrEtDmfsqBDZ3r+vxtq4clERESkIVDQ\n5iT7c3OJ9PenICuHqAM7ye/dr0blgn18CPf1BWBomzYMb9uWxLvvxsWYqnMqu0cBYu+6mTJjePTn\nnxmyZx8h100htZkvsx55WFk2ERGR85i+5Z2g3NqqoG339z/gZsvxuWBIjct3Dg0FYFDr1gC08vfn\n0COP8N5ll9GiWbNjMm3+MW14aegwbtmwgQ+m/5UcTw/+9NTveeeWm5z7UCIiItKgaJ02Jzh05AhH\ny8qIDAgg76sfAGh18fAal5/SqROhPj4EeXtXHQvy9uaW3r15YdmyY4K2/bm5PD1yBKFFhaT6+vLy\noEE8NWQg7q6uznsgERERaXAUtDnB/txcAFr7++O6aSOH/IIIjWlT4/L3DRzIfQMHnvAzLze3qokI\nAHuzsylzdWX2ffexcNcuAKIDA8+h9iIiItIYqHvUCfY5grZIf3+CkxI4GNXRadf2dnc/Zkxbck4O\nAEMiI6uORTdv7rT7iYiISMOkoM0J9jkCqTA3dyJT91DQpbvTru3t5nZM92hl0FY5/g2UaRMREWkK\nFLQ5wcG8PNxcXChcl4B7eRkefXs77donyrSF+/oS48iuBXp50bzaWDgRERE5P07S7FEAAAyDSURB\nVCloc4LUI0eIaNaM7OVxAIRdcOLxaWfjV5m23FzaBATQyrEmnLJsIiIiTYOCNidIycsjolkzytev\np8Ddi5Z9ndc9evxEhOScHNoEBODj7k5zLy+NZxMREWkiNHvUCVLz82kTEID/tniSW8fS6TQ7IZyJ\n6t2j1lqSc3IYHxsLwKtjx9I+ONhp9xIREZGGS5k2J0jNzyfc15fI5O3kdOzq1GtX7x7NLCykoKSE\nSEfX6E29eh0zi1RERETOXwrazlFpeTnpR44QmZGNX3EBprfzJiGAI2hzZNoO5OUBVO1xKiIiIk2H\ngrZzdOjIESwQsWM3AIFD+jv1+t7u7lVj2g441oNrpaBNRESkyVHQdo5S8vMBaBGfwFEXN9oMH+DU\n63u5uVFmLSVlZcfsvCAiIiJNi4K2c5TqCNraxm9hd5sOePk3c+r1vd0qJjUUlpZyIC8PA7Ro5tx7\niIiISMOnoO0cZBUWsnDnTjxKS+m4dztZvZzbNQoV3aMAhSUlHMjNJczXV5vDi4iINEFa8uMcPLNk\nCW/GxTEoJQXPshI8L7zA6fc4PtOm8WwiIiJNkzJt52BTejoAN2cXABB56cVOv0dlpq2oMmjz83P6\nPURERKThU9B2Dg7m5XFN165csCmBnS1jCYlp4/R7eFVm2kpK2J+bq0kIIiIiTZSCtrNUbi17s7Np\nV2bpmLSJ9BHOz7LBL92jWUVFZBYWKtMmIiLSRCloO0speXmUlJcTu2INrrac4GuvqJX7VHaPJmVm\nAlqjTUREpKlS0HaWdmdnA9B9/kLSAkKJHT+iVu4T5O0NwKa0NABl2kRERJooBW1naU92NrGHD9M/\ncT27plyPi1vtLMNRuc/oiv37AS2sKyIi0lTV2pIfxphI4EMgHLDADGvt68aYT4GOjtMCgWxrba8T\nlN8D5AFlQKm1tl9t1fVs7MnO5uHlyylxcaX9E/fX2n0CvLzw8/BgY2oqoO5RERGRpqo212krBf7P\nWrvOGOMHrDXGLLTWXlN5gjHmFSDnFNcYaa3NqMU6nhVbXo797EvuWLuWlZdNZVCH6Fq9X5uAAOIP\nHaKZhwf+np61ei8RERFpmGotaLPWpgApjtd5xpgEoBWwFcAYY4CrgVG1VQdnO7h+K8nvziTi8094\nOm0P20LC6PnBW7V+30hH0KbxbCIiIk1XneyIYIyJAnoDq6odvgBIs9buOEkxCywyxpQB71hrZ5zk\n2rcDtwO0aeP8ddKqKlNeTvnFYxh0+ADb23TmqeumsuKCwSwOrP1AqnJcm8aziYiINF21PhHBGNMM\n+Bx4wFqbW+2j64BPTlF0mGOs23jgbmPM8BOdZK2dYa3tZ63tFxoa6rR6H8+4uJD75t85sGYLHfZu\nZd6FQ/EJDKy1+1XXJiAA0Hg2ERGRpqxWM23GGHcqAraPrbVzqh13Ay4H+p6srLX2gON3ujHmC2AA\n8GNt1vd0ulx7adXr7KIiutRikFhdZaZN3aMiIiJNV61l2hxj1v4FJFhrpx/38Wgg0Vq7/yRlfR2T\nFzDG+AJjgC21VdezkVNcTKCXV53cK7Iy06agTUREpMmqze7RocBUYJQxZoPjZ4Ljs2s5rmvUGNPS\nGDPP8TYc+MkYsxFYDXxrrf2+Fut6Rqy15BQVEVBHMzl7hIfTITiYIZGRdXI/ERERaXhqc/boT4A5\nyWc3n+DYQWCC4/UuoGdt1e1cFZSUUGYtAXWUaQvx8WHbPffUyb1ERESkYdKOCGchp7gYoM4ybSIi\nIiIK2s5CdlERQJ2NaRMRERFR0HYWchxBW111j4qIiIgoaDsL6h4VERGRulYnOyKcLz7fuhVvd3fy\nKoM2ZdpERESkjihoOwN/+ukngn18uLxTJ0CZNhEREak76h49A51CQkjMyKjqHtVEBBEREakrCtrO\nQOeQEJJzcjiYl4erMfi4u9d3lURERKSJUNB2BjqFhACw+sABAry8qNipS0RERKT2KWg7A5VB28r9\n+zWeTUREROqUgrYzEBsUBIAFQn1967cyIiIi0qQoaDsDnm6/TLb93bBh9VgTERERaWq05McZen3c\nOPKPHmWSY9kPERERkbqgoO0M3TdwYH1XQURERJogdY+KiIiINAIK2kREREQaAQVtIiIiIo2AgjYR\nERGRRkBBm4iIiEgjoKBNREREpBFQ0CYiIiLSCChoExEREWkEFLSJiIiINAIK2kREREQaAQVtIiIi\nIo2AgjYRERGRRkBBm4iIiEgjoKBNREREpBFQ0CYiIiLSCChoExEREWkEFLSJiIiINAIK2kREREQa\nAWOtre86OI0x5hCwt5ZvEwJk1PI95NypnRo+tVHjoHZqHNRODd+J2qittTa0phc4r4K2umCMWWOt\n7Vff9ZBTUzs1fGqjxkHt1DionRo+Z7SRukdFREREGgEFbSIiIiKNgIK2MzejvisgNaJ2avjURo2D\n2qlxUDs1fOfcRhrTJiIiItIIKNMmIiIi0ggoaKshY8w4Y8w2Y0ySMebx+q5PU2aMec8Yk26M2VLt\nWJAxZqExZofjd/Nqnz3haLdtxpix9VPrpsUYE2mMWWKM2WqMiTfG3O84rnZqQIwxXsaY1caYjY52\netZxXO3UABljXI0x640x3zjeq50aGGPMHmPMZmPMBmPMGscxp7WTgrYaMMa4Am8B44EuwHXGmC71\nW6sm7X1g3HHHHgcWW2vbA4sd73G007VAV0eZtx3tKbWrFPg/a20XYBBwt6Mt1E4NSzEwylrbE+gF\njDPGDELt1FDdDyRUe692aphGWmt7VVvew2ntpKCtZgYASdbaXdbao8AsYFI916nJstb+CGQed3gS\n8IHj9QfA5GrHZ1lri621u4EkKtpTapG1NsVau87xOo+KL5pWqJ0aFFsh3/HW3fFjUTs1OMaY1sBE\n4N1qh9VOjYPT2klBW820AvZVe7/fcUwajnBrbYrjdSoQ7nittqtnxpgooDewCrVTg+PoctsApAML\nrbVqp4bpNeBRoLzaMbVTw2OBRcaYtcaY2x3HnNZObs6sqUhDYK21xhhNi24AjDHNgM+BB6y1ucaY\nqs/UTg2DtbYM6GWMCQS+MMZ0O+5ztVM9M8ZcAqRba9caY0ac6By1U4MxzFp7wBgTBiw0xiRW//Bc\n20mZtpo5AERWe9/acUwajjRjTAsAx+90x3G1XT0xxrhTEbB9bK2d4zisdmqgrLXZwBIqxtaonRqW\nocBlxpg9VAzPGWWMmYnaqcGx1h5w/E4HvqCiu9Np7aSgrWbigPbGmGhjjAcVAwe/quc6ybG+Am5y\nvL4JmFvt+LXGGE9jTDTQHlhdD/VrUkxFSu1fQIK1dnq1j9RODYgxJtSRYcMY4w1cDCSidmpQrLVP\nWGtbW2ujqPj++Z+19jeonRoUY4yvMcav8jUwBtiCE9tJ3aM1YK0tNcbcA8wHXIH3rLXx9VytJssY\n8wkwAggxxuwHfg/8GZhtjJkG7AWuBrDWxhtjZgNbqZjReLejO0hq11BgKrDZMV4K4HeonRqaFsAH\njhlrLsBsa+03xpgVqJ0aA/19aljCqRhiABXx1X+std8bY+JwUjtpRwQRERGRRkDdoyIiIiKNgII2\nERERkUZAQZuIiIhII6CgTURERKQRUNAmIiIi0ghoyQ8RaXKMMWXAZir22iwFPgRetdaWn7KgiEg9\nUtAmIk1RobW2F4Bju5n/AP5UrPknItIgqXtURJo0x3YztwP3mApRxphlxph1jp8hAMaYD40xkyvL\nGWM+NsZMMsZ0NcasNsZsMMZsMsa0r69nEZHzmxbXFZEmxxiTb61tdtyxbKAjkAeUW2uLHAHYJ9ba\nfsaYC4EHrbWTjTEBwAYqtp15FVhprf3Ysc2dq7W2sG6fSESaAnWPiogcyx140xjTCygDOgBYa38w\nxrxtjAkFrgA+d2xxtwJ40hjTGphjrd1RbzUXkfOaukdFpMkzxrSjIkBLBx4E0oCeQD/Ao9qpHwK/\nAW4B3gOw1v4HuAwoBOYZY0bVXc1FpClRpk1EmjRH5uwfwJvWWuvo+txvrS03xtwEuFY7/X1gNZBq\nrd3qKN8O2GWtfcMY0wboAfyvTh9CRJoEBW0i0hR5G2M28MuSHx8B0x2fvQ18boy5EfgeOFJZyFqb\nZoxJAL6sdq2rganGmBIgFXixDuovIk2QJiKIiNSQMcaHivXd+lhrc+q7PiLStGhMm4hIDRhjRgMJ\nwN8UsIlIfVCmTURERKQRUKZNREREpBFQ0CYiIiLSCChoExEREWkEFLSJiIiINAIK2kREREQaAQVt\nIiIiIo3A/wf4cxo1y5A48gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1da68c305f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_diff = denormalize(file_csv_name, p_test)\n",
    "train_diff = denormalize(file_csv_name, p_train)\n",
    "diff = np.append(train_diff, test_diff)\n",
    "plt.plot(diff)\n",
    "\n",
    "actual_test = denormalize(file_csv_name, y_test)\n",
    "actual_train = denormalize(file_csv_name, y_train)\n",
    "actual = np.append(actual_train, actual_test)\n",
    "\n",
    "plt.plot(actual, label = 'Actual_prices', color = 'teal')\n",
    "plt.plot(diff, label = 'Predicted_prices', color = 'red')\n",
    "plt2.legend(loc='best')\n",
    "plt2.title('The test result for {}'.format(file_csv_name))\n",
    "plt2.xlabel('Days')\n",
    "plt2.ylabel('Close')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('LSTM_Stock_prediction-20170429.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 22\n",
    "shape = [4, window_size, 1]\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_measure(file_csv_name, window_size, d, shape, neurons, batch_size, epochs):\n",
    "    df = get_stock_price(file_csv_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, window_size)\n",
    "    \n",
    "    model = build_model(shape, neurons, d)\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-c573afe5f162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrainScore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquick_measure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_csv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdropout_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestScore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-220-59050b3bf010>\u001b[0m in \u001b[0;36mquick_measure\u001b[1;34m(file_csv_name, window_size, d, shape, neurons, batch_size, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# model.save('LSTM_Stock_prediction-20170429.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrainScore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1414\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    942\u001b[0m                                              \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 944\u001b[1;33m                                              **self._function_kwargs)\n\u001b[0m\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Invalid argument \"%s\" passed to K.function with Theano backend'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                                         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m   1193\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\compile\\function.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\compile\\pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1792\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1794\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m             defaults)\n\u001b[0;32m   1796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1472\u001b[0m                         optimizer, inputs, outputs)\n\u001b[0;32m   1473\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                     \u001b[0moptimizer_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mnb_nodes_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                 \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mnb_nodes_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                 \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\opt.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\tensor\\blas.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[0mnb_iter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m             \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1434\u001b[1;33m             \u001b[0mnodelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio_toposort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1435\u001b[0m             \u001b[0mtime_toposort\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m             \u001b[0mdid_something\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36mio_toposort\u001b[1;34m(inputs, outputs, orderings, clients)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     topo = general_toposort(outputs, deps=compute_deps,\n\u001b[0;32m   1030\u001b[0m                             \u001b[0mcompute_deps_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_deps_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m                             deps_cache=deps_cache, clients=clients)\n\u001b[0m\u001b[0;32m   1032\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopo\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mApply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36mgeneral_toposort\u001b[1;34m(r_out, deps, debug_print, compute_deps_cache, deps_cache, clients)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     reachable, _clients = stack_search(deque(r_out), compute_deps_cache,\n\u001b[1;32m--> 938\u001b[1;33m                                        'dfs', True)\n\u001b[0m\u001b[0;32m    939\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclients\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[0mclients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36mstack_search\u001b[1;34m(start, expand, mode, build_inv)\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[0mrval_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[0mrval_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mexpand_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mexpand_l\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbuild_inv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cherr\\Anaconda3\\lib\\site-packages\\theano\\gof\\graph.py\u001b[0m in \u001b[0;36mcompute_deps_cache\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1007\u001b[0m                             \u001b[1;34m\"Non-deterministic collections here make\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m                             \" toposort non-deterministic.\")\n\u001b[1;32m-> 1009\u001b[1;33m                     \u001b[0mdeps_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m                     \u001b[0mdeps_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dlist = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "neurons_LSTM = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "dropout_result = {}\n",
    "\n",
    "for d in dlist:    \n",
    "    trainScore, testScore = quick_measure(file_csv_name, window_size, d, shape, neurons, batch_size, epochs)\n",
    "    dropout_result[d] = testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best dropout parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.2: 0.28307063250165238, 0.3: 0.32187784267099279, 0.4: 0.30487769606866333, 0.5: 0.31210789006007345, 0.6: 0.29026408571945994, 0.7: 0.21538778590528587, 0.8: 0.35243767343069377}\n",
      "[0.7]\n"
     ]
    }
   ],
   "source": [
    "min_val = min(dropout_result.values())\n",
    "min_val_key = [k for k, v in dropout_result.items() if v == min_val]\n",
    "print(dropout_result)\n",
    "print(min_val_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAHwCAYAAAD5BSj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a146570ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(dropout_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Dropout')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimial epochs value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochslist = [10,20,30,40,50,60,70,80,90,100]\n",
    "d = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_111 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/10\n",
      "310/310 [==============================] - 34s - loss: 0.1203 - acc: 0.0000e+00 - val_loss: 0.5602 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 8s - loss: 0.1195 - acc: 0.0000e+00 - val_loss: 0.5584 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 7s - loss: 0.1186 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 7s - loss: 0.1177 - acc: 0.0000e+00 - val_loss: 0.5546 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 7s - loss: 0.1169 - acc: 0.0000e+00 - val_loss: 0.5527 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 8s - loss: 0.1161 - acc: 0.0000e+00 - val_loss: 0.5507 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "310/310 [==============================] - 7s - loss: 0.1153 - acc: 0.0000e+00 - val_loss: 0.5485 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "310/310 [==============================] - 7s - loss: 0.1144 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "310/310 [==============================] - 7s - loss: 0.1136 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "310/310 [==============================] - 7s - loss: 0.1126 - acc: 0.0000e+00 - val_loss: 0.5411 - val_acc: 0.0000e+00\n",
      "Train Score: 0.15522 MSE (0.39 RMSE)\n",
      "Test Score: 0.78638 MSE (0.89 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_113 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_114 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/20\n",
      "310/310 [==============================] - 41s - loss: 0.1196 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 7s - loss: 0.1186 - acc: 0.0000e+00 - val_loss: 0.5557 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 7s - loss: 0.1175 - acc: 0.0000e+00 - val_loss: 0.5529 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 7s - loss: 0.1165 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 8s - loss: 0.1153 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 7s - loss: 0.1142 - acc: 0.0000e+00 - val_loss: 0.5425 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 8s - loss: 0.1127 - acc: 0.0000e+00 - val_loss: 0.5382 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 7s - loss: 0.1113 - acc: 0.0000e+00 - val_loss: 0.5335 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 7s - loss: 0.1097 - acc: 0.0000e+00 - val_loss: 0.5282 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 8s - loss: 0.1078 - acc: 0.0000e+00 - val_loss: 0.5222 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 7s - loss: 0.1060 - acc: 0.0000e+00 - val_loss: 0.5154 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 7s - loss: 0.1034 - acc: 0.0000e+00 - val_loss: 0.5078 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 7s - loss: 0.1010 - acc: 0.0000e+00 - val_loss: 0.4992 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 7s - loss: 0.0976 - acc: 0.0000e+00 - val_loss: 0.4896 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 8s - loss: 0.0953 - acc: 0.0000e+00 - val_loss: 0.4789 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 7s - loss: 0.0909 - acc: 0.0000e+00 - val_loss: 0.4671 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 8s - loss: 0.0870 - acc: 0.0000e+00 - val_loss: 0.4542 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 7s - loss: 0.0811 - acc: 0.0000e+00 - val_loss: 0.4402 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 8s - loss: 0.0765 - acc: 0.0000e+00 - val_loss: 0.4252 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 7s - loss: 0.0703 - acc: 0.0000e+00 - val_loss: 0.4093 - val_acc: 0.0000e+00\n",
      "Train Score: 0.09820 MSE (0.31 RMSE)\n",
      "Test Score: 0.62372 MSE (0.79 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_115 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_116 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/30\n",
      "310/310 [==============================] - 38s - loss: 0.1199 - acc: 0.0000e+00 - val_loss: 0.5595 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "310/310 [==============================] - 7s - loss: 0.1187 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "310/310 [==============================] - 7s - loss: 0.1177 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "310/310 [==============================] - 7s - loss: 0.1166 - acc: 0.0000e+00 - val_loss: 0.5508 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "310/310 [==============================] - 8s - loss: 0.1155 - acc: 0.0000e+00 - val_loss: 0.5474 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "310/310 [==============================] - 7s - loss: 0.1142 - acc: 0.0000e+00 - val_loss: 0.5438 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "310/310 [==============================] - 8s - loss: 0.1129 - acc: 0.0000e+00 - val_loss: 0.5399 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "310/310 [==============================] - 7s - loss: 0.1117 - acc: 0.0000e+00 - val_loss: 0.5357 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "310/310 [==============================] - 8s - loss: 0.1103 - acc: 0.0000e+00 - val_loss: 0.5309 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "310/310 [==============================] - 7s - loss: 0.1087 - acc: 0.0000e+00 - val_loss: 0.5256 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "310/310 [==============================] - 8s - loss: 0.1068 - acc: 0.0000e+00 - val_loss: 0.5197 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "310/310 [==============================] - 7s - loss: 0.1052 - acc: 0.0000e+00 - val_loss: 0.5129 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "310/310 [==============================] - 8s - loss: 0.1034 - acc: 0.0000e+00 - val_loss: 0.5052 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "310/310 [==============================] - 8s - loss: 0.1006 - acc: 0.0000e+00 - val_loss: 0.4964 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "310/310 [==============================] - 7s - loss: 0.0977 - acc: 0.0000e+00 - val_loss: 0.4863 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "310/310 [==============================] - 7s - loss: 0.0945 - acc: 0.0000e+00 - val_loss: 0.4749 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "310/310 [==============================] - 7s - loss: 0.0910 - acc: 0.0000e+00 - val_loss: 0.4618 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "310/310 [==============================] - 7s - loss: 0.0875 - acc: 0.0000e+00 - val_loss: 0.4472 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "310/310 [==============================] - 8s - loss: 0.0817 - acc: 0.0000e+00 - val_loss: 0.4309 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "310/310 [==============================] - 7s - loss: 0.0758 - acc: 0.0000e+00 - val_loss: 0.4130 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "310/310 [==============================] - 8s - loss: 0.0697 - acc: 0.0000e+00 - val_loss: 0.3937 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "310/310 [==============================] - 7s - loss: 0.0643 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "310/310 [==============================] - 8s - loss: 0.0558 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "310/310 [==============================] - 7s - loss: 0.0498 - acc: 0.0000e+00 - val_loss: 0.3280 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "310/310 [==============================] - 7s - loss: 0.0437 - acc: 0.0000e+00 - val_loss: 0.3043 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "310/310 [==============================] - 7s - loss: 0.0350 - acc: 0.0000e+00 - val_loss: 0.2798 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "310/310 [==============================] - 7s - loss: 0.0283 - acc: 0.0000e+00 - val_loss: 0.2552 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "310/310 [==============================] - 7s - loss: 0.0242 - acc: 0.0000e+00 - val_loss: 0.2308 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "310/310 [==============================] - 7s - loss: 0.0215 - acc: 0.0000e+00 - val_loss: 0.2071 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "310/310 [==============================] - 7s - loss: 0.0215 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02925 MSE (0.17 RMSE)\n",
      "Test Score: 0.33643 MSE (0.58 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_117 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_118 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/40\n",
      "310/310 [==============================] - 41s - loss: 0.1187 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "310/310 [==============================] - 8s - loss: 0.1177 - acc: 0.0000e+00 - val_loss: 0.5528 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "310/310 [==============================] - 8s - loss: 0.1165 - acc: 0.0000e+00 - val_loss: 0.5483 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "310/310 [==============================] - 8s - loss: 0.1149 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "310/310 [==============================] - 8s - loss: 0.1131 - acc: 0.0000e+00 - val_loss: 0.5365 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "310/310 [==============================] - 7s - loss: 0.1110 - acc: 0.0000e+00 - val_loss: 0.5291 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "310/310 [==============================] - 8s - loss: 0.1089 - acc: 0.0000e+00 - val_loss: 0.5200 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "310/310 [==============================] - 7s - loss: 0.1061 - acc: 0.0000e+00 - val_loss: 0.5097 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "310/310 [==============================] - 8s - loss: 0.1031 - acc: 0.0000e+00 - val_loss: 0.4981 - val_acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      "310/310 [==============================] - 7s - loss: 0.0994 - acc: 0.0000e+00 - val_loss: 0.4849 - val_acc: 0.0000e+00\n",
      "Epoch 11/40\n",
      "310/310 [==============================] - 7s - loss: 0.0954 - acc: 0.0000e+00 - val_loss: 0.4702 - val_acc: 0.0000e+00\n",
      "Epoch 12/40\n",
      "310/310 [==============================] - 7s - loss: 0.0908 - acc: 0.0000e+00 - val_loss: 0.4541 - val_acc: 0.0000e+00\n",
      "Epoch 13/40\n",
      "310/310 [==============================] - 8s - loss: 0.0857 - acc: 0.0000e+00 - val_loss: 0.4364 - val_acc: 0.0000e+00\n",
      "Epoch 14/40\n",
      "310/310 [==============================] - 8s - loss: 0.0785 - acc: 0.0000e+00 - val_loss: 0.4173 - val_acc: 0.0000e+00\n",
      "Epoch 15/40\n",
      "310/310 [==============================] - 8s - loss: 0.0728 - acc: 0.0000e+00 - val_loss: 0.3971 - val_acc: 0.0000e+00\n",
      "Epoch 16/40\n",
      "310/310 [==============================] - 7s - loss: 0.0648 - acc: 0.0000e+00 - val_loss: 0.3756 - val_acc: 0.0000e+00\n",
      "Epoch 17/40\n",
      "310/310 [==============================] - 8s - loss: 0.0573 - acc: 0.0000e+00 - val_loss: 0.3531 - val_acc: 0.0000e+00\n",
      "Epoch 18/40\n",
      "310/310 [==============================] - 7s - loss: 0.0517 - acc: 0.0000e+00 - val_loss: 0.3299 - val_acc: 0.0000e+00\n",
      "Epoch 19/40\n",
      "310/310 [==============================] - 8s - loss: 0.0444 - acc: 0.0000e+00 - val_loss: 0.3058 - val_acc: 0.0000e+00\n",
      "Epoch 20/40\n",
      "310/310 [==============================] - 8s - loss: 0.0347 - acc: 0.0000e+00 - val_loss: 0.2814 - val_acc: 0.0000e+00\n",
      "Epoch 21/40\n",
      "310/310 [==============================] - 8s - loss: 0.0301 - acc: 0.0000e+00 - val_loss: 0.2567 - val_acc: 0.0000e+00\n",
      "Epoch 22/40\n",
      "310/310 [==============================] - 8s - loss: 0.0262 - acc: 0.0000e+00 - val_loss: 0.2321 - val_acc: 0.0000e+00\n",
      "Epoch 23/40\n",
      "310/310 [==============================] - 8s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.2082 - val_acc: 0.0000e+00\n",
      "Epoch 24/40\n",
      "310/310 [==============================] - 7s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 25/40\n",
      "310/310 [==============================] - 8s - loss: 0.0190 - acc: 0.0000e+00 - val_loss: 0.1649 - val_acc: 0.0000e+00\n",
      "Epoch 26/40\n",
      "310/310 [==============================] - 8s - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.1480 - val_acc: 0.0000e+00\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.1357 - val_acc: 0.0000e+00\n",
      "Epoch 28/40\n",
      "310/310 [==============================] - 7s - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.1278 - val_acc: 0.0000e+00\n",
      "Epoch 29/40\n",
      "310/310 [==============================] - 8s - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.1244 - val_acc: 0.0000e+00\n",
      "Epoch 30/40\n",
      "310/310 [==============================] - 8s - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.1245 - val_acc: 0.0000e+00\n",
      "Epoch 31/40\n",
      "310/310 [==============================] - 7s - loss: 0.0259 - acc: 0.0000e+00 - val_loss: 0.1276 - val_acc: 0.0000e+00\n",
      "Epoch 32/40\n",
      "310/310 [==============================] - 7s - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.1331 - val_acc: 0.0000e+00\n",
      "Epoch 33/40\n",
      "310/310 [==============================] - 7s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.1403 - val_acc: 0.0000e+00\n",
      "Epoch 34/40\n",
      "310/310 [==============================] - 7s - loss: 0.0254 - acc: 0.0000e+00 - val_loss: 0.1491 - val_acc: 0.0000e+00\n",
      "Epoch 35/40\n",
      "310/310 [==============================] - 8s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.1587 - val_acc: 0.0000e+00\n",
      "Epoch 36/40\n",
      "310/310 [==============================] - 7s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.1687 - val_acc: 0.0000e+00\n",
      "Epoch 37/40\n",
      "310/310 [==============================] - 8s - loss: 0.0202 - acc: 0.0000e+00 - val_loss: 0.1782 - val_acc: 0.0000e+00\n",
      "Epoch 38/40\n",
      "310/310 [==============================] - 7s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Epoch 39/40\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1952 - val_acc: 0.0000e+00\n",
      "Epoch 40/40\n",
      "310/310 [==============================] - 9s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2020 - val_acc: 0.0000e+00\n",
      "Train Score: 0.03170 MSE (0.18 RMSE)\n",
      "Test Score: 0.35770 MSE (0.60 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_119 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_120 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/50\n",
      "310/310 [==============================] - 42s - loss: 0.1200 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "310/310 [==============================] - 8s - loss: 0.1189 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "310/310 [==============================] - 8s - loss: 0.1179 - acc: 0.0000e+00 - val_loss: 0.5545 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "310/310 [==============================] - 8s - loss: 0.1169 - acc: 0.0000e+00 - val_loss: 0.5514 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "310/310 [==============================] - 8s - loss: 0.1157 - acc: 0.0000e+00 - val_loss: 0.5482 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "310/310 [==============================] - 8s - loss: 0.1145 - acc: 0.0000e+00 - val_loss: 0.5447 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "310/310 [==============================] - 8s - loss: 0.1134 - acc: 0.0000e+00 - val_loss: 0.5408 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "310/310 [==============================] - 8s - loss: 0.1121 - acc: 0.0000e+00 - val_loss: 0.5366 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "310/310 [==============================] - 8s - loss: 0.1107 - acc: 0.0000e+00 - val_loss: 0.5319 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "310/310 [==============================] - 8s - loss: 0.1092 - acc: 0.0000e+00 - val_loss: 0.5267 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "310/310 [==============================] - 8s - loss: 0.1077 - acc: 0.0000e+00 - val_loss: 0.5209 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "310/310 [==============================] - 8s - loss: 0.1060 - acc: 0.0000e+00 - val_loss: 0.5142 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "310/310 [==============================] - 8s - loss: 0.1036 - acc: 0.0000e+00 - val_loss: 0.5066 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "310/310 [==============================] - 8s - loss: 0.1015 - acc: 0.0000e+00 - val_loss: 0.4979 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "310/310 [==============================] - 8s - loss: 0.0991 - acc: 0.0000e+00 - val_loss: 0.4879 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "310/310 [==============================] - 8s - loss: 0.0957 - acc: 0.0000e+00 - val_loss: 0.4763 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "310/310 [==============================] - 8s - loss: 0.0926 - acc: 0.0000e+00 - val_loss: 0.4630 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "310/310 [==============================] - 8s - loss: 0.0886 - acc: 0.0000e+00 - val_loss: 0.4479 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "310/310 [==============================] - 8s - loss: 0.0842 - acc: 0.0000e+00 - val_loss: 0.4312 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "310/310 [==============================] - 8s - loss: 0.0776 - acc: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "310/310 [==============================] - 8s - loss: 0.0720 - acc: 0.0000e+00 - val_loss: 0.3943 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "310/310 [==============================] - 8s - loss: 0.0655 - acc: 0.0000e+00 - val_loss: 0.3746 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "310/310 [==============================] - 9s - loss: 0.0580 - acc: 0.0000e+00 - val_loss: 0.3543 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "310/310 [==============================] - 9s - loss: 0.0502 - acc: 0.0000e+00 - val_loss: 0.3335 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "310/310 [==============================] - 9s - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.3124 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "310/310 [==============================] - 8s - loss: 0.0356 - acc: 0.0000e+00 - val_loss: 0.2911 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "310/310 [==============================] - 7s - loss: 0.0299 - acc: 0.0000e+00 - val_loss: 0.2696 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "310/310 [==============================] - 9s - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.2483 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "310/310 [==============================] - 8s - loss: 0.0216 - acc: 0.0000e+00 - val_loss: 0.2276 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "310/310 [==============================] - 9s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.2077 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1891 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1724 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "310/310 [==============================] - 8s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.1580 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1478 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "310/310 [==============================] - 10s - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "310/310 [==============================] - 7s - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "310/310 [==============================] - 7s - loss: 0.0230 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "310/310 [==============================] - 7s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "310/310 [==============================] - 8s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.1460 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "310/310 [==============================] - 8s - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "310/310 [==============================] - 7s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.1592 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "310/310 [==============================] - 7s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1667 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "310/310 [==============================] - 6s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.1744 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "310/310 [==============================] - 7s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1819 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "310/310 [==============================] - 7s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1891 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "310/310 [==============================] - 7s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1958 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "310/310 [==============================] - 6s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2017 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "310/310 [==============================] - 6s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.2066 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "310/310 [==============================] - 6s - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.2108 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "310/310 [==============================] - 6s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2142 - val_acc: 0.0000e+00\n",
      "Train Score: 0.03380 MSE (0.18 RMSE)\n",
      "Test Score: 0.37589 MSE (0.61 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_121 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_122 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/60\n",
      "310/310 [==============================] - 56s - loss: 0.1198 - acc: 0.0000e+00 - val_loss: 0.5562 - val_acc: 0.0000e+00\n",
      "Epoch 2/60\n",
      "310/310 [==============================] - 8s - loss: 0.1183 - acc: 0.0000e+00 - val_loss: 0.5522 - val_acc: 0.0000e+00\n",
      "Epoch 3/60\n",
      "310/310 [==============================] - 8s - loss: 0.1169 - acc: 0.0000e+00 - val_loss: 0.5480 - val_acc: 0.0000e+00\n",
      "Epoch 4/60\n",
      "310/310 [==============================] - 8s - loss: 0.1154 - acc: 0.0000e+00 - val_loss: 0.5436 - val_acc: 0.0000e+00\n",
      "Epoch 5/60\n",
      "310/310 [==============================] - 7s - loss: 0.1140 - acc: 0.0000e+00 - val_loss: 0.5387 - val_acc: 0.0000e+00\n",
      "Epoch 6/60\n",
      "310/310 [==============================] - 7s - loss: 0.1124 - acc: 0.0000e+00 - val_loss: 0.5335 - val_acc: 0.0000e+00\n",
      "Epoch 7/60\n",
      "310/310 [==============================] - 7s - loss: 0.1106 - acc: 0.0000e+00 - val_loss: 0.5276 - val_acc: 0.0000e+00\n",
      "Epoch 8/60\n",
      "310/310 [==============================] - 8s - loss: 0.1084 - acc: 0.0000e+00 - val_loss: 0.5209 - val_acc: 0.0000e+00\n",
      "Epoch 9/60\n",
      "310/310 [==============================] - 7s - loss: 0.1067 - acc: 0.0000e+00 - val_loss: 0.5132 - val_acc: 0.0000e+00\n",
      "Epoch 10/60\n",
      "310/310 [==============================] - 8s - loss: 0.1041 - acc: 0.0000e+00 - val_loss: 0.5046 - val_acc: 0.0000e+00\n",
      "Epoch 11/60\n",
      "310/310 [==============================] - 7s - loss: 0.1008 - acc: 0.0000e+00 - val_loss: 0.4948 - val_acc: 0.0000e+00\n",
      "Epoch 12/60\n",
      "310/310 [==============================] - 7s - loss: 0.0978 - acc: 0.0000e+00 - val_loss: 0.4839 - val_acc: 0.0000e+00\n",
      "Epoch 13/60\n",
      "310/310 [==============================] - 7s - loss: 0.0943 - acc: 0.0000e+00 - val_loss: 0.4717 - val_acc: 0.0000e+00\n",
      "Epoch 14/60\n",
      "310/310 [==============================] - 8s - loss: 0.0901 - acc: 0.0000e+00 - val_loss: 0.4582 - val_acc: 0.0000e+00\n",
      "Epoch 15/60\n",
      "310/310 [==============================] - 7s - loss: 0.0846 - acc: 0.0000e+00 - val_loss: 0.4434 - val_acc: 0.0000e+00\n",
      "Epoch 16/60\n",
      "310/310 [==============================] - 8s - loss: 0.0794 - acc: 0.0000e+00 - val_loss: 0.4274 - val_acc: 0.0000e+00\n",
      "Epoch 17/60\n",
      "310/310 [==============================] - 7s - loss: 0.0742 - acc: 0.0000e+00 - val_loss: 0.4101 - val_acc: 0.0000e+00\n",
      "Epoch 18/60\n",
      "310/310 [==============================] - 8s - loss: 0.0678 - acc: 0.0000e+00 - val_loss: 0.3919 - val_acc: 0.0000e+00\n",
      "Epoch 19/60\n",
      "310/310 [==============================] - 7s - loss: 0.0593 - acc: 0.0000e+00 - val_loss: 0.3729 - val_acc: 0.0000e+00\n",
      "Epoch 20/60\n",
      "310/310 [==============================] - 8s - loss: 0.0569 - acc: 0.0000e+00 - val_loss: 0.3536 - val_acc: 0.0000e+00\n",
      "Epoch 21/60\n",
      "310/310 [==============================] - 7s - loss: 0.0473 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 22/60\n",
      "310/310 [==============================] - 8s - loss: 0.0408 - acc: 0.0000e+00 - val_loss: 0.3151 - val_acc: 0.0000e+00\n",
      "Epoch 23/60\n",
      "310/310 [==============================] - 7s - loss: 0.0367 - acc: 0.0000e+00 - val_loss: 0.2959 - val_acc: 0.0000e+00\n",
      "Epoch 24/60\n",
      "310/310 [==============================] - 8s - loss: 0.0319 - acc: 0.0000e+00 - val_loss: 0.2767 - val_acc: 0.0000e+00\n",
      "Epoch 25/60\n",
      "310/310 [==============================] - 7s - loss: 0.0262 - acc: 0.0000e+00 - val_loss: 0.2577 - val_acc: 0.0000e+00\n",
      "Epoch 26/60\n",
      "310/310 [==============================] - 7s - loss: 0.0246 - acc: 0.0000e+00 - val_loss: 0.2389 - val_acc: 0.0000e+00\n",
      "Epoch 27/60\n",
      "310/310 [==============================] - 8s - loss: 0.0214 - acc: 0.0000e+00 - val_loss: 0.2205 - val_acc: 0.0000e+00\n",
      "Epoch 28/60\n",
      "310/310 [==============================] - 8s - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.2027 - val_acc: 0.0000e+00\n",
      "Epoch 29/60\n",
      "310/310 [==============================] - 8s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.1863 - val_acc: 0.0000e+00\n",
      "Epoch 30/60\n",
      "310/310 [==============================] - 7s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1717 - val_acc: 0.0000e+00\n",
      "Epoch 31/60\n",
      "310/310 [==============================] - 7s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.1591 - val_acc: 0.0000e+00\n",
      "Epoch 32/60\n",
      "310/310 [==============================] - 8s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1495 - val_acc: 0.0000e+00\n",
      "Epoch 33/60\n",
      "310/310 [==============================] - 7s - loss: 0.0231 - acc: 0.0000e+00 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
      "Epoch 34/60\n",
      "310/310 [==============================] - 8s - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 35/60\n",
      "310/310 [==============================] - 7s - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 36/60\n",
      "310/310 [==============================] - 8s - loss: 0.0230 - acc: 0.0000e+00 - val_loss: 0.1428 - val_acc: 0.0000e+00\n",
      "Epoch 37/60\n",
      "310/310 [==============================] - 8s - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.1469 - val_acc: 0.0000e+00\n",
      "Epoch 38/60\n",
      "310/310 [==============================] - 8s - loss: 0.0203 - acc: 0.0000e+00 - val_loss: 0.1525 - val_acc: 0.0000e+00\n",
      "Epoch 39/60\n",
      "310/310 [==============================] - 8s - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.1593 - val_acc: 0.0000e+00\n",
      "Epoch 40/60\n",
      "310/310 [==============================] - 8s - loss: 0.0190 - acc: 0.0000e+00 - val_loss: 0.1668 - val_acc: 0.0000e+00\n",
      "Epoch 41/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.1744 - val_acc: 0.0000e+00\n",
      "Epoch 42/60\n",
      "310/310 [==============================] - 7s - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0000e+00\n",
      "Epoch 43/60\n",
      "310/310 [==============================] - 7s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1894 - val_acc: 0.0000e+00\n",
      "Epoch 44/60\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1959 - val_acc: 0.0000e+00\n",
      "Epoch 45/60\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2019 - val_acc: 0.0000e+00\n",
      "Epoch 46/60\n",
      "310/310 [==============================] - 7s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.2068 - val_acc: 0.0000e+00\n",
      "Epoch 47/60\n",
      "310/310 [==============================] - 8s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.2112 - val_acc: 0.0000e+00\n",
      "Epoch 48/60\n",
      "310/310 [==============================] - 7s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2145 - val_acc: 0.0000e+00\n",
      "Epoch 49/60\n",
      "310/310 [==============================] - 8s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.2167 - val_acc: 0.0000e+00\n",
      "Epoch 50/60\n",
      "310/310 [==============================] - 7s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2182 - val_acc: 0.0000e+00\n",
      "Epoch 51/60\n",
      "310/310 [==============================] - 7s - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.2191 - val_acc: 0.0000e+00\n",
      "Epoch 52/60\n",
      "310/310 [==============================] - 8s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.2191 - val_acc: 0.0000e+00\n",
      "Epoch 53/60\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.2184 - val_acc: 0.0000e+00\n",
      "Epoch 54/60\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2169 - val_acc: 0.0000e+00\n",
      "Epoch 55/60\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.2147 - val_acc: 0.0000e+00\n",
      "Epoch 56/60\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.2122 - val_acc: 0.0000e+00\n",
      "Epoch 57/60\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2094 - val_acc: 0.0000e+00\n",
      "Epoch 58/60\n",
      "310/310 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.2065 - val_acc: 0.0000e+00\n",
      "Epoch 59/60\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2035 - val_acc: 0.0000e+00\n",
      "Epoch 60/60\n",
      "310/310 [==============================] - 7s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2004 - val_acc: 0.0000e+00\n",
      "Train Score: 0.03146 MSE (0.18 RMSE)\n",
      "Test Score: 0.35773 MSE (0.60 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_123 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_124 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 72s - loss: 0.1196 - acc: 0.0000e+00 - val_loss: 0.5574 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 8s - loss: 0.1184 - acc: 0.0000e+00 - val_loss: 0.5539 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 7s - loss: 0.1172 - acc: 0.0000e+00 - val_loss: 0.5503 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 8s - loss: 0.1160 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 8s - loss: 0.1145 - acc: 0.0000e+00 - val_loss: 0.5419 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 9s - loss: 0.1131 - acc: 0.0000e+00 - val_loss: 0.5368 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 9s - loss: 0.1114 - acc: 0.0000e+00 - val_loss: 0.5311 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 8s - loss: 0.1098 - acc: 0.0000e+00 - val_loss: 0.5246 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 8s - loss: 0.1079 - acc: 0.0000e+00 - val_loss: 0.5170 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 8s - loss: 0.1058 - acc: 0.0000e+00 - val_loss: 0.5084 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 8s - loss: 0.1030 - acc: 0.0000e+00 - val_loss: 0.4986 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 8s - loss: 0.0994 - acc: 0.0000e+00 - val_loss: 0.4878 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 8s - loss: 0.0967 - acc: 0.0000e+00 - val_loss: 0.4757 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 8s - loss: 0.0918 - acc: 0.0000e+00 - val_loss: 0.4627 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 8s - loss: 0.0871 - acc: 0.0000e+00 - val_loss: 0.4487 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 8s - loss: 0.0829 - acc: 0.0000e+00 - val_loss: 0.4339 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 8s - loss: 0.0767 - acc: 0.0000e+00 - val_loss: 0.4182 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 8s - loss: 0.0702 - acc: 0.0000e+00 - val_loss: 0.4018 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 8s - loss: 0.0639 - acc: 0.0000e+00 - val_loss: 0.3847 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 8s - loss: 0.0586 - acc: 0.0000e+00 - val_loss: 0.3668 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 8s - loss: 0.0507 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 8s - loss: 0.0452 - acc: 0.0000e+00 - val_loss: 0.3290 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 8s - loss: 0.0400 - acc: 0.0000e+00 - val_loss: 0.3091 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 8s - loss: 0.0355 - acc: 0.0000e+00 - val_loss: 0.2886 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 8s - loss: 0.0319 - acc: 0.0000e+00 - val_loss: 0.2676 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 8s - loss: 0.0250 - acc: 0.0000e+00 - val_loss: 0.2464 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 8s - loss: 0.0226 - acc: 0.0000e+00 - val_loss: 0.2252 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 8s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.2042 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 8s - loss: 0.0198 - acc: 0.0000e+00 - val_loss: 0.1841 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 7s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1658 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 8s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.1505 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 8s - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 8s - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.1319 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 8s - loss: 0.0219 - acc: 0.0000e+00 - val_loss: 0.1288 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 7s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.1292 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 8s - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.1319 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 8s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.1364 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 8s - loss: 0.0227 - acc: 0.0000e+00 - val_loss: 0.1423 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 7s - loss: 0.0216 - acc: 0.0000e+00 - val_loss: 0.1494 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 8s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.1571 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 8s - loss: 0.0198 - acc: 0.0000e+00 - val_loss: 0.1654 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1737 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.1816 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1953 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 8s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.2050 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 8s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.2081 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2102 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 8s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.2113 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2115 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2108 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 7s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.2093 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2073 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.2048 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2020 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1986 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 8s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 8s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1877 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 9s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1841 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 9s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 9s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1772 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1743 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1702 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1689 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 8s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.1683 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1680 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02672 MSE (0.16 RMSE)\n",
      "Test Score: 0.31045 MSE (0.56 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_125 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_126 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/80\n",
      "310/310 [==============================] - 52s - loss: 0.1184 - acc: 0.0000e+00 - val_loss: 0.5527 - val_acc: 0.0000e+00\n",
      "Epoch 2/80\n",
      "310/310 [==============================] - 9s - loss: 0.1169 - acc: 0.0000e+00 - val_loss: 0.5483 - val_acc: 0.0000e+00\n",
      "Epoch 3/80\n",
      "310/310 [==============================] - 9s - loss: 0.1154 - acc: 0.0000e+00 - val_loss: 0.5432 - val_acc: 0.0000e+00\n",
      "Epoch 4/80\n",
      "310/310 [==============================] - 10s - loss: 0.1137 - acc: 0.0000e+00 - val_loss: 0.5375 - val_acc: 0.0000e+00\n",
      "Epoch 5/80\n",
      "310/310 [==============================] - 9s - loss: 0.1122 - acc: 0.0000e+00 - val_loss: 0.5310 - val_acc: 0.0000e+00\n",
      "Epoch 6/80\n",
      "310/310 [==============================] - 8s - loss: 0.1098 - acc: 0.0000e+00 - val_loss: 0.5232 - val_acc: 0.0000e+00\n",
      "Epoch 7/80\n",
      "310/310 [==============================] - 8s - loss: 0.1076 - acc: 0.0000e+00 - val_loss: 0.5141 - val_acc: 0.0000e+00\n",
      "Epoch 8/80\n",
      "310/310 [==============================] - 8s - loss: 0.1047 - acc: 0.0000e+00 - val_loss: 0.5036 - val_acc: 0.0000e+00\n",
      "Epoch 9/80\n",
      "310/310 [==============================] - 9s - loss: 0.1013 - acc: 0.0000e+00 - val_loss: 0.4916 - val_acc: 0.0000e+00\n",
      "Epoch 10/80\n",
      "310/310 [==============================] - 10s - loss: 0.0982 - acc: 0.0000e+00 - val_loss: 0.4780 - val_acc: 0.0000e+00\n",
      "Epoch 11/80\n",
      "310/310 [==============================] - 9s - loss: 0.0935 - acc: 0.0000e+00 - val_loss: 0.4628 - val_acc: 0.0000e+00\n",
      "Epoch 12/80\n",
      "310/310 [==============================] - 8s - loss: 0.0889 - acc: 0.0000e+00 - val_loss: 0.4460 - val_acc: 0.0000e+00\n",
      "Epoch 13/80\n",
      "310/310 [==============================] - 8s - loss: 0.0838 - acc: 0.0000e+00 - val_loss: 0.4275 - val_acc: 0.0000e+00\n",
      "Epoch 14/80\n",
      "310/310 [==============================] - 8s - loss: 0.0767 - acc: 0.0000e+00 - val_loss: 0.4074 - val_acc: 0.0000e+00\n",
      "Epoch 15/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0699 - acc: 0.0000e+00 - val_loss: 0.3860 - val_acc: 0.0000e+00\n",
      "Epoch 16/80\n",
      "310/310 [==============================] - 8s - loss: 0.0617 - acc: 0.0000e+00 - val_loss: 0.3633 - val_acc: 0.0000e+00\n",
      "Epoch 17/80\n",
      "310/310 [==============================] - 8s - loss: 0.0547 - acc: 0.0000e+00 - val_loss: 0.3397 - val_acc: 0.0000e+00\n",
      "Epoch 18/80\n",
      "310/310 [==============================] - 8s - loss: 0.0464 - acc: 0.0000e+00 - val_loss: 0.3154 - val_acc: 0.0000e+00\n",
      "Epoch 19/80\n",
      "310/310 [==============================] - 8s - loss: 0.0401 - acc: 0.0000e+00 - val_loss: 0.2908 - val_acc: 0.0000e+00\n",
      "Epoch 20/80\n",
      "310/310 [==============================] - 8s - loss: 0.0324 - acc: 0.0000e+00 - val_loss: 0.2661 - val_acc: 0.0000e+00\n",
      "Epoch 21/80\n",
      "310/310 [==============================] - 8s - loss: 0.0291 - acc: 0.0000e+00 - val_loss: 0.2417 - val_acc: 0.0000e+00\n",
      "Epoch 22/80\n",
      "310/310 [==============================] - 8s - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.2179 - val_acc: 0.0000e+00\n",
      "Epoch 23/80\n",
      "310/310 [==============================] - 8s - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.1953 - val_acc: 0.0000e+00\n",
      "Epoch 24/80\n",
      "310/310 [==============================] - 8s - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.1745 - val_acc: 0.0000e+00\n",
      "Epoch 25/80\n",
      "310/310 [==============================] - 8s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.1574 - val_acc: 0.0000e+00\n",
      "Epoch 26/80\n",
      "310/310 [==============================] - 8s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1447 - val_acc: 0.0000e+00\n",
      "Epoch 27/80\n",
      "310/310 [==============================] - 8s - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.1360 - val_acc: 0.0000e+00\n",
      "Epoch 28/80\n",
      "310/310 [==============================] - 8s - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.1312 - val_acc: 0.0000e+00\n",
      "Epoch 29/80\n",
      "310/310 [==============================] - 8s - loss: 0.0296 - acc: 0.0000e+00 - val_loss: 0.1309 - val_acc: 0.0000e+00\n",
      "Epoch 30/80\n",
      "310/310 [==============================] - 9s - loss: 0.0251 - acc: 0.0000e+00 - val_loss: 0.1336 - val_acc: 0.0000e+00\n",
      "Epoch 31/80\n",
      "310/310 [==============================] - 8s - loss: 0.0279 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
      "Epoch 32/80\n",
      "310/310 [==============================] - 8s - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.1461 - val_acc: 0.0000e+00\n",
      "Epoch 33/80\n",
      "310/310 [==============================] - 9s - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.1544 - val_acc: 0.0000e+00\n",
      "Epoch 34/80\n",
      "310/310 [==============================] - 9s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1632 - val_acc: 0.0000e+00\n",
      "Epoch 35/80\n",
      "310/310 [==============================] - 8s - loss: 0.0216 - acc: 0.0000e+00 - val_loss: 0.1724 - val_acc: 0.0000e+00\n",
      "Epoch 36/80\n",
      "310/310 [==============================] - 8s - loss: 0.0198 - acc: 0.0000e+00 - val_loss: 0.1818 - val_acc: 0.0000e+00\n",
      "Epoch 37/80\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1905 - val_acc: 0.0000e+00\n",
      "Epoch 38/80\n",
      "310/310 [==============================] - 8s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1983 - val_acc: 0.0000e+00\n",
      "Epoch 39/80\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2053 - val_acc: 0.0000e+00\n",
      "Epoch 40/80\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.2113 - val_acc: 0.0000e+00\n",
      "Epoch 41/80\n",
      "310/310 [==============================] - 8s - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.2165 - val_acc: 0.0000e+00\n",
      "Epoch 42/80\n",
      "310/310 [==============================] - 8s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.2201 - val_acc: 0.0000e+00\n",
      "Epoch 43/80\n",
      "310/310 [==============================] - 9s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2227 - val_acc: 0.0000e+00\n",
      "Epoch 44/80\n",
      "310/310 [==============================] - 10s - loss: 0.0190 - acc: 0.0000e+00 - val_loss: 0.2240 - val_acc: 0.0000e+00\n",
      "Epoch 45/80\n",
      "310/310 [==============================] - 10s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.2244 - val_acc: 0.0000e+00\n",
      "Epoch 46/80\n",
      "310/310 [==============================] - 10s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2235 - val_acc: 0.0000e+00\n",
      "Epoch 47/80\n",
      "310/310 [==============================] - 9s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.2216 - val_acc: 0.0000e+00\n",
      "Epoch 48/80\n",
      "310/310 [==============================] - 9s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.2191 - val_acc: 0.0000e+00\n",
      "Epoch 49/80\n",
      "310/310 [==============================] - 9s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2160 - val_acc: 0.0000e+00\n",
      "Epoch 50/80\n",
      "310/310 [==============================] - 9s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.2125 - val_acc: 0.0000e+00\n",
      "Epoch 51/80\n",
      "310/310 [==============================] - 9s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2079 - val_acc: 0.0000e+00\n",
      "Epoch 52/80\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.2031 - val_acc: 0.0000e+00\n",
      "Epoch 53/80\n",
      "310/310 [==============================] - 9s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.1980 - val_acc: 0.0000e+00\n",
      "Epoch 54/80\n",
      "310/310 [==============================] - 8s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.1934 - val_acc: 0.0000e+00\n",
      "Epoch 55/80\n",
      "310/310 [==============================] - 9s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 56/80\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 57/80\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1817 - val_acc: 0.0000e+00\n",
      "Epoch 58/80\n",
      "310/310 [==============================] - 9s - loss: 0.0196 - acc: 0.0000e+00 - val_loss: 0.1792 - val_acc: 0.0000e+00\n",
      "Epoch 59/80\n",
      "310/310 [==============================] - 10s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1773 - val_acc: 0.0000e+00\n",
      "Epoch 60/80\n",
      "310/310 [==============================] - 10s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.1761 - val_acc: 0.0000e+00\n",
      "Epoch 61/80\n",
      "310/310 [==============================] - 9s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1759 - val_acc: 0.0000e+00\n",
      "Epoch 62/80\n",
      "310/310 [==============================] - 9s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1767 - val_acc: 0.0000e+00\n",
      "Epoch 63/80\n",
      "310/310 [==============================] - 9s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1782 - val_acc: 0.0000e+00\n",
      "Epoch 64/80\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1806 - val_acc: 0.0000e+00\n",
      "Epoch 65/80\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1835 - val_acc: 0.0000e+00\n",
      "Epoch 66/80\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Epoch 67/80\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1898 - val_acc: 0.0000e+00\n",
      "Epoch 68/80\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1929 - val_acc: 0.0000e+00\n",
      "Epoch 69/80\n",
      "310/310 [==============================] - 8s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.1960 - val_acc: 0.0000e+00\n",
      "Epoch 70/80\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1985 - val_acc: 0.0000e+00\n",
      "Epoch 71/80\n",
      "310/310 [==============================] - 8s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.2005 - val_acc: 0.0000e+00\n",
      "Epoch 72/80\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.2018 - val_acc: 0.0000e+00\n",
      "Epoch 73/80\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.2027 - val_acc: 0.0000e+00\n",
      "Epoch 74/80\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2029 - val_acc: 0.0000e+00\n",
      "Epoch 75/80\n",
      "310/310 [==============================] - 8s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.2027 - val_acc: 0.0000e+00\n",
      "Epoch 76/80\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.2019 - val_acc: 0.0000e+00\n",
      "Epoch 77/80\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.2008 - val_acc: 0.0000e+00\n",
      "Epoch 78/80\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1992 - val_acc: 0.0000e+00\n",
      "Epoch 79/80\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1973 - val_acc: 0.0000e+00\n",
      "Epoch 80/80\n",
      "310/310 [==============================] - 8s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1951 - val_acc: 0.0000e+00\n",
      "Train Score: 0.03056 MSE (0.17 RMSE)\n",
      "Test Score: 0.34940 MSE (0.59 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_127 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_128 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/90\n",
      "310/310 [==============================] - 47s - loss: 0.1198 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "310/310 [==============================] - 8s - loss: 0.1186 - acc: 0.0000e+00 - val_loss: 0.5546 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "310/310 [==============================] - 8s - loss: 0.1173 - acc: 0.0000e+00 - val_loss: 0.5508 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "310/310 [==============================] - 9s - loss: 0.1159 - acc: 0.0000e+00 - val_loss: 0.5467 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "310/310 [==============================] - 9s - loss: 0.1146 - acc: 0.0000e+00 - val_loss: 0.5421 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "310/310 [==============================] - 8s - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.5368 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "310/310 [==============================] - 8s - loss: 0.1113 - acc: 0.0000e+00 - val_loss: 0.5306 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "310/310 [==============================] - 8s - loss: 0.1093 - acc: 0.0000e+00 - val_loss: 0.5236 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "310/310 [==============================] - 8s - loss: 0.1072 - acc: 0.0000e+00 - val_loss: 0.5156 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "310/310 [==============================] - 8s - loss: 0.1048 - acc: 0.0000e+00 - val_loss: 0.5066 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "310/310 [==============================] - 8s - loss: 0.1020 - acc: 0.0000e+00 - val_loss: 0.4964 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "310/310 [==============================] - 8s - loss: 0.0981 - acc: 0.0000e+00 - val_loss: 0.4851 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "310/310 [==============================] - 8s - loss: 0.0956 - acc: 0.0000e+00 - val_loss: 0.4726 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "310/310 [==============================] - 9s - loss: 0.0899 - acc: 0.0000e+00 - val_loss: 0.4589 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "310/310 [==============================] - 8s - loss: 0.0854 - acc: 0.0000e+00 - val_loss: 0.4442 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "310/310 [==============================] - 9s - loss: 0.0800 - acc: 0.0000e+00 - val_loss: 0.4283 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "310/310 [==============================] - 8s - loss: 0.0746 - acc: 0.0000e+00 - val_loss: 0.4113 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "310/310 [==============================] - 8s - loss: 0.0662 - acc: 0.0000e+00 - val_loss: 0.3933 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "310/310 [==============================] - 8s - loss: 0.0596 - acc: 0.0000e+00 - val_loss: 0.3743 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "310/310 [==============================] - 8s - loss: 0.0535 - acc: 0.0000e+00 - val_loss: 0.3544 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "310/310 [==============================] - 8s - loss: 0.0486 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "310/310 [==============================] - 8s - loss: 0.0425 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "310/310 [==============================] - 8s - loss: 0.0356 - acc: 0.0000e+00 - val_loss: 0.2903 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "310/310 [==============================] - 8s - loss: 0.0312 - acc: 0.0000e+00 - val_loss: 0.2681 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "310/310 [==============================] - 8s - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.2459 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "310/310 [==============================] - 8s - loss: 0.0226 - acc: 0.0000e+00 - val_loss: 0.2241 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "310/310 [==============================] - 8s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.2031 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "310/310 [==============================] - 8s - loss: 0.0196 - acc: 0.0000e+00 - val_loss: 0.1837 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1666 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "310/310 [==============================] - 8s - loss: 0.0204 - acc: 0.0000e+00 - val_loss: 0.1528 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.1422 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "310/310 [==============================] - 8s - loss: 0.0210 - acc: 0.0000e+00 - val_loss: 0.1353 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "310/310 [==============================] - 8s - loss: 0.0246 - acc: 0.0000e+00 - val_loss: 0.1321 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "310/310 [==============================] - 8s - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.1324 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "310/310 [==============================] - 8s - loss: 0.0265 - acc: 0.0000e+00 - val_loss: 0.1355 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "310/310 [==============================] - 8s - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "310/310 [==============================] - 8s - loss: 0.0223 - acc: 0.0000e+00 - val_loss: 0.1474 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "310/310 [==============================] - 8s - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.1554 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "310/310 [==============================] - 8s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.1637 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "310/310 [==============================] - 8s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.1722 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "310/310 [==============================] - 8s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.1807 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1962 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.2026 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2080 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2124 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.2156 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.2178 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.2191 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.2194 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.2187 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "310/310 [==============================] - 8s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.2171 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "310/310 [==============================] - 8s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.2150 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.2123 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "310/310 [==============================] - 8s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.2092 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2060 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2024 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.1987 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1952 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "310/310 [==============================] - 8s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.1861 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "310/310 [==============================] - 8s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1836 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "310/310 [==============================] - 8s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1817 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1797 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1796 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1798 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1813 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "310/310 [==============================] - 8s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "310/310 [==============================] - 8s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.1839 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1854 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1877 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "310/310 [==============================] - 9s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1903 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1917 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "310/310 [==============================] - 8s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1926 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1932 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1936 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1936 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "310/310 [==============================] - 8s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1935 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "310/310 [==============================] - 9s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1932 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1924 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "310/310 [==============================] - 8s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1914 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1905 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1896 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02979 MSE (0.17 RMSE)\n",
      "Test Score: 0.34108 MSE (0.58 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_129 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_130 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/100\n",
      "310/310 [==============================] - 44s - loss: 0.1194 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "310/310 [==============================] - 9s - loss: 0.1185 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "310/310 [==============================] - 10s - loss: 0.1174 - acc: 0.0000e+00 - val_loss: 0.5519 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "310/310 [==============================] - 8s - loss: 0.1164 - acc: 0.0000e+00 - val_loss: 0.5482 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "310/310 [==============================] - 8s - loss: 0.1152 - acc: 0.0000e+00 - val_loss: 0.5440 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "310/310 [==============================] - 8s - loss: 0.1140 - acc: 0.0000e+00 - val_loss: 0.5394 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "310/310 [==============================] - 8s - loss: 0.1126 - acc: 0.0000e+00 - val_loss: 0.5342 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "310/310 [==============================] - 10s - loss: 0.1108 - acc: 0.0000e+00 - val_loss: 0.5283 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "310/310 [==============================] - 8s - loss: 0.1094 - acc: 0.0000e+00 - val_loss: 0.5216 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "310/310 [==============================] - 8s - loss: 0.1071 - acc: 0.0000e+00 - val_loss: 0.5139 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "310/310 [==============================] - 8s - loss: 0.1050 - acc: 0.0000e+00 - val_loss: 0.5051 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "310/310 [==============================] - 8s - loss: 0.1024 - acc: 0.0000e+00 - val_loss: 0.4951 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "310/310 [==============================] - 8s - loss: 0.0997 - acc: 0.0000e+00 - val_loss: 0.4836 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "310/310 [==============================] - 8s - loss: 0.0964 - acc: 0.0000e+00 - val_loss: 0.4705 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "310/310 [==============================] - 8s - loss: 0.0923 - acc: 0.0000e+00 - val_loss: 0.4558 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "310/310 [==============================] - 8s - loss: 0.0870 - acc: 0.0000e+00 - val_loss: 0.4395 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "310/310 [==============================] - 8s - loss: 0.0823 - acc: 0.0000e+00 - val_loss: 0.4219 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "310/310 [==============================] - 8s - loss: 0.0755 - acc: 0.0000e+00 - val_loss: 0.4033 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "310/310 [==============================] - 8s - loss: 0.0685 - acc: 0.0000e+00 - val_loss: 0.3841 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "310/310 [==============================] - 8s - loss: 0.0602 - acc: 0.0000e+00 - val_loss: 0.3645 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "310/310 [==============================] - 8s - loss: 0.0528 - acc: 0.0000e+00 - val_loss: 0.3448 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "310/310 [==============================] - 8s - loss: 0.0484 - acc: 0.0000e+00 - val_loss: 0.3248 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "310/310 [==============================] - 8s - loss: 0.0416 - acc: 0.0000e+00 - val_loss: 0.3050 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "310/310 [==============================] - 8s - loss: 0.0343 - acc: 0.0000e+00 - val_loss: 0.2850 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "310/310 [==============================] - 8s - loss: 0.0288 - acc: 0.0000e+00 - val_loss: 0.2648 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "310/310 [==============================] - 8s - loss: 0.0249 - acc: 0.0000e+00 - val_loss: 0.2445 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "310/310 [==============================] - 8s - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.2246 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "310/310 [==============================] - 8s - loss: 0.0196 - acc: 0.0000e+00 - val_loss: 0.2050 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "310/310 [==============================] - 8s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.1862 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1696 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "310/310 [==============================] - 8s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.1559 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1458 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "310/310 [==============================] - 8s - loss: 0.0244 - acc: 0.0000e+00 - val_loss: 0.1403 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "310/310 [==============================] - 8s - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "310/310 [==============================] - 8s - loss: 0.0210 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "310/310 [==============================] - 8s - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.1424 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "310/310 [==============================] - 8s - loss: 0.0203 - acc: 0.0000e+00 - val_loss: 0.1472 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "310/310 [==============================] - 8s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.1532 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "310/310 [==============================] - 8s - loss: 0.0230 - acc: 0.0000e+00 - val_loss: 0.1601 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "310/310 [==============================] - 8s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.1676 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1749 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.1820 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.1952 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.2009 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2057 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.2096 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2125 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2147 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "310/310 [==============================] - 9s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.2159 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2162 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "310/310 [==============================] - 9s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2159 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2149 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.2132 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2110 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.2084 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "310/310 [==============================] - 9s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2053 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2022 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1990 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1957 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "310/310 [==============================] - 8s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.1925 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1898 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "310/310 [==============================] - 9s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1852 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1835 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1823 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1813 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1809 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1807 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "310/310 [==============================] - 8s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1811 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "310/310 [==============================] - 8s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1818 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1830 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "310/310 [==============================] - 8s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1843 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1857 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "310/310 [==============================] - 9s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1873 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.1904 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "310/310 [==============================] - 8s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1920 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "310/310 [==============================] - 8s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1932 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "310/310 [==============================] - 8s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1940 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "310/310 [==============================] - 9s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "310/310 [==============================] - 8s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1954 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "310/310 [==============================] - 8s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.1956 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1954 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1951 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1947 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1943 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1936 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "310/310 [==============================] - 8s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1927 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "310/310 [==============================] - 8s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.1920 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "310/310 [==============================] - 8s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1917 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "310/310 [==============================] - 8s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1912 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1910 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "310/310 [==============================] - 8s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1908 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "310/310 [==============================] - 8s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1903 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1892 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "310/310 [==============================] - 8s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1882 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "310/310 [==============================] - 9s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1876 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02951 MSE (0.17 RMSE)\n",
      "Test Score: 0.33746 MSE (0.58 RMSE)\n"
     ]
    }
   ],
   "source": [
    "epochs_result = {}\n",
    "\n",
    "for epochs in epochslist:    \n",
    "    trainScore, testScore = quick_measure(file_csv_name, window_size, d, shape, neurons, epochs)\n",
    "    epochs_result[epochs] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHwCAYAAAAIDnN0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15c596e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(epochs_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.310454328593\n",
      "Number epochs: [70]\n"
     ]
    }
   ],
   "source": [
    "min_val_epochs = min(epochs_result.values())\n",
    "min_val_epochs_key = [k for k, v in epochs_result.items() if v == min_val_epochs]\n",
    "print ('MSE:', min_val_epochs)\n",
    "print ('Number epochs:', min_val_epochs_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_131 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_132 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,601\n",
      "Trainable params: 13,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 56s - loss: 0.1200 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 10s - loss: 0.1188 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 8s - loss: 0.1177 - acc: 0.0000e+00 - val_loss: 0.5522 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 8s - loss: 0.1165 - acc: 0.0000e+00 - val_loss: 0.5493 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 8s - loss: 0.1154 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 8s - loss: 0.1142 - acc: 0.0000e+00 - val_loss: 0.5425 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 7s - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.5384 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 8s - loss: 0.1116 - acc: 0.0000e+00 - val_loss: 0.5340 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 8s - loss: 0.1103 - acc: 0.0000e+00 - val_loss: 0.5290 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 8s - loss: 0.1088 - acc: 0.0000e+00 - val_loss: 0.5235 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 8s - loss: 0.1070 - acc: 0.0000e+00 - val_loss: 0.5172 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 8s - loss: 0.1052 - acc: 0.0000e+00 - val_loss: 0.5103 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 8s - loss: 0.1031 - acc: 0.0000e+00 - val_loss: 0.5025 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 8s - loss: 0.1009 - acc: 0.0000e+00 - val_loss: 0.4938 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 8s - loss: 0.0982 - acc: 0.0000e+00 - val_loss: 0.4842 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 8s - loss: 0.0948 - acc: 0.0000e+00 - val_loss: 0.4736 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 8s - loss: 0.0910 - acc: 0.0000e+00 - val_loss: 0.4620 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 8s - loss: 0.0873 - acc: 0.0000e+00 - val_loss: 0.4492 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 8s - loss: 0.0824 - acc: 0.0000e+00 - val_loss: 0.4353 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 8s - loss: 0.0789 - acc: 0.0000e+00 - val_loss: 0.4204 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 8s - loss: 0.0728 - acc: 0.0000e+00 - val_loss: 0.4045 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 8s - loss: 0.0663 - acc: 0.0000e+00 - val_loss: 0.3878 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 8s - loss: 0.0607 - acc: 0.0000e+00 - val_loss: 0.3704 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 8s - loss: 0.0550 - acc: 0.0000e+00 - val_loss: 0.3523 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 8s - loss: 0.0478 - acc: 0.0000e+00 - val_loss: 0.3337 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 8s - loss: 0.0422 - acc: 0.0000e+00 - val_loss: 0.3145 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 8s - loss: 0.0374 - acc: 0.0000e+00 - val_loss: 0.2947 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 8s - loss: 0.0313 - acc: 0.0000e+00 - val_loss: 0.2744 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 8s - loss: 0.0274 - acc: 0.0000e+00 - val_loss: 0.2540 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 7s - loss: 0.0244 - acc: 0.0000e+00 - val_loss: 0.2334 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 8s - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.2132 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 8s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.1940 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 8s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.1762 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1605 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 9s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.1488 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 11s - loss: 0.0203 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 10s - loss: 0.0215 - acc: 0.0000e+00 - val_loss: 0.1364 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 11s - loss: 0.0263 - acc: 0.0000e+00 - val_loss: 0.1359 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 9s - loss: 0.0227 - acc: 0.0000e+00 - val_loss: 0.1375 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 7s - loss: 0.0263 - acc: 0.0000e+00 - val_loss: 0.1418 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 8s - loss: 0.0223 - acc: 0.0000e+00 - val_loss: 0.1477 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 7s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.1544 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 8s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.1617 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1693 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 8s - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.1769 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 8s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1844 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 8s - loss: 0.0211 - acc: 0.0000e+00 - val_loss: 0.1914 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.1979 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2036 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2082 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 8s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.2119 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 8s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.2146 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2162 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2168 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 8s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.2166 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.2159 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 8s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.2144 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.2122 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 8s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2096 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 8s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2068 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2038 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1975 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 8s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1941 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1910 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 8s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1883 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1863 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1847 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 8s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1836 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 9s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1827 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02890 MSE (0.17 RMSE)\n",
      "Test Score: 0.33245 MSE (0.58 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_133 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_134 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 14,145\n",
      "Trainable params: 14,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 39s - loss: 0.1198 - acc: 0.0000e+00 - val_loss: 0.5569 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 9s - loss: 0.1182 - acc: 0.0000e+00 - val_loss: 0.5517 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 8s - loss: 0.1168 - acc: 0.0000e+00 - val_loss: 0.5464 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 8s - loss: 0.1151 - acc: 0.0000e+00 - val_loss: 0.5411 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 8s - loss: 0.1134 - acc: 0.0000e+00 - val_loss: 0.5352 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 8s - loss: 0.1115 - acc: 0.0000e+00 - val_loss: 0.5284 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 8s - loss: 0.1093 - acc: 0.0000e+00 - val_loss: 0.5206 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 8s - loss: 0.1067 - acc: 0.0000e+00 - val_loss: 0.5117 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 8s - loss: 0.1039 - acc: 0.0000e+00 - val_loss: 0.5012 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 8s - loss: 0.1008 - acc: 0.0000e+00 - val_loss: 0.4890 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 8s - loss: 0.0966 - acc: 0.0000e+00 - val_loss: 0.4749 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 8s - loss: 0.0931 - acc: 0.0000e+00 - val_loss: 0.4587 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 8s - loss: 0.0875 - acc: 0.0000e+00 - val_loss: 0.4401 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 8s - loss: 0.0818 - acc: 0.0000e+00 - val_loss: 0.4192 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 8s - loss: 0.0766 - acc: 0.0000e+00 - val_loss: 0.3957 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 8s - loss: 0.0677 - acc: 0.0000e+00 - val_loss: 0.3701 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 8s - loss: 0.0601 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 7s - loss: 0.0507 - acc: 0.0000e+00 - val_loss: 0.3133 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 8s - loss: 0.0423 - acc: 0.0000e+00 - val_loss: 0.2832 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 8s - loss: 0.0315 - acc: 0.0000e+00 - val_loss: 0.2528 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 8s - loss: 0.0264 - acc: 0.0000e+00 - val_loss: 0.2230 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 8s - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.1687 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.1465 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 8s - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.1306 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 8s - loss: 0.0232 - acc: 0.0000e+00 - val_loss: 0.1212 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 8s - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.1184 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 8s - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.1197 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 8s - loss: 0.0277 - acc: 0.0000e+00 - val_loss: 0.1248 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 8s - loss: 0.0237 - acc: 0.0000e+00 - val_loss: 0.1325 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 8s - loss: 0.0228 - acc: 0.0000e+00 - val_loss: 0.1420 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 8s - loss: 0.0191 - acc: 0.0000e+00 - val_loss: 0.1523 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 8s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 8s - loss: 0.0203 - acc: 0.0000e+00 - val_loss: 0.1744 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 8s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.1943 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.2023 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 8s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.2088 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 8s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.2137 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 7s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.2169 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 8s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.2187 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 8s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.2192 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 8s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2183 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2162 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 8s - loss: 0.0187 - acc: 0.0000e+00 - val_loss: 0.2131 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 8s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2092 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 8s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.2050 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 8s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.2002 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 8s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 8s - loss: 0.0197 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 8s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1806 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1763 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 8s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.1731 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 8s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1704 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 8s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.1685 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1677 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 8s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1676 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 8s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1679 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1688 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 8s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1701 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 8s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1716 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 9s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1731 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 8s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1748 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 9s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1782 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 9s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1796 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 8s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 9s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1808 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 10s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02812 MSE (0.17 RMSE)\n",
      "Test Score: 0.32655 MSE (0.57 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_135 (LSTM)              (None, 120, 32)           4736      \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_136 (LSTM)              (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 15,233\n",
      "Trainable params: 15,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 45s - loss: 0.1191 - acc: 0.0000e+00 - val_loss: 0.5519 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 10s - loss: 0.1172 - acc: 0.0000e+00 - val_loss: 0.5454 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 8s - loss: 0.1152 - acc: 0.0000e+00 - val_loss: 0.5379 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 8s - loss: 0.1129 - acc: 0.0000e+00 - val_loss: 0.5291 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 8s - loss: 0.1102 - acc: 0.0000e+00 - val_loss: 0.5186 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 9s - loss: 0.1069 - acc: 0.0000e+00 - val_loss: 0.5062 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 9s - loss: 0.1031 - acc: 0.0000e+00 - val_loss: 0.4915 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 9s - loss: 0.0990 - acc: 0.0000e+00 - val_loss: 0.4743 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 9s - loss: 0.0946 - acc: 0.0000e+00 - val_loss: 0.4541 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 8s - loss: 0.0885 - acc: 0.0000e+00 - val_loss: 0.4309 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 9s - loss: 0.0813 - acc: 0.0000e+00 - val_loss: 0.4042 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 9s - loss: 0.0729 - acc: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 9s - loss: 0.0648 - acc: 0.0000e+00 - val_loss: 0.3399 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 8s - loss: 0.0559 - acc: 0.0000e+00 - val_loss: 0.3023 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 8s - loss: 0.0457 - acc: 0.0000e+00 - val_loss: 0.2614 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 9s - loss: 0.0346 - acc: 0.0000e+00 - val_loss: 0.2183 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 8s - loss: 0.0245 - acc: 0.0000e+00 - val_loss: 0.1745 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 9s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1331 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 9s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.0982 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 8s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0767 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 9s - loss: 0.0282 - acc: 0.0000e+00 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 8s - loss: 0.0338 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 9s - loss: 0.0279 - acc: 0.0000e+00 - val_loss: 0.0788 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 8s - loss: 0.0261 - acc: 0.0000e+00 - val_loss: 0.0908 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 8s - loss: 0.0240 - acc: 0.0000e+00 - val_loss: 0.1056 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 9s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1216 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 8s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1378 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 8s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1525 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1654 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 9s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1756 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 8s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1833 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 8s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1883 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 8s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.1905 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 9s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 8s - loss: 0.0199 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 8s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 8s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1757 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 8s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1674 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 9s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1583 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 8s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1485 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 8s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 8s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1300 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 8s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1219 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 8s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1150 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 9s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1096 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 8s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1060 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 9s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1041 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 9s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1041 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 9s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1053 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 8s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1074 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 8s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1106 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 8s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1141 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 8s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1173 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 8s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1198 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 8s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1215 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 8s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1223 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 9s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1218 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 8s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1200 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 9s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1164 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 8s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1115 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 8s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 8s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.0976 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 8s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0906 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 8s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0839 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 9s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0781 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 9s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.0739 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 8s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.0714 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 8s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.0696 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 9s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0691 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 9s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0691 - val_acc: 0.0000e+00\n",
      "Train Score: 0.01101 MSE (0.10 RMSE)\n",
      "Test Score: 0.13947 MSE (0.37 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_137 (LSTM)              (None, 120, 64)           17664     \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_138 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 51,745\n",
      "Trainable params: 51,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 58s - loss: 0.1200 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 21s - loss: 0.1185 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 20s - loss: 0.1173 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 20s - loss: 0.1159 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 19s - loss: 0.1144 - acc: 0.0000e+00 - val_loss: 0.5388 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 19s - loss: 0.1124 - acc: 0.0000e+00 - val_loss: 0.5317 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 19s - loss: 0.1105 - acc: 0.0000e+00 - val_loss: 0.5231 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 19s - loss: 0.1079 - acc: 0.0000e+00 - val_loss: 0.5124 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 20s - loss: 0.1046 - acc: 0.0000e+00 - val_loss: 0.4990 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 19s - loss: 0.1006 - acc: 0.0000e+00 - val_loss: 0.4827 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 19s - loss: 0.0957 - acc: 0.0000e+00 - val_loss: 0.4633 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 21s - loss: 0.0890 - acc: 0.0000e+00 - val_loss: 0.4412 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 19s - loss: 0.0796 - acc: 0.0000e+00 - val_loss: 0.4168 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 20s - loss: 0.0707 - acc: 0.0000e+00 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 21s - loss: 0.0611 - acc: 0.0000e+00 - val_loss: 0.3642 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 19s - loss: 0.0509 - acc: 0.0000e+00 - val_loss: 0.3379 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 19s - loss: 0.0417 - acc: 0.0000e+00 - val_loss: 0.3116 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 20s - loss: 0.0343 - acc: 0.0000e+00 - val_loss: 0.2848 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 19s - loss: 0.0286 - acc: 0.0000e+00 - val_loss: 0.2575 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 20s - loss: 0.0216 - acc: 0.0000e+00 - val_loss: 0.2296 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 20s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.2014 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 19s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1750 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 21s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1518 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 21s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1351 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 19s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1254 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 19s - loss: 0.0219 - acc: 0.0000e+00 - val_loss: 0.1222 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 20s - loss: 0.0196 - acc: 0.0000e+00 - val_loss: 0.1238 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 19s - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.1294 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 21s - loss: 0.0215 - acc: 0.0000e+00 - val_loss: 0.1379 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 19s - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.1485 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 19s - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 21s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1717 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 19s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1832 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 19s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 20s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.2029 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 19s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.2105 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 19s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.2164 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 20s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.2205 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 19s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2231 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 20s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.2238 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 19s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2230 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 19s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2210 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 19s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2178 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 20s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.2136 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 19s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2087 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 20s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.2030 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 19s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1973 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 21s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1916 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 19s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1859 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 19s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1813 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 20s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1775 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 20s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1745 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 20s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1725 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 19s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1716 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 20s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1718 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 19s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1729 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 19s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1748 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 20s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 20s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 19s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1836 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 21s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1867 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 19s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1891 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 20s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1911 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 20s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1927 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 19s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1938 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 21s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 19s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1945 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 19s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 20s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1939 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 19s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1930 - val_acc: 0.0000e+00\n",
      "Train Score: 0.03038 MSE (0.17 RMSE)\n",
      "Test Score: 0.34719 MSE (0.59 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_139 (LSTM)              (None, 120, 64)           17664     \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_140 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,801\n",
      "Trainable params: 52,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 55s - loss: 0.1183 - acc: 0.0000e+00 - val_loss: 0.5507 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 20s - loss: 0.1159 - acc: 0.0000e+00 - val_loss: 0.5395 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 20s - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.5254 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 20s - loss: 0.1093 - acc: 0.0000e+00 - val_loss: 0.5073 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 20s - loss: 0.1042 - acc: 0.0000e+00 - val_loss: 0.4844 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 21s - loss: 0.0979 - acc: 0.0000e+00 - val_loss: 0.4556 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 20s - loss: 0.0902 - acc: 0.0000e+00 - val_loss: 0.4198 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 20s - loss: 0.0798 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 20s - loss: 0.0671 - acc: 0.0000e+00 - val_loss: 0.3288 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 21s - loss: 0.0510 - acc: 0.0000e+00 - val_loss: 0.2781 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 20s - loss: 0.0357 - acc: 0.0000e+00 - val_loss: 0.2280 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 21s - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.1811 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 20s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 20s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1109 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 20s - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.0988 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 20s - loss: 0.0303 - acc: 0.0000e+00 - val_loss: 0.0995 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 21s - loss: 0.0278 - acc: 0.0000e+00 - val_loss: 0.1073 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 20s - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.1204 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 21s - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.1367 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 20s - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.1545 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 20s - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.1727 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 22s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 21s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.2043 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 20s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2159 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 21s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.2241 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 20s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2294 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 20s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.2315 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 20s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.2311 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 20s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2286 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 21s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2239 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 20s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2176 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 20s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.2102 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 21s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.2022 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 22s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 20s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1854 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 21s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1778 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 21s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1714 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 21s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.1660 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 20s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1621 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 20s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 20s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1598 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 20s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1612 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 21s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1641 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 20s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1675 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 21s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1713 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 20s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1751 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 22s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1787 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 20s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1819 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 21s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1847 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 20s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1870 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 21s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1886 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 21s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1893 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 20s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1890 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 20s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1882 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 20s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1868 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 20s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1849 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 21s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1820 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 22s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1789 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 21s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1751 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 20s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1719 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 22s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1691 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 21s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1668 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 21s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1652 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 21s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1638 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 21s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1635 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 20s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1641 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 20s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1646 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 21s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1650 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 21s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1651 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 20s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1650 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02580 MSE (0.16 RMSE)\n",
      "Test Score: 0.30201 MSE (0.55 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_141 (LSTM)              (None, 120, 64)           17664     \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_142 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 54,913\n",
      "Trainable params: 54,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 59s - loss: 0.1186 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 22s - loss: 0.1150 - acc: 0.0000e+00 - val_loss: 0.5305 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 22s - loss: 0.1110 - acc: 0.0000e+00 - val_loss: 0.5123 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 22s - loss: 0.1056 - acc: 0.0000e+00 - val_loss: 0.4893 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 23s - loss: 0.0993 - acc: 0.0000e+00 - val_loss: 0.4587 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 22s - loss: 0.0912 - acc: 0.0000e+00 - val_loss: 0.4175 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 22s - loss: 0.0807 - acc: 0.0000e+00 - val_loss: 0.3635 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 22s - loss: 0.0652 - acc: 0.0000e+00 - val_loss: 0.2959 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 22s - loss: 0.0464 - acc: 0.0000e+00 - val_loss: 0.2203 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 22s - loss: 0.0297 - acc: 0.0000e+00 - val_loss: 0.1454 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 21s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.0830 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 22s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.0557 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 22s - loss: 0.0292 - acc: 0.0000e+00 - val_loss: 0.0546 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 22s - loss: 0.0323 - acc: 0.0000e+00 - val_loss: 0.0669 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 22s - loss: 0.0261 - acc: 0.0000e+00 - val_loss: 0.0878 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 22s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1128 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 22s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 22s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.1628 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 22s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1808 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 23s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.1923 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 23s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1973 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 23s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1967 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 21s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 22s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1818 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 21s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1697 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 22s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1554 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 22s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 21s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1265 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 22s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1144 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 22s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1056 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 22s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1004 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 22s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.0992 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 22s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1009 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 22s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1054 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 22s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.1111 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 22s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1175 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 21s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1239 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 22s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1289 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 23s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1324 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 22s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1341 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 22s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1330 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 22s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1296 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 22s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1246 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 22s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1181 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 22s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1105 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 22s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1025 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 22s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.0946 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 22s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0876 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 22s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0817 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 22s - loss: 0.0091 - acc: 0.0000e+00 - val_loss: 0.0780 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 21s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 22s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0761 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 21s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 22s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0766 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 22s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 22s - loss: 0.0079 - acc: 0.0000e+00 - val_loss: 0.0755 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 24s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0726 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 22s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0680 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 21s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 22s - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.0545 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 21s - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0482 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 22s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0429 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 22s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 21s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 22s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 22s - loss: 0.0079 - acc: 0.0000e+00 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 23s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 22s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 22s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 22s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0241 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00477 MSE (0.07 RMSE)\n",
      "Test Score: 0.05290 MSE (0.23 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_143 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_144 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 201,761\n",
      "Trainable params: 201,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 67s - loss: 0.1195 - acc: 0.0000e+00 - val_loss: 0.5499 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 28s - loss: 0.1169 - acc: 0.0000e+00 - val_loss: 0.5401 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 26s - loss: 0.1143 - acc: 0.0000e+00 - val_loss: 0.5261 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 26s - loss: 0.1106 - acc: 0.0000e+00 - val_loss: 0.5051 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 29s - loss: 0.1056 - acc: 0.0000e+00 - val_loss: 0.4730 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 28s - loss: 0.0972 - acc: 0.0000e+00 - val_loss: 0.4293 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 27s - loss: 0.0838 - acc: 0.0000e+00 - val_loss: 0.3814 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 28s - loss: 0.0638 - acc: 0.0000e+00 - val_loss: 0.3359 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 28s - loss: 0.0433 - acc: 0.0000e+00 - val_loss: 0.2939 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 28s - loss: 0.0301 - acc: 0.0000e+00 - val_loss: 0.2538 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 29s - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.2138 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 30s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 27s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1437 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 28s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1254 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 27s - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.1190 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 27s - loss: 0.0218 - acc: 0.0000e+00 - val_loss: 0.1224 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 29s - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.1322 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 30s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1455 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 28s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.1608 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 28s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1766 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 27s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1916 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 27s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.2046 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 28s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.2144 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 29s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.2210 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 29s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.2242 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 29s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.2244 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 28s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.2218 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 28s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.2173 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 28s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.2110 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 28s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.2033 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 29s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 31s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1867 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 28s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1792 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 31s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1727 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 28s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1680 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 29s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1649 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 30s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1640 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 29s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1650 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 29s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1677 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 29s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 30s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 29s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1826 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 29s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1874 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 28s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1915 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 30s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 29s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1968 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 30s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1979 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 28s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1977 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 28s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1965 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 29s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1942 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 30s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1912 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 29s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1880 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 30s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.1848 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 30s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1817 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 28s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1792 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 28s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1773 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 28s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1762 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 29s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1759 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 29s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1765 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 30s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1780 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 29s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 28s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1824 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 31s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1843 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 31s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1860 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 30s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 29s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1881 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 29s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1887 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 30s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1891 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 28s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1884 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 30s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02966 MSE (0.17 RMSE)\n",
      "Test Score: 0.33996 MSE (0.58 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_145 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_146 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 104s - loss: 0.1194 - acc: 0.0000e+00 - val_loss: 0.5511 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 34s - loss: 0.1167 - acc: 0.0000e+00 - val_loss: 0.5396 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 31s - loss: 0.1135 - acc: 0.0000e+00 - val_loss: 0.5217 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 31s - loss: 0.1090 - acc: 0.0000e+00 - val_loss: 0.4944 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 31s - loss: 0.1027 - acc: 0.0000e+00 - val_loss: 0.4508 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 31s - loss: 0.0916 - acc: 0.0000e+00 - val_loss: 0.3872 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 31s - loss: 0.0749 - acc: 0.0000e+00 - val_loss: 0.3085 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 31s - loss: 0.0475 - acc: 0.0000e+00 - val_loss: 0.2285 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 30s - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.1572 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 31s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1055 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 33s - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0926 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 31s - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.0994 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 31s - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.1162 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 34s - loss: 0.0211 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 32s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.1643 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 32s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1892 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 31s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.2100 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 33s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.2249 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 31s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2341 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 31s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2380 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 30s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.2368 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 31s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.2316 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 32s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.2232 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 34s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.2128 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 31s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.2009 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 33s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1885 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 31s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1768 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 33s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1666 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 32s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1587 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 31s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1538 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 31s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 33s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1533 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 31s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1565 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 31s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1614 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 34s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1678 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 33s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1743 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 33s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1806 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 31s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1858 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 33s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 31s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1928 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 32s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1945 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 32s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 31s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1933 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 31s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1906 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 32s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 32s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1818 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 32s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1765 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 32s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1711 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 31s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1663 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 33s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1621 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 34s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1595 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 31s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1577 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 31s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1571 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 31s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1576 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 33s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1589 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 32s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 32s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1604 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 32s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1596 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 31s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1574 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 32s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1538 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 32s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1498 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 32s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1462 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 32s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1440 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 32s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 32s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1382 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 32s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1323 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 31s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1316 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 31s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1281 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 31s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1222 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 31s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1177 - val_acc: 0.0000e+00\n",
      "Train Score: 0.01830 MSE (0.14 RMSE)\n",
      "Test Score: 0.23721 MSE (0.49 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_147 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_148 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 84s - loss: 0.1179 - acc: 0.0000e+00 - val_loss: 0.5253 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 30s - loss: 0.1107 - acc: 0.0000e+00 - val_loss: 0.4866 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 30s - loss: 0.1013 - acc: 0.0000e+00 - val_loss: 0.4316 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 29s - loss: 0.0888 - acc: 0.0000e+00 - val_loss: 0.3485 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 29s - loss: 0.0685 - acc: 0.0000e+00 - val_loss: 0.2337 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 30s - loss: 0.0391 - acc: 0.0000e+00 - val_loss: 0.1079 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 30s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 30s - loss: 0.0368 - acc: 0.0000e+00 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 31s - loss: 0.0338 - acc: 0.0000e+00 - val_loss: 0.0551 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 31s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.0961 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 31s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1370 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 30s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1657 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 30s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1803 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 30s - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 30s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 31s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1611 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 30s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1429 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 31s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1233 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 31s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1051 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 31s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0900 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 30s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0799 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 32s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 30s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.0774 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 30s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0829 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 34s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0914 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 31s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1006 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 30s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1099 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 32s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1178 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 31s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.1228 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 32s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1246 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 31s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1226 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 30s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1170 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 32s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.1082 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 30s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0979 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 31s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0871 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 31s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0773 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 30s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 32s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0642 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 30s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 30s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 30s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0627 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 33s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0648 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 31s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0671 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 31s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 31s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0657 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 30s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 32s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0533 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 32s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0449 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 30s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 30s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0302 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 30s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 30s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 31s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 31s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 30s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 31s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 30s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 30s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0130 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 33s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 31s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 31s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0060 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 32s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 30s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 31s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 30s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 31s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 30s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 30s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 31s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 31s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00210 MSE (0.05 RMSE)\n",
      "Test Score: 0.00886 MSE (0.09 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_149 (LSTM)              (None, 120, 256)          267264    \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 120, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_150 (LSTM)              (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 796,705\n",
      "Trainable params: 796,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 96s - loss: 0.1195 - acc: 0.0000e+00 - val_loss: 0.5481 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 44s - loss: 0.1158 - acc: 0.0000e+00 - val_loss: 0.5201 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 42s - loss: 0.1093 - acc: 0.0000e+00 - val_loss: 0.4662 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 43s - loss: 0.0962 - acc: 0.0000e+00 - val_loss: 0.3655 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 45s - loss: 0.0673 - acc: 0.0000e+00 - val_loss: 0.2572 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 43s - loss: 0.0261 - acc: 0.0000e+00 - val_loss: 0.1718 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 42s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1171 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 45s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.1142 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 45s - loss: 0.0216 - acc: 0.0000e+00 - val_loss: 0.1327 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 42s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1620 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 44s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1933 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 44s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.2187 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 44s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.2309 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 46s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.2324 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 46s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.2251 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 45s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.2118 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 43s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1964 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 44s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 44s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1671 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 43s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1586 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 44s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1550 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 44s - loss: 0.0151 - acc: 0.0000e+00 - val_loss: 0.1570 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 44s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1632 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 44s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1725 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 45s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 42s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1916 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 44s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1982 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 44s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.2022 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 43s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.2032 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 44s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.2010 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 43s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1961 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 44s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1895 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 45s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1823 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 45s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 44s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1705 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 46s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1679 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 43s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1679 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 46s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1699 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 46s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1738 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 44s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1785 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 45s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1826 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 43s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1861 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 45s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1884 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 45s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1895 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 42s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 45s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1873 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 46s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1853 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 44s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1831 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 45s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1809 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 47s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1790 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 45s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1774 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 45s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1766 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 45s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 46s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1786 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 46s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 50s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 44s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1839 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 44s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1846 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 46s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 45s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1837 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 45s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 45s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1797 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 46s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1777 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 46s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1770 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 46s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1766 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 45s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 49s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1755 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 45s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1770 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 45s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1780 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 46s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02842 MSE (0.17 RMSE)\n",
      "Test Score: 0.32592 MSE (0.57 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_151 (LSTM)              (None, 120, 256)          267264    \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 120, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_152 (LSTM)              (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 800,833\n",
      "Trainable params: 800,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 110s - loss: 0.1205 - acc: 0.0000e+00 - val_loss: 0.5413 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 47s - loss: 0.1147 - acc: 0.0000e+00 - val_loss: 0.5060 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 46s - loss: 0.1064 - acc: 0.0000e+00 - val_loss: 0.4317 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 47s - loss: 0.0880 - acc: 0.0000e+00 - val_loss: 0.2964 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 47s - loss: 0.0498 - acc: 0.0000e+00 - val_loss: 0.1663 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 48s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.0875 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 48s - loss: 0.0284 - acc: 0.0000e+00 - val_loss: 0.0972 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 49s - loss: 0.0246 - acc: 0.0000e+00 - val_loss: 0.1283 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 46s - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.1689 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 46s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.2083 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 47s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.2362 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 47s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.2504 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 49s - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.2528 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 48s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2462 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 48s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2337 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 48s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.2180 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 47s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.2008 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 48s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1841 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 47s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1705 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 47s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1611 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 47s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1570 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 48s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.1579 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 47s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1626 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 49s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1702 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 48s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1794 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 47s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 47s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1975 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 47s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.2042 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 46s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.2083 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 48s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.2094 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 49s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.2083 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 47s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.2050 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 48s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.2005 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 50s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1947 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 48s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1889 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 47s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1834 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 49s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1783 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 48s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1750 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 47s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1735 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 47s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 48s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 47s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 48s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1842 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 49s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1884 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 49s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 49s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 49s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1961 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 46s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1962 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 48s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 49s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1915 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 49s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1875 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 49s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1838 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 48s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 49s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1773 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 48s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1760 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 50s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1767 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 50s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1801 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 50s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1787 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 48s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1736 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 47s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1712 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 48s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 48s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1737 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 48s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1685 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 47s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 48s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1621 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 49s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 48s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1605 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 48s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1543 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 48s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 47s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1522 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02435 MSE (0.16 RMSE)\n",
      "Test Score: 0.28798 MSE (0.54 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_153 (LSTM)              (None, 120, 256)          267264    \n",
      "_________________________________________________________________\n",
      "dropout_153 (Dropout)        (None, 120, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_154 (LSTM)              (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 809,089\n",
      "Trainable params: 809,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 113s - loss: 0.1189 - acc: 0.0000e+00 - val_loss: 0.5169 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 50s - loss: 0.1083 - acc: 0.0000e+00 - val_loss: 0.4367 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 49s - loss: 0.0902 - acc: 0.0000e+00 - val_loss: 0.2724 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 49s - loss: 0.0518 - acc: 0.0000e+00 - val_loss: 0.0451 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 50s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 49s - loss: 0.0402 - acc: 0.0000e+00 - val_loss: 0.0623 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 49s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.1419 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 49s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1795 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 49s - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 49s - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.1572 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 49s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1218 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 48s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.0859 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 48s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0601 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 48s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.0508 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 48s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.0553 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 47s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0709 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 48s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0927 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 49s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.1136 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 48s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.1288 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 48s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1360 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 50s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1347 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 48s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1264 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 47s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1128 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 48s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0966 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 47s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0812 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 47s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0686 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 49s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 47s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0596 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 48s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0627 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 47s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0697 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 48s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0776 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 47s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0837 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 50s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0864 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 49s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0844 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 48s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0776 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 50s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 49s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0566 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 47s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0468 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 48s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 47s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 48s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 47s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 48s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 48s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 48s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 50s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0305 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 49s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 47s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0183 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 48s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 49s - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0117 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 48s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0111 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 48s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 47s - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 47s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 49s - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 48s - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 48s - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 49s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 48s - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 48s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 48s - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 48s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0119 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 47s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0103 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 49s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0105 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 49s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 48s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 48s - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 48s - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 48s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 51s - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00185 MSE (0.04 RMSE)\n",
      "Test Score: 0.01143 MSE (0.11 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_155 (LSTM)              (None, 120, 512)          1058816   \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 120, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_156 (LSTM)              (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,166,241\n",
      "Trainable params: 3,166,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 194s - loss: 0.1198 - acc: 0.0000e+00 - val_loss: 0.5465 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 87s - loss: 0.1156 - acc: 0.0000e+00 - val_loss: 0.4864 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 78s - loss: 0.1016 - acc: 0.0000e+00 - val_loss: 0.3073 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 83s - loss: 0.0466 - acc: 0.0000e+00 - val_loss: 0.1487 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 80s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1331 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 80s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.1710 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 82s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.2124 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 82s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.2232 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 86s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.2067 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 81s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1786 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 82s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1590 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 85s - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.1572 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 83s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1692 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 80s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 81s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1969 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 84s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.2021 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 81s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1995 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 83s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 84s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 81s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1725 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 80s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1684 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 81s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1693 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 83s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1762 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 84s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1840 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 85s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1904 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 82s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1925 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 83s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1909 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 81s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 83s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1817 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 85s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 84s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1745 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 82s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1746 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 87s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1772 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 86s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1808 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 85s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1844 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 83s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1865 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 86s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1864 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 87s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 86s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1800 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 88s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1770 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 87s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 84s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1776 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 86s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1786 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 84s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1818 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 87s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1811 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 84s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 90s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 88s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1775 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 85s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1814 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 86s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1812 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 86s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1812 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 84s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 84s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1809 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 85s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1800 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 87s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1768 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 86s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1772 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 84s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1792 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 84s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1813 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 85s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1750 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 87s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 85s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 86s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1824 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 84s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1791 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 85s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1748 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 87s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1750 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 85s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1771 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 85s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1797 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 86s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 85s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1848 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 85s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1838 - val_acc: 0.0000e+00\n",
      "Train Score: 0.02917 MSE (0.17 RMSE)\n",
      "Test Score: 0.33436 MSE (0.58 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_157 (LSTM)              (None, 120, 512)          1058816   \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 120, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_158 (LSTM)              (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,174,465\n",
      "Trainable params: 3,174,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 197s - loss: 0.1188 - acc: 0.0000e+00 - val_loss: 0.4920 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 89s - loss: 0.1032 - acc: 0.0000e+00 - val_loss: 0.2753 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 90s - loss: 0.0530 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 86s - loss: 0.1245 - acc: 0.0000e+00 - val_loss: 0.1081 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 89s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.2209 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 89s - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.2497 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 84s - loss: 0.0271 - acc: 0.0000e+00 - val_loss: 0.2399 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 85s - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.2189 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 87s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1961 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 86s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1744 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 89s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1579 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 86s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1501 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 87s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1512 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 86s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1577 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 86s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1676 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 86s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1788 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 88s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1892 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 85s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1966 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 87s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 87s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.2019 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 86s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.2001 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 84s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1957 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 84s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1892 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 89s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1815 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 86s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1730 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 84s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1644 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 91s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1564 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 89s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1489 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 88s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 87s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1363 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 91s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1336 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 86s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1318 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 93s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1365 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 94s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1288 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 90s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1361 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 90s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1246 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 93s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1053 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 91s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1115 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 94s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1347 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 124s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1675 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 109s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 102s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1807 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 102s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1685 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 90s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1502 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 90s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1295 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 91s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1119 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 126s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 128s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1079 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 124s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1166 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 122s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1264 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 119s - loss: 0.0091 - acc: 0.0000e+00 - val_loss: 0.1346 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 109s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 96s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1413 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 90s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1376 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 91s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1297 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 96s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1179 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 99s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.1033 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 93s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0906 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 92s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0816 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 75s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0804 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 98s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0865 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 87s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0895 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 89s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0830 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 91s - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0680 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 90s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0653 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 95s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0679 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 95s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0563 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 88s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 92s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0566 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 90s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0680 - val_acc: 0.0000e+00\n",
      "Train Score: 0.01311 MSE (0.11 RMSE)\n",
      "Test Score: 0.13594 MSE (0.37 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_159 (LSTM)              (None, 120, 512)          1058816   \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 120, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_160 (LSTM)              (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 200s - loss: 0.1204 - acc: 0.0000e+00 - val_loss: 0.5063 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 90s - loss: 0.1064 - acc: 0.0000e+00 - val_loss: 0.3323 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 90s - loss: 0.0676 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 91s - loss: 0.0467 - acc: 0.0000e+00 - val_loss: 0.1743 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 86s - loss: 0.0282 - acc: 0.0000e+00 - val_loss: 0.2464 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 88s - loss: 0.0418 - acc: 0.0000e+00 - val_loss: 0.2392 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 87s - loss: 0.0387 - acc: 0.0000e+00 - val_loss: 0.1936 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 91s - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.1288 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 87s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0700 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 87s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.0497 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 90s - loss: 0.0212 - acc: 0.0000e+00 - val_loss: 0.0630 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 91s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.0946 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 95s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1300 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 94s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1580 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 91s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1737 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 89s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 87s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1683 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 86s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1524 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 87s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1325 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 87s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1123 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 87s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0950 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 86s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0839 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 92s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.0804 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 95s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0846 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 90s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0939 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 90s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.1064 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 91s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.1181 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 92s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.1269 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 89s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.1308 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 90s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.1292 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 92s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.1221 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 86s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.1110 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 87s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0975 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 91s - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.0836 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 91s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0718 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 91s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0642 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 87s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0624 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 95s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0654 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 91s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0699 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 88s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0742 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 87s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0747 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 89s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 94s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 97s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0417 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 85s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0264 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 92s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 91s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 96s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 91s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 90s - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 91s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 91s - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 98s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 93s - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 90s - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 88s - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 90s - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0075 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 91s - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 94s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 93s - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 90s - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 91s - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 89s - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 91s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 92s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 95s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 93s - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 94s - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 90s - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 91s - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0083 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00212 MSE (0.05 RMSE)\n",
      "Test Score: 0.01189 MSE (0.11 RMSE)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 70\n",
    "dropout = 0.7\n",
    "neuronlist1 = [32, 64, 128, 256, 512]\n",
    "neuronlist2 = [16, 32, 64]\n",
    "neurons_result = {}\n",
    "\n",
    "for neuron_lstm in neuronlist1:\n",
    "    neurons = [neuron_lstm, neuron_lstm]\n",
    "    for activation in neuronlist2:\n",
    "        neurons.append(activation)\n",
    "        neurons.append(1)\n",
    "        trainScore, testScore = quick_measure(file_csv_name, window_size, d, shape, neurons, epochs)\n",
    "        neurons_result[str(neurons)] = testScore\n",
    "        neurons = neurons[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAI6CAYAAAB4ng+2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQCMyRoAwJisAQCMyRoAwJisAQCMBURjB+E+/v1qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15c443e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(neurons_result.items())\n",
    "x,y = zip(*lists)\n",
    "\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('neurons')\n",
    "plt.ylabel('Mean Square Error')\n",
    "\n",
    "plt.bar(range(len(lists)), y, align='center')\n",
    "plt.xticks(range(len(lists)), x)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "shape = [4, window_size, 1] # feature, window, output\n",
    "neurons = [128, 128, 64, 1]\n",
    "epochs = 70\n",
    "decaylist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "d = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d, decay):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_measure(file_csv_name, window_size, d, shape, neurons, epochs, decay):\n",
    "    df = get_stock_price(file_csv_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, window_size)\n",
    "    model = build_model2(shape, neurons, d, decay)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_161 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_162 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_162 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 121s - loss: 0.1184 - acc: 0.0000e+00 - val_loss: 0.5442 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 37s - loss: 0.1146 - acc: 0.0000e+00 - val_loss: 0.5198 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 39s - loss: 0.1082 - acc: 0.0000e+00 - val_loss: 0.4772 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 38s - loss: 0.0985 - acc: 0.0000e+00 - val_loss: 0.4169 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 40s - loss: 0.0840 - acc: 0.0000e+00 - val_loss: 0.3259 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 36s - loss: 0.0598 - acc: 0.0000e+00 - val_loss: 0.2133 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 37s - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.1045 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 38s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 37s - loss: 0.0350 - acc: 0.0000e+00 - val_loss: 0.0430 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 40s - loss: 0.0348 - acc: 0.0000e+00 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 39s - loss: 0.0192 - acc: 0.0000e+00 - val_loss: 0.1074 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 39s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1471 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 39s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1795 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 39s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.2001 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 39s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.2091 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 39s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.2084 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 40s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.2000 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 40s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1859 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 37s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1683 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 39s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1499 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 39s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1323 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 38s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1179 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 38s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1087 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 39s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1041 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 36s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1043 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 37s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1085 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 37s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1156 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 39s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.1249 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 38s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1348 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 37s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 39s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1496 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 38s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1524 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 40s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 40s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.1481 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 40s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1415 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 37s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1327 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 39s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.1224 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 38s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1125 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 40s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.1039 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 39s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0968 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 38s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0910 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 39s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.0876 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 38s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0862 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 39s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0867 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 39s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0876 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 38s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0876 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 39s - loss: 0.0079 - acc: 0.0000e+00 - val_loss: 0.0855 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 37s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0809 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 38s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0727 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 38s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 39s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0502 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 40s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 39s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 40s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 41s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0302 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 42s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0291 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 39s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 40s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 44s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 55s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 55s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 54s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 50s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 50s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 51s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 55s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 55s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 56s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 53s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 48s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00232 MSE (0.05 RMSE)\n",
      "Test Score: 0.01248 MSE (0.11 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_163 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_164 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 153s - loss: 0.1191 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 53s - loss: 0.1146 - acc: 0.0000e+00 - val_loss: 0.5217 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 51s - loss: 0.1088 - acc: 0.0000e+00 - val_loss: 0.4897 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 53s - loss: 0.1009 - acc: 0.0000e+00 - val_loss: 0.4413 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 56s - loss: 0.0893 - acc: 0.0000e+00 - val_loss: 0.3680 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 53s - loss: 0.0698 - acc: 0.0000e+00 - val_loss: 0.2667 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 55s - loss: 0.0437 - acc: 0.0000e+00 - val_loss: 0.1514 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 55s - loss: 0.0148 - acc: 0.0000e+00 - val_loss: 0.0538 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 54s - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 51s - loss: 0.0337 - acc: 0.0000e+00 - val_loss: 0.0562 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 57s - loss: 0.0228 - acc: 0.0000e+00 - val_loss: 0.0893 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 57s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1273 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 54s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1602 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 55s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1823 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 57s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1928 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 52s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1934 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 48s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.1863 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 51s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1735 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 58s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1574 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 57s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1399 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 41s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1232 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 46s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1100 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 40s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1008 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 40s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0971 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 41s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0991 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 39s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1051 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 40s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1137 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 43s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.1231 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 44s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1316 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 44s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 42s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.1432 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 43s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1444 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 42s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1418 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 42s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1357 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 41s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.1273 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 43s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1173 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 43s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1066 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 43s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0966 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 43s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.0882 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 42s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0823 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 42s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0799 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 42s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0800 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 43s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0820 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 42s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0841 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 39s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0860 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 38s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0858 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 40s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0828 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 41s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0768 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 42s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0688 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 41s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 40s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0512 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 41s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 41s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 45s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 40s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 40s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 40s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 41s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 42s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 45s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 41s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 42s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0106 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 43s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 43s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 43s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0114 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 43s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0120 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 42s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0099 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 40s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 39s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 43s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00190 MSE (0.04 RMSE)\n",
      "Test Score: 0.00955 MSE (0.10 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_165 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_166 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 120s - loss: 0.1203 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 40s - loss: 0.1155 - acc: 0.0000e+00 - val_loss: 0.5223 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 38s - loss: 0.1098 - acc: 0.0000e+00 - val_loss: 0.4922 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 40s - loss: 0.1021 - acc: 0.0000e+00 - val_loss: 0.4468 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 40s - loss: 0.0910 - acc: 0.0000e+00 - val_loss: 0.3792 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 44s - loss: 0.0730 - acc: 0.0000e+00 - val_loss: 0.2894 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 41s - loss: 0.0454 - acc: 0.0000e+00 - val_loss: 0.1895 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 44s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1017 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 43s - loss: 0.0150 - acc: 0.0000e+00 - val_loss: 0.0681 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 41s - loss: 0.0315 - acc: 0.0000e+00 - val_loss: 0.0749 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 41s - loss: 0.0288 - acc: 0.0000e+00 - val_loss: 0.0976 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 39s - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.1279 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 42s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.1598 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 42s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.1871 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 42s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.2074 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 43s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.2194 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 43s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.2238 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 44s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2220 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 42s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.2156 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 42s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.2057 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 42s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.1932 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 42s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1796 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 42s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1660 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 40s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1541 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 41s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1445 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 40s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 40s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1363 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 42s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1376 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 39s - loss: 0.0143 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 44s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1477 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 41s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1547 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 38s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1622 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 39s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1685 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 40s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1731 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 42s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1755 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 39s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1754 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 40s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1728 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 40s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 40s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1619 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 42s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1546 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 40s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1470 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 43s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1399 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 43s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1336 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 42s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1290 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 44s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1261 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 42s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1249 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 39s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.1254 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 40s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1268 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 39s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1281 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 41s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1271 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 43s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1242 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 41s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.1192 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 42s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1113 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 43s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.1017 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 44s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0912 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 43s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0824 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 42s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0740 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 46s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0687 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 44s - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0655 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 42s - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0609 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 44s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0522 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 44s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 43s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 43s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 44s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 42s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 42s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 43s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 41s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 41s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00253 MSE (0.05 RMSE)\n",
      "Test Score: 0.03262 MSE (0.18 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_167 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_168 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 117s - loss: 0.1193 - acc: 0.0000e+00 - val_loss: 0.5414 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 40s - loss: 0.1145 - acc: 0.0000e+00 - val_loss: 0.5197 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 42s - loss: 0.1082 - acc: 0.0000e+00 - val_loss: 0.4843 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 41s - loss: 0.0996 - acc: 0.0000e+00 - val_loss: 0.4301 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 39s - loss: 0.0855 - acc: 0.0000e+00 - val_loss: 0.3490 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 39s - loss: 0.0641 - acc: 0.0000e+00 - val_loss: 0.2456 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 40s - loss: 0.0341 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 43s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0591 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 40s - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0560 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 39s - loss: 0.0327 - acc: 0.0000e+00 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 41s - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.1084 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 39s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1443 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 44s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1767 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 43s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.2009 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 38s - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.2145 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 38s - loss: 0.0175 - acc: 0.0000e+00 - val_loss: 0.2186 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 41s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.2147 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 38s - loss: 0.0184 - acc: 0.0000e+00 - val_loss: 0.2049 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 41s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 42s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1753 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 42s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1584 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 41s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1427 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 41s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1296 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 44s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1197 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 43s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1143 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 44s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1132 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 42s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1158 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 42s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1221 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 44s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1305 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 41s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 42s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1481 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 43s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1549 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 43s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1588 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 39s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1590 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 42s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1556 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 43s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1488 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 42s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 43s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1277 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 41s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1156 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 42s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1043 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 43s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0955 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 40s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0898 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 43s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0861 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 42s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0842 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 43s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0830 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 43s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0810 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 44s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0761 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 44s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0676 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 43s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0570 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 44s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0456 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 45s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 46s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 45s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 44s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 46s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 42s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 44s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 42s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 43s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 43s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 42s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 44s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 44s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 43s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 42s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 42s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 41s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 57s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 57s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 56s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00265 MSE (0.05 RMSE)\n",
      "Test Score: 0.01527 MSE (0.12 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_169 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_169 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_170 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_170 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 162s - loss: 0.1196 - acc: 0.0000e+00 - val_loss: 0.5349 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 59s - loss: 0.1136 - acc: 0.0000e+00 - val_loss: 0.5083 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 56s - loss: 0.1073 - acc: 0.0000e+00 - val_loss: 0.4742 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 59s - loss: 0.0991 - acc: 0.0000e+00 - val_loss: 0.4161 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 61s - loss: 0.0846 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 58s - loss: 0.0623 - acc: 0.0000e+00 - val_loss: 0.2136 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 57s - loss: 0.0324 - acc: 0.0000e+00 - val_loss: 0.1033 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 56s - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 60s - loss: 0.0342 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 57s - loss: 0.0333 - acc: 0.0000e+00 - val_loss: 0.0648 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 56s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.0998 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 57s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1377 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 56s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.1692 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 57s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1906 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 55s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 59s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.2014 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 58s - loss: 0.0178 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 54s - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.1834 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 59s - loss: 0.0153 - acc: 0.0000e+00 - val_loss: 0.1683 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 63s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1519 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 61s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1359 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 60s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1221 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 61s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1113 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 57s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 56s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1032 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 59s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1061 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 59s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1121 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 56s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1208 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 56s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1300 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 59s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 60s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1453 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 57s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1498 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 57s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1520 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 58s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1510 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 61s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1470 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 63s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 59s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1316 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 61s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1218 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 57s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.1124 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 58s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1038 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 61s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0969 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 57s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0925 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 60s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0896 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 60s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0890 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 52s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.0890 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 55s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0894 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 63s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0893 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 57s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0873 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 55s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0828 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 60s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 62s - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0679 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 57s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0591 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 53s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0507 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 44s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 45s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 45s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 44s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 44s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 44s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 45s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 45s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0134 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 45s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 46s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 47s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 42s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 39s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 41s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 42s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 44s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 47s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00196 MSE (0.04 RMSE)\n",
      "Test Score: 0.00826 MSE (0.09 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_171 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_171 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_172 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_172 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 162s - loss: 0.1213 - acc: 0.0000e+00 - val_loss: 0.5418 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 42s - loss: 0.1158 - acc: 0.0000e+00 - val_loss: 0.5163 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 43s - loss: 0.1095 - acc: 0.0000e+00 - val_loss: 0.4803 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 43s - loss: 0.1006 - acc: 0.0000e+00 - val_loss: 0.4252 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 41s - loss: 0.0879 - acc: 0.0000e+00 - val_loss: 0.3429 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 42s - loss: 0.0678 - acc: 0.0000e+00 - val_loss: 0.2343 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 41s - loss: 0.0387 - acc: 0.0000e+00 - val_loss: 0.1212 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 42s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 44s - loss: 0.0321 - acc: 0.0000e+00 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 43s - loss: 0.0337 - acc: 0.0000e+00 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 44s - loss: 0.0209 - acc: 0.0000e+00 - val_loss: 0.0961 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 40s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1331 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 42s - loss: 0.0125 - acc: 0.0000e+00 - val_loss: 0.1637 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 42s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1842 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 42s - loss: 0.0163 - acc: 0.0000e+00 - val_loss: 0.1941 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 42s - loss: 0.0177 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 42s - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 40s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1775 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 40s - loss: 0.0132 - acc: 0.0000e+00 - val_loss: 0.1634 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 43s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1479 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 41s - loss: 0.0130 - acc: 0.0000e+00 - val_loss: 0.1332 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 41s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1209 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 41s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1115 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 43s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1058 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 42s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 42s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1079 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 40s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1135 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 41s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1212 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 41s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1294 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 41s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1368 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 40s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1427 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 43s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1461 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 43s - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.1471 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 40s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1452 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 41s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 41s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1337 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 41s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.1255 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 41s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1167 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 40s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1077 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 40s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.0994 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 42s - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0927 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 40s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0874 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 40s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0835 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 41s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0820 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 40s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0815 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 40s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0812 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 42s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 41s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0752 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 40s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0685 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 41s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 44s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0501 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 43s - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 41s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 43s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 41s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 46s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 46s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 47s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0133 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 45s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0091 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 50s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 50s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 49s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 43s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 44s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 48s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 51s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 43s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 43s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 47s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 45s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00207 MSE (0.05 RMSE)\n",
      "Test Score: 0.00586 MSE (0.08 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_173 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_174 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_173 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 181s - loss: 0.1180 - acc: 0.0000e+00 - val_loss: 0.5349 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 50s - loss: 0.1125 - acc: 0.0000e+00 - val_loss: 0.5076 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 48s - loss: 0.1053 - acc: 0.0000e+00 - val_loss: 0.4656 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 49s - loss: 0.0942 - acc: 0.0000e+00 - val_loss: 0.3965 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 46s - loss: 0.0776 - acc: 0.0000e+00 - val_loss: 0.2906 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 48s - loss: 0.0510 - acc: 0.0000e+00 - val_loss: 0.1606 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 45s - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.0523 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 47s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 47s - loss: 0.0335 - acc: 0.0000e+00 - val_loss: 0.0546 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 46s - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0916 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 51s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1325 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 48s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.1650 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 45s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.1834 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 48s - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.1884 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 47s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1828 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 47s - loss: 0.0158 - acc: 0.0000e+00 - val_loss: 0.1692 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 50s - loss: 0.0142 - acc: 0.0000e+00 - val_loss: 0.1508 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 45s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1306 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 48s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1112 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 50s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0959 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 50s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.0870 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 50s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0851 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 49s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0894 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 44s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.0984 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 49s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1096 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 48s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1208 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 49s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1299 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 49s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.1360 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 47s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1378 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 46s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1356 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 49s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.1293 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 51s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1199 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 50s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.1084 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 45s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0965 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 43s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0854 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 46s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 43s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0707 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 44s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0694 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 44s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.0716 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 44s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0755 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 43s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 44s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0794 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 44s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0762 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 44s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0690 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 43s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0577 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 43s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0442 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 44s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 43s - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 44s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 44s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 43s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 42s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 44s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 43s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0106 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 43s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 48s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 45s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 51s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 51s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 50s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 46s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 58s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0123 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 59s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 60s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 56s - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 53s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 53s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 53s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 51s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 51s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00209 MSE (0.05 RMSE)\n",
      "Test Score: 0.00907 MSE (0.10 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_175 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_176 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 164s - loss: 0.1172 - acc: 0.0000e+00 - val_loss: 0.5346 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 47s - loss: 0.1119 - acc: 0.0000e+00 - val_loss: 0.5048 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 52s - loss: 0.1043 - acc: 0.0000e+00 - val_loss: 0.4573 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 50s - loss: 0.0928 - acc: 0.0000e+00 - val_loss: 0.3820 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 53s - loss: 0.0728 - acc: 0.0000e+00 - val_loss: 0.2749 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 53s - loss: 0.0443 - acc: 0.0000e+00 - val_loss: 0.1516 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 44s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.0517 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 45s - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 45s - loss: 0.0314 - acc: 0.0000e+00 - val_loss: 0.0694 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 46s - loss: 0.0172 - acc: 0.0000e+00 - val_loss: 0.1041 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 47s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.1415 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 48s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 45s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1909 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 48s - loss: 0.0173 - acc: 0.0000e+00 - val_loss: 0.1983 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 49s - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.1962 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 46s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 50s - loss: 0.0149 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 57s - loss: 0.0136 - acc: 0.0000e+00 - val_loss: 0.1549 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 56s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1376 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 55s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1223 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 55s - loss: 0.0122 - acc: 0.0000e+00 - val_loss: 0.1103 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 55s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1032 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 55s - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.1015 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 56s - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.1045 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 57s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.1112 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 66s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1201 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 60s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.1290 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 59s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1366 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 58s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 57s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 59s - loss: 0.0111 - acc: 0.0000e+00 - val_loss: 0.1415 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 50s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1367 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 47s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.1288 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 58s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.1187 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 52s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.1072 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 45s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0956 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 44s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0862 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 44s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0798 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 44s - loss: 0.0091 - acc: 0.0000e+00 - val_loss: 0.0768 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 44s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0769 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 49s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0783 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 51s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0802 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 50s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0796 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 47s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0752 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 48s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 48s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 44s - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 44s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 43s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 44s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 44s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0272 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 45s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0266 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 45s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 46s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 48s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0128 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 44s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 43s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 47s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 46s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 45s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 44s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 45s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 46s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 45s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 45s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 43s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 45s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 45s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 43s - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 44s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00189 MSE (0.04 RMSE)\n",
      "Test Score: 0.00618 MSE (0.08 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_177 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_178 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 161s - loss: 0.1184 - acc: 0.0000e+00 - val_loss: 0.5396 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 46s - loss: 0.1131 - acc: 0.0000e+00 - val_loss: 0.5086 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 46s - loss: 0.1054 - acc: 0.0000e+00 - val_loss: 0.4612 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 45s - loss: 0.0945 - acc: 0.0000e+00 - val_loss: 0.3847 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 47s - loss: 0.0762 - acc: 0.0000e+00 - val_loss: 0.2710 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 45s - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 45s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 45s - loss: 0.0249 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 44s - loss: 0.0311 - acc: 0.0000e+00 - val_loss: 0.0533 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 46s - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.0884 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 46s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1263 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 46s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1557 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 46s - loss: 0.0133 - acc: 0.0000e+00 - val_loss: 0.1717 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 44s - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.1746 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 44s - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1677 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 44s - loss: 0.0157 - acc: 0.0000e+00 - val_loss: 0.1539 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 44s - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.1360 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 44s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1172 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 46s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1003 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 44s - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0878 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 46s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0809 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 44s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.0806 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 44s - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.0853 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 44s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.0938 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 45s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.1045 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 45s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1149 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 48s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.1234 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 53s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.1283 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 46s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1293 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 46s - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.1259 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 47s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1189 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 46s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.1095 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 47s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0990 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 46s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0884 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 55s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 49s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0727 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 50s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0690 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 53s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0685 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 49s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0697 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 49s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0723 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 50s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0742 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 47s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0745 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 53s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0727 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 46s - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0682 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 46s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0609 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 50s - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0516 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "310/310 [==============================] - 52s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 50s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0343 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 55s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0289 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 51s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0268 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 44s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 47s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 47s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0277 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 49s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0245 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 49s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 47s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 47s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 55s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 52s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0079 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 54s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0085 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 58s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 60s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 54s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 48s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 56s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 47s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 51s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 47s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 48s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 47s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00200 MSE (0.04 RMSE)\n",
      "Test Score: 0.00690 MSE (0.08 RMSE)\n"
     ]
    }
   ],
   "source": [
    "decay_result = {}\n",
    "\n",
    "for decay in decaylist:    \n",
    "    trainScore, testScore = quick_measure(file_csv_name, window_size, d, shape, neurons, epochs, decay)\n",
    "    decay_result[decay] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnEAAAHwCAYAAADJiTnYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAYds1qOSspuLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1e11f6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(decay_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Decay')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter window_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neurons = [128, 128, 64, 1]\n",
    "epochs = 70\n",
    "d = 0.7 #dropout\n",
    "decay = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_179 (LSTM)              (None, 5, 128)            68096     \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_180 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 403 samples, validate on 45 samples\n",
      "Epoch 1/70\n",
      "403/403 [==============================] - 120s - loss: 0.0837 - acc: 0.0025 - val_loss: 0.4554 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "403/403 [==============================] - 2s - loss: 0.0818 - acc: 0.0025 - val_loss: 0.4487 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "403/403 [==============================] - 2s - loss: 0.0800 - acc: 0.0025 - val_loss: 0.4413 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "403/403 [==============================] - 2s - loss: 0.0781 - acc: 0.0025 - val_loss: 0.4325 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "403/403 [==============================] - 2s - loss: 0.0758 - acc: 0.0025 - val_loss: 0.4217 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "403/403 [==============================] - 2s - loss: 0.0730 - acc: 0.0025 - val_loss: 0.4082 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "403/403 [==============================] - 2s - loss: 0.0701 - acc: 0.0025 - val_loss: 0.3911 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "403/403 [==============================] - 2s - loss: 0.0659 - acc: 0.0025 - val_loss: 0.3696 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "403/403 [==============================] - 2s - loss: 0.0609 - acc: 0.0025 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "403/403 [==============================] - 2s - loss: 0.0551 - acc: 0.0025 - val_loss: 0.3083 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "403/403 [==============================] - 3s - loss: 0.0481 - acc: 0.0025 - val_loss: 0.2661 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "403/403 [==============================] - 3s - loss: 0.0396 - acc: 0.0025 - val_loss: 0.2153 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "403/403 [==============================] - 2s - loss: 0.0303 - acc: 0.0025 - val_loss: 0.1566 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "403/403 [==============================] - 2s - loss: 0.0196 - acc: 0.0025 - val_loss: 0.0939 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "403/403 [==============================] - 2s - loss: 0.0096 - acc: 0.0025 - val_loss: 0.0370 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "403/403 [==============================] - 2s - loss: 0.0040 - acc: 0.0025 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "403/403 [==============================] - 2s - loss: 0.0082 - acc: 0.0025 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "403/403 [==============================] - 2s - loss: 0.0176 - acc: 0.0025 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "403/403 [==============================] - 2s - loss: 0.0168 - acc: 0.0025 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "403/403 [==============================] - 2s - loss: 0.0141 - acc: 0.0025 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "403/403 [==============================] - 2s - loss: 0.0081 - acc: 0.0025 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "403/403 [==============================] - 2s - loss: 0.0044 - acc: 0.0025 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "403/403 [==============================] - 1s - loss: 0.0040 - acc: 0.0025 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "403/403 [==============================] - 2s - loss: 0.0053 - acc: 0.0025 - val_loss: 0.0744 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "403/403 [==============================] - 2s - loss: 0.0066 - acc: 0.0025 - val_loss: 0.0868 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "403/403 [==============================] - 2s - loss: 0.0081 - acc: 0.0025 - val_loss: 0.0939 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "403/403 [==============================] - 2s - loss: 0.0089 - acc: 0.0025 - val_loss: 0.0959 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "403/403 [==============================] - 2s - loss: 0.0093 - acc: 0.0025 - val_loss: 0.0933 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "403/403 [==============================] - 1s - loss: 0.0087 - acc: 0.0025 - val_loss: 0.0868 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "403/403 [==============================] - 1s - loss: 0.0084 - acc: 0.0025 - val_loss: 0.0771 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "403/403 [==============================] - 2s - loss: 0.0061 - acc: 0.0025 - val_loss: 0.0653 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "403/403 [==============================] - 2s - loss: 0.0057 - acc: 0.0025 - val_loss: 0.0528 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "403/403 [==============================] - 1s - loss: 0.0043 - acc: 0.0025 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "403/403 [==============================] - 2s - loss: 0.0045 - acc: 0.0025 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "403/403 [==============================] - 1s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "403/403 [==============================] - 1s - loss: 0.0048 - acc: 0.0025 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "403/403 [==============================] - 1s - loss: 0.0057 - acc: 0.0025 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "403/403 [==============================] - 1s - loss: 0.0055 - acc: 0.0025 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "403/403 [==============================] - 2s - loss: 0.0053 - acc: 0.0025 - val_loss: 0.0177 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "403/403 [==============================] - 2s - loss: 0.0050 - acc: 0.0025 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "403/403 [==============================] - 2s - loss: 0.0045 - acc: 0.0025 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "403/403 [==============================] - 2s - loss: 0.0042 - acc: 0.0025 - val_loss: 0.0342 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "403/403 [==============================] - 1s - loss: 0.0043 - acc: 0.0025 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "403/403 [==============================] - 1s - loss: 0.0039 - acc: 0.0025 - val_loss: 0.0451 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "403/403 [==============================] - 2s - loss: 0.0038 - acc: 0.0025 - val_loss: 0.0483 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "403/403 [==============================] - 1s - loss: 0.0040 - acc: 0.0025 - val_loss: 0.0496 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "403/403 [==============================] - 2s - loss: 0.0042 - acc: 0.0025 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "403/403 [==============================] - 2s - loss: 0.0047 - acc: 0.0025 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "403/403 [==============================] - 2s - loss: 0.0039 - acc: 0.0025 - val_loss: 0.0426 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "403/403 [==============================] - 2s - loss: 0.0042 - acc: 0.0025 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "403/403 [==============================] - 2s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "403/403 [==============================] - 2s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "403/403 [==============================] - 2s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "403/403 [==============================] - 2s - loss: 0.0039 - acc: 0.0025 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "403/403 [==============================] - 2s - loss: 0.0035 - acc: 0.0025 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "403/403 [==============================] - 2s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "403/403 [==============================] - 1s - loss: 0.0041 - acc: 0.0025 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "403/403 [==============================] - 2s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "403/403 [==============================] - 2s - loss: 0.0035 - acc: 0.0025 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "403/403 [==============================] - 2s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "403/403 [==============================] - 2s - loss: 0.0034 - acc: 0.0025 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "403/403 [==============================] - 2s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "403/403 [==============================] - 2s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "403/403 [==============================] - 1s - loss: 0.0034 - acc: 0.0025 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "403/403 [==============================] - 2s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "403/403 [==============================] - 2s - loss: 0.0035 - acc: 0.0025 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "403/403 [==============================] - 2s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "403/403 [==============================] - 2s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "403/403 [==============================] - 2s - loss: 0.0028 - acc: 0.0025 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "403/403 [==============================] - 2s - loss: 0.0034 - acc: 0.0025 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00274 MSE (0.05 RMSE)\n",
      "Test Score: 0.03407 MSE (0.18 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_181 (LSTM)              (None, 10, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_182 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 399 samples, validate on 45 samples\n",
      "Epoch 1/70\n",
      "399/399 [==============================] - 107s - loss: 0.0847 - acc: 0.0025 - val_loss: 0.4583 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "399/399 [==============================] - 4s - loss: 0.0822 - acc: 0.0025 - val_loss: 0.4483 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "399/399 [==============================] - 4s - loss: 0.0796 - acc: 0.0025 - val_loss: 0.4351 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "399/399 [==============================] - 4s - loss: 0.0763 - acc: 0.0025 - val_loss: 0.4170 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "399/399 [==============================] - 4s - loss: 0.0722 - acc: 0.0025 - val_loss: 0.3921 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "399/399 [==============================] - 4s - loss: 0.0667 - acc: 0.0025 - val_loss: 0.3574 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "399/399 [==============================] - 4s - loss: 0.0593 - acc: 0.0025 - val_loss: 0.3100 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "399/399 [==============================] - 4s - loss: 0.0491 - acc: 0.0025 - val_loss: 0.2484 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "399/399 [==============================] - 5s - loss: 0.0373 - acc: 0.0025 - val_loss: 0.1735 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "399/399 [==============================] - 4s - loss: 0.0208 - acc: 0.0025 - val_loss: 0.0936 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "399/399 [==============================] - 3s - loss: 0.0083 - acc: 0.0025 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "399/399 [==============================] - 4s - loss: 0.0065 - acc: 0.0025 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "399/399 [==============================] - 5s - loss: 0.0212 - acc: 0.0025 - val_loss: 0.0081 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "399/399 [==============================] - 4s - loss: 0.0208 - acc: 0.0025 - val_loss: 0.0176 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "399/399 [==============================] - 4s - loss: 0.0120 - acc: 0.0025 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "399/399 [==============================] - 3s - loss: 0.0068 - acc: 0.0025 - val_loss: 0.0638 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "399/399 [==============================] - 3s - loss: 0.0053 - acc: 0.0025 - val_loss: 0.0898 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "399/399 [==============================] - 4s - loss: 0.0066 - acc: 0.0025 - val_loss: 0.1104 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "399/399 [==============================] - 3s - loss: 0.0080 - acc: 0.0025 - val_loss: 0.1236 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "399/399 [==============================] - 4s - loss: 0.0103 - acc: 0.0025 - val_loss: 0.1291 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "399/399 [==============================] - 4s - loss: 0.0112 - acc: 0.0025 - val_loss: 0.1278 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "399/399 [==============================] - 4s - loss: 0.0115 - acc: 0.0025 - val_loss: 0.1206 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "399/399 [==============================] - 4s - loss: 0.0094 - acc: 0.0025 - val_loss: 0.1090 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "399/399 [==============================] - 4s - loss: 0.0081 - acc: 0.0025 - val_loss: 0.0944 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "399/399 [==============================] - 4s - loss: 0.0061 - acc: 0.0025 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "399/399 [==============================] - 3s - loss: 0.0048 - acc: 0.0025 - val_loss: 0.0640 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "399/399 [==============================] - 3s - loss: 0.0048 - acc: 0.0025 - val_loss: 0.0519 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "399/399 [==============================] - 3s - loss: 0.0059 - acc: 0.0025 - val_loss: 0.0439 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "399/399 [==============================] - 4s - loss: 0.0069 - acc: 0.0025 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "399/399 [==============================] - 5s - loss: 0.0068 - acc: 0.0025 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "399/399 [==============================] - 4s - loss: 0.0069 - acc: 0.0025 - val_loss: 0.0465 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "399/399 [==============================] - 3s - loss: 0.0062 - acc: 0.0025 - val_loss: 0.0537 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "399/399 [==============================] - 5s - loss: 0.0051 - acc: 0.0025 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "399/399 [==============================] - 4s - loss: 0.0049 - acc: 0.0025 - val_loss: 0.0705 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "399/399 [==============================] - 3s - loss: 0.0046 - acc: 0.0025 - val_loss: 0.0775 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "399/399 [==============================] - 3s - loss: 0.0052 - acc: 0.0025 - val_loss: 0.0823 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "399/399 [==============================] - 4s - loss: 0.0057 - acc: 0.0025 - val_loss: 0.0842 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "399/399 [==============================] - 3s - loss: 0.0054 - acc: 0.0025 - val_loss: 0.0833 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 4s - loss: 0.0052 - acc: 0.0025 - val_loss: 0.0798 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "399/399 [==============================] - 4s - loss: 0.0050 - acc: 0.0025 - val_loss: 0.0741 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "399/399 [==============================] - 4s - loss: 0.0043 - acc: 0.0025 - val_loss: 0.0673 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "399/399 [==============================] - 4s - loss: 0.0045 - acc: 0.0025 - val_loss: 0.0600 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "399/399 [==============================] - 4s - loss: 0.0042 - acc: 0.0025 - val_loss: 0.0533 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "399/399 [==============================] - 4s - loss: 0.0041 - acc: 0.0025 - val_loss: 0.0478 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "399/399 [==============================] - 3s - loss: 0.0043 - acc: 0.0025 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "399/399 [==============================] - 3s - loss: 0.0042 - acc: 0.0025 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "399/399 [==============================] - 4s - loss: 0.0045 - acc: 0.0025 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "399/399 [==============================] - 4s - loss: 0.0043 - acc: 0.0025 - val_loss: 0.0437 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "399/399 [==============================] - 4s - loss: 0.0044 - acc: 0.0025 - val_loss: 0.0462 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "399/399 [==============================] - 4s - loss: 0.0039 - acc: 0.0025 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "399/399 [==============================] - 4s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0514 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "399/399 [==============================] - 3s - loss: 0.0038 - acc: 0.0025 - val_loss: 0.0529 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "399/399 [==============================] - 3s - loss: 0.0041 - acc: 0.0025 - val_loss: 0.0531 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "399/399 [==============================] - 4s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0518 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "399/399 [==============================] - 4s - loss: 0.0041 - acc: 0.0025 - val_loss: 0.0488 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "399/399 [==============================] - 4s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0449 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "399/399 [==============================] - 4s - loss: 0.0038 - acc: 0.0025 - val_loss: 0.0405 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "399/399 [==============================] - 4s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "399/399 [==============================] - 4s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "399/399 [==============================] - 3s - loss: 0.0040 - acc: 0.0025 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "399/399 [==============================] - 3s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "399/399 [==============================] - 3s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0295 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "399/399 [==============================] - 4s - loss: 0.0031 - acc: 0.0025 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "399/399 [==============================] - 3s - loss: 0.0033 - acc: 0.0025 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "399/399 [==============================] - 4s - loss: 0.0036 - acc: 0.0025 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "399/399 [==============================] - 4s - loss: 0.0034 - acc: 0.0025 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "399/399 [==============================] - 4s - loss: 0.0032 - acc: 0.0025 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "399/399 [==============================] - 3s - loss: 0.0035 - acc: 0.0025 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "399/399 [==============================] - 4s - loss: 0.0037 - acc: 0.0025 - val_loss: 0.0266 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "399/399 [==============================] - 4s - loss: 0.0033 - acc: 0.0025 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00374 MSE (0.06 RMSE)\n",
      "Test Score: 0.06214 MSE (0.25 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_183 (LSTM)              (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_184 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 389 samples, validate on 44 samples\n",
      "Epoch 1/70\n",
      "389/389 [==============================] - 98s - loss: 0.0878 - acc: 0.0000e+00 - val_loss: 0.4589 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "389/389 [==============================] - 12s - loss: 0.0836 - acc: 0.0000e+00 - val_loss: 0.4323 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "389/389 [==============================] - 9s - loss: 0.0779 - acc: 0.0000e+00 - val_loss: 0.3932 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "389/389 [==============================] - 9s - loss: 0.0694 - acc: 0.0000e+00 - val_loss: 0.3363 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "389/389 [==============================] - 8s - loss: 0.0567 - acc: 0.0000e+00 - val_loss: 0.2539 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "389/389 [==============================] - 9s - loss: 0.0396 - acc: 0.0000e+00 - val_loss: 0.1492 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "389/389 [==============================] - 8s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.0504 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "389/389 [==============================] - 9s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0101 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "389/389 [==============================] - 11s - loss: 0.0323 - acc: 0.0000e+00 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "389/389 [==============================] - 11s - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "389/389 [==============================] - 10s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0751 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "389/389 [==============================] - 9s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.1123 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "389/389 [==============================] - 9s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.1401 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "389/389 [==============================] - 10s - loss: 0.0118 - acc: 0.0000e+00 - val_loss: 0.1555 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "389/389 [==============================] - 10s - loss: 0.0145 - acc: 0.0000e+00 - val_loss: 0.1595 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "389/389 [==============================] - 9s - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.1541 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "389/389 [==============================] - 9s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "389/389 [==============================] - 9s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1247 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "389/389 [==============================] - 10s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.1054 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "389/389 [==============================] - 8s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0864 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "389/389 [==============================] - 9s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0707 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "389/389 [==============================] - 9s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0603 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "389/389 [==============================] - 9s - loss: 0.0079 - acc: 0.0000e+00 - val_loss: 0.0560 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "389/389 [==============================] - 10s - loss: 0.0094 - acc: 0.0000e+00 - val_loss: 0.0580 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "389/389 [==============================] - 10s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0650 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "389/389 [==============================] - 9s - loss: 0.0079 - acc: 0.0000e+00 - val_loss: 0.0753 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "389/389 [==============================] - 9s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0872 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "389/389 [==============================] - 9s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0989 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "389/389 [==============================] - 9s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.1086 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "389/389 [==============================] - 9s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.1147 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "389/389 [==============================] - 9s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.1171 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "389/389 [==============================] - 10s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.1151 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "389/389 [==============================] - 10s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.1097 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "389/389 [==============================] - 9s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.1019 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "389/389 [==============================] - 10s - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0928 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "389/389 [==============================] - 9s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0831 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "389/389 [==============================] - 10s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0744 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "389/389 [==============================] - 9s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "389/389 [==============================] - 10s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0643 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "389/389 [==============================] - 10s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0638 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "389/389 [==============================] - 10s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0661 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "389/389 [==============================] - 9s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0705 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "389/389 [==============================] - 10s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0759 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "389/389 [==============================] - 9s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0808 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "389/389 [==============================] - 10s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0839 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "389/389 [==============================] - 10s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0845 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "389/389 [==============================] - 9s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0823 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "389/389 [==============================] - 10s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0777 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "389/389 [==============================] - 10s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "389/389 [==============================] - 10s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0642 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "389/389 [==============================] - 10s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0581 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "389/389 [==============================] - 9s - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0532 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "389/389 [==============================] - 8s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0501 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "389/389 [==============================] - 9s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0487 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "389/389 [==============================] - 8s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0493 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "389/389 [==============================] - 9s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "389/389 [==============================] - 9s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0525 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "389/389 [==============================] - 9s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0540 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "389/389 [==============================] - 10s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0542 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "389/389 [==============================] - 9s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0522 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "389/389 [==============================] - 10s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0488 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "389/389 [==============================] - 9s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "389/389 [==============================] - 9s - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "389/389 [==============================] - 9s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "389/389 [==============================] - 10s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "389/389 [==============================] - 10s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "389/389 [==============================] - 9s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "389/389 [==============================] - 9s - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "389/389 [==============================] - 9s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "389/389 [==============================] - 9s - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00518 MSE (0.07 RMSE)\n",
      "Test Score: 0.07489 MSE (0.27 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_185 (LSTM)              (None, 60, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_186 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 359 samples, validate on 40 samples\n",
      "Epoch 1/70\n",
      "359/359 [==============================] - 141s - loss: 0.1007 - acc: 0.0000e+00 - val_loss: 0.4885 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "359/359 [==============================] - 27s - loss: 0.0964 - acc: 0.0000e+00 - val_loss: 0.4643 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "359/359 [==============================] - 25s - loss: 0.0903 - acc: 0.0000e+00 - val_loss: 0.4286 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 24s - loss: 0.0825 - acc: 0.0000e+00 - val_loss: 0.3724 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "359/359 [==============================] - 25s - loss: 0.0701 - acc: 0.0000e+00 - val_loss: 0.2864 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "359/359 [==============================] - 25s - loss: 0.0493 - acc: 0.0000e+00 - val_loss: 0.1757 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "359/359 [==============================] - 24s - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.0707 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "359/359 [==============================] - 28s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0255 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "359/359 [==============================] - 28s - loss: 0.0365 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "359/359 [==============================] - 24s - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.0645 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "359/359 [==============================] - 24s - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.1025 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "359/359 [==============================] - 24s - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.1379 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "359/359 [==============================] - 24s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.1638 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "359/359 [==============================] - 26s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.1779 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "359/359 [==============================] - 26s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.1819 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "359/359 [==============================] - 25s - loss: 0.0159 - acc: 0.0000e+00 - val_loss: 0.1777 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "359/359 [==============================] - 24s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1675 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "359/359 [==============================] - 24s - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.1531 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "359/359 [==============================] - 24s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1367 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "359/359 [==============================] - 24s - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.1204 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "359/359 [==============================] - 24s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.1062 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "359/359 [==============================] - 28s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0955 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "359/359 [==============================] - 26s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0898 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "359/359 [==============================] - 27s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0891 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "359/359 [==============================] - 27s - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.0932 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "359/359 [==============================] - 26s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.1004 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "359/359 [==============================] - 27s - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.1090 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "359/359 [==============================] - 26s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.1181 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "359/359 [==============================] - 27s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.1267 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "359/359 [==============================] - 26s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.1339 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "359/359 [==============================] - 26s - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "359/359 [==============================] - 26s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "359/359 [==============================] - 30s - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "359/359 [==============================] - 27s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.1354 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "359/359 [==============================] - 25s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.1294 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "359/359 [==============================] - 26s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.1220 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "359/359 [==============================] - 25s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.1140 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "359/359 [==============================] - 27s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.1060 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "359/359 [==============================] - 26s - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0991 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "359/359 [==============================] - 26s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0940 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "359/359 [==============================] - 27s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0901 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "359/359 [==============================] - 25s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0885 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "359/359 [==============================] - 27s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0884 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "359/359 [==============================] - 25s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0902 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "359/359 [==============================] - 26s - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0927 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "359/359 [==============================] - 27s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0954 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "359/359 [==============================] - 26s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0970 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "359/359 [==============================] - 27s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0964 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "359/359 [==============================] - 26s - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0935 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "359/359 [==============================] - 24s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0883 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "359/359 [==============================] - 25s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0809 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "359/359 [==============================] - 24s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0728 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "359/359 [==============================] - 24s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0652 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "359/359 [==============================] - 25s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0594 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "359/359 [==============================] - 25s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0552 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "359/359 [==============================] - 26s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0530 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "359/359 [==============================] - 26s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0522 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "359/359 [==============================] - 29s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0510 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "359/359 [==============================] - 29s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "359/359 [==============================] - 28s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0462 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "359/359 [==============================] - 28s - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "359/359 [==============================] - 27s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "359/359 [==============================] - 25s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "359/359 [==============================] - 26s - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "359/359 [==============================] - 25s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "359/359 [==============================] - 27s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "359/359 [==============================] - 26s - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "359/359 [==============================] - 28s - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "359/359 [==============================] - 28s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "359/359 [==============================] - 26s - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00252 MSE (0.05 RMSE)\n",
      "Test Score: 0.03790 MSE (0.19 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_187 (LSTM)              (None, 120, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_188 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 310 samples, validate on 35 samples\n",
      "Epoch 1/70\n",
      "310/310 [==============================] - 179s - loss: 0.1193 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "310/310 [==============================] - 58s - loss: 0.1151 - acc: 0.0000e+00 - val_loss: 0.5153 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "310/310 [==============================] - 54s - loss: 0.1087 - acc: 0.0000e+00 - val_loss: 0.4759 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "310/310 [==============================] - 52s - loss: 0.0996 - acc: 0.0000e+00 - val_loss: 0.4220 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "310/310 [==============================] - 54s - loss: 0.0872 - acc: 0.0000e+00 - val_loss: 0.3414 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "310/310 [==============================] - 52s - loss: 0.0681 - acc: 0.0000e+00 - val_loss: 0.2327 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "310/310 [==============================] - 51s - loss: 0.0391 - acc: 0.0000e+00 - val_loss: 0.1178 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "310/310 [==============================] - 52s - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "310/310 [==============================] - 53s - loss: 0.0252 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "310/310 [==============================] - 54s - loss: 0.0353 - acc: 0.0000e+00 - val_loss: 0.0475 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "310/310 [==============================] - 53s - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.0783 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "310/310 [==============================] - 51s - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.1153 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "310/310 [==============================] - 52s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1463 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "310/310 [==============================] - 51s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1657 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "310/310 [==============================] - 52s - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.1725 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "310/310 [==============================] - 52s - loss: 0.0182 - acc: 0.0000e+00 - val_loss: 0.1689 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "310/310 [==============================] - 51s - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.1575 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "310/310 [==============================] - 55s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1411 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "310/310 [==============================] - 56s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1224 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "310/310 [==============================] - 57s - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.1046 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "310/310 [==============================] - 57s - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0893 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "310/310 [==============================] - 56s - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0783 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "310/310 [==============================] - 56s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.0726 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "310/310 [==============================] - 56s - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.0731 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "310/310 [==============================] - 42s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0781 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "310/310 [==============================] - 52s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0868 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "310/310 [==============================] - 52s - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.0972 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "310/310 [==============================] - 52s - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.1074 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "310/310 [==============================] - 51s - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.1160 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "310/310 [==============================] - 51s - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.1212 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "310/310 [==============================] - 51s - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.1222 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "310/310 [==============================] - 52s - loss: 0.0107 - acc: 0.0000e+00 - val_loss: 0.1193 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "310/310 [==============================] - 51s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.1129 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "310/310 [==============================] - 51s - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.1037 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "310/310 [==============================] - 51s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.0924 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "310/310 [==============================] - 51s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0810 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "310/310 [==============================] - 51s - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "310/310 [==============================] - 51s - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0640 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "310/310 [==============================] - 51s - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "310/310 [==============================] - 51s - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0590 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "310/310 [==============================] - 50s - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "310/310 [==============================] - 51s - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0616 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "310/310 [==============================] - 52s - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.0639 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "310/310 [==============================] - 55s - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0655 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "310/310 [==============================] - 51s - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0653 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "310/310 [==============================] - 50s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0620 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 54s - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0558 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "310/310 [==============================] - 50s - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "310/310 [==============================] - 49s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "310/310 [==============================] - 49s - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "310/310 [==============================] - 50s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "310/310 [==============================] - 50s - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "310/310 [==============================] - 52s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "310/310 [==============================] - 51s - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "310/310 [==============================] - 56s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0244 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "310/310 [==============================] - 56s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "310/310 [==============================] - 52s - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "310/310 [==============================] - 52s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "310/310 [==============================] - 53s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0075 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "310/310 [==============================] - 54s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "310/310 [==============================] - 53s - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "310/310 [==============================] - 52s - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "310/310 [==============================] - 53s - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "310/310 [==============================] - 52s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "310/310 [==============================] - 52s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "310/310 [==============================] - 53s - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "310/310 [==============================] - 52s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "310/310 [==============================] - 52s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "310/310 [==============================] - 53s - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "310/310 [==============================] - 53s - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00206 MSE (0.05 RMSE)\n",
      "Test Score: 0.00594 MSE (0.08 RMSE)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_189 (LSTM)              (None, 180, 128)          68096     \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 180, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_190 (LSTM)              (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 208,001\n",
      "Trainable params: 208,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 261 samples, validate on 30 samples\n",
      "Epoch 1/70\n",
      "261/261 [==============================] - 218s - loss: 0.1500 - acc: 0.0000e+00 - val_loss: 0.6333 - val_acc: 0.0000e+00\n",
      "Epoch 2/70\n",
      "261/261 [==============================] - 77s - loss: 0.1430 - acc: 0.0000e+00 - val_loss: 0.6009 - val_acc: 0.0000e+00\n",
      "Epoch 3/70\n",
      "261/261 [==============================] - 77s - loss: 0.1344 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 4/70\n",
      "261/261 [==============================] - 77s - loss: 0.1218 - acc: 0.0000e+00 - val_loss: 0.4826 - val_acc: 0.0000e+00\n",
      "Epoch 5/70\n",
      "261/261 [==============================] - 76s - loss: 0.1039 - acc: 0.0000e+00 - val_loss: 0.3713 - val_acc: 0.0000e+00\n",
      "Epoch 6/70\n",
      "261/261 [==============================] - 74s - loss: 0.0718 - acc: 0.0000e+00 - val_loss: 0.2353 - val_acc: 0.0000e+00\n",
      "Epoch 7/70\n",
      "261/261 [==============================] - 75s - loss: 0.0332 - acc: 0.0000e+00 - val_loss: 0.1125 - val_acc: 0.0000e+00\n",
      "Epoch 8/70\n",
      "261/261 [==============================] - 74s - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.0500 - val_acc: 0.0000e+00\n",
      "Epoch 9/70\n",
      "261/261 [==============================] - 77s - loss: 0.0415 - acc: 0.0000e+00 - val_loss: 0.0554 - val_acc: 0.0000e+00\n",
      "Epoch 10/70\n",
      "261/261 [==============================] - 74s - loss: 0.0373 - acc: 0.0000e+00 - val_loss: 0.0819 - val_acc: 0.0000e+00\n",
      "Epoch 11/70\n",
      "261/261 [==============================] - 76s - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.1224 - val_acc: 0.0000e+00\n",
      "Epoch 12/70\n",
      "261/261 [==============================] - 76s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1665 - val_acc: 0.0000e+00\n",
      "Epoch 13/70\n",
      "261/261 [==============================] - 75s - loss: 0.0152 - acc: 0.0000e+00 - val_loss: 0.2049 - val_acc: 0.0000e+00\n",
      "Epoch 14/70\n",
      "261/261 [==============================] - 73s - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.2305 - val_acc: 0.0000e+00\n",
      "Epoch 15/70\n",
      "261/261 [==============================] - 76s - loss: 0.0210 - acc: 0.0000e+00 - val_loss: 0.2432 - val_acc: 0.0000e+00\n",
      "Epoch 16/70\n",
      "261/261 [==============================] - 70s - loss: 0.0241 - acc: 0.0000e+00 - val_loss: 0.2445 - val_acc: 0.0000e+00\n",
      "Epoch 17/70\n",
      "261/261 [==============================] - 71s - loss: 0.0231 - acc: 0.0000e+00 - val_loss: 0.2370 - val_acc: 0.0000e+00\n",
      "Epoch 18/70\n",
      "261/261 [==============================] - 69s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.2233 - val_acc: 0.0000e+00\n",
      "Epoch 19/70\n",
      "261/261 [==============================] - 70s - loss: 0.0181 - acc: 0.0000e+00 - val_loss: 0.2058 - val_acc: 0.0000e+00\n",
      "Epoch 20/70\n",
      "261/261 [==============================] - 70s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1869 - val_acc: 0.0000e+00\n",
      "Epoch 21/70\n",
      "261/261 [==============================] - 71s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0000e+00\n",
      "Epoch 22/70\n",
      "261/261 [==============================] - 70s - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.1523 - val_acc: 0.0000e+00\n",
      "Epoch 23/70\n",
      "261/261 [==============================] - 72s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1401 - val_acc: 0.0000e+00\n",
      "Epoch 24/70\n",
      "261/261 [==============================] - 70s - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.1321 - val_acc: 0.0000e+00\n",
      "Epoch 25/70\n",
      "261/261 [==============================] - 69s - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1296 - val_acc: 0.0000e+00\n",
      "Epoch 26/70\n",
      "261/261 [==============================] - 72s - loss: 0.0156 - acc: 0.0000e+00 - val_loss: 0.1315 - val_acc: 0.0000e+00\n",
      "Epoch 27/70\n",
      "261/261 [==============================] - 70s - loss: 0.0183 - acc: 0.0000e+00 - val_loss: 0.1372 - val_acc: 0.0000e+00\n",
      "Epoch 28/70\n",
      "261/261 [==============================] - 76s - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.1457 - val_acc: 0.0000e+00\n",
      "Epoch 29/70\n",
      "261/261 [==============================] - 69s - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1556 - val_acc: 0.0000e+00\n",
      "Epoch 30/70\n",
      "261/261 [==============================] - 69s - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.1653 - val_acc: 0.0000e+00\n",
      "Epoch 31/70\n",
      "261/261 [==============================] - 68s - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.1739 - val_acc: 0.0000e+00\n",
      "Epoch 32/70\n",
      "261/261 [==============================] - 70s - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0000e+00\n",
      "Epoch 33/70\n",
      "261/261 [==============================] - 72s - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.1837 - val_acc: 0.0000e+00\n",
      "Epoch 34/70\n",
      "261/261 [==============================] - 71s - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.1838 - val_acc: 0.0000e+00\n",
      "Epoch 35/70\n",
      "261/261 [==============================] - 69s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1801 - val_acc: 0.0000e+00\n",
      "Epoch 36/70\n",
      "261/261 [==============================] - 69s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1730 - val_acc: 0.0000e+00\n",
      "Epoch 37/70\n",
      "261/261 [==============================] - 69s - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
      "Epoch 38/70\n",
      "261/261 [==============================] - 70s - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.1519 - val_acc: 0.0000e+00\n",
      "Epoch 39/70\n",
      "261/261 [==============================] - 69s - loss: 0.0135 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
      "Epoch 40/70\n",
      "261/261 [==============================] - 74s - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.1285 - val_acc: 0.0000e+00\n",
      "Epoch 41/70\n",
      "261/261 [==============================] - 70s - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.1192 - val_acc: 0.0000e+00\n",
      "Epoch 42/70\n",
      "261/261 [==============================] - 71s - loss: 0.0119 - acc: 0.0000e+00 - val_loss: 0.1135 - val_acc: 0.0000e+00\n",
      "Epoch 43/70\n",
      "261/261 [==============================] - 69s - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.1097 - val_acc: 0.0000e+00\n",
      "Epoch 44/70\n",
      "261/261 [==============================] - 71s - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.1076 - val_acc: 0.0000e+00\n",
      "Epoch 45/70\n",
      "261/261 [==============================] - 70s - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.1067 - val_acc: 0.0000e+00\n",
      "Epoch 46/70\n",
      "261/261 [==============================] - 71s - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.1065 - val_acc: 0.0000e+00\n",
      "Epoch 47/70\n",
      "261/261 [==============================] - 71s - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1060 - val_acc: 0.0000e+00\n",
      "Epoch 48/70\n",
      "261/261 [==============================] - 73s - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.1033 - val_acc: 0.0000e+00\n",
      "Epoch 49/70\n",
      "261/261 [==============================] - 70s - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0976 - val_acc: 0.0000e+00\n",
      "Epoch 50/70\n",
      "261/261 [==============================] - 69s - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.0888 - val_acc: 0.0000e+00\n",
      "Epoch 51/70\n",
      "261/261 [==============================] - 72s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0770 - val_acc: 0.0000e+00\n",
      "Epoch 52/70\n",
      "261/261 [==============================] - 70s - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0622 - val_acc: 0.0000e+00\n",
      "Epoch 53/70\n",
      "261/261 [==============================] - 72s - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0487 - val_acc: 0.0000e+00\n",
      "Epoch 54/70\n",
      "261/261 [==============================] - 70s - loss: 0.0077 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 55/70\n",
      "261/261 [==============================] - 69s - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0326 - val_acc: 0.0000e+00\n",
      "Epoch 56/70\n",
      "261/261 [==============================] - 69s - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0000e+00\n",
      "Epoch 57/70\n",
      "261/261 [==============================] - 71s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
      "Epoch 58/70\n",
      "261/261 [==============================] - 70s - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0242 - val_acc: 0.0000e+00\n",
      "Epoch 59/70\n",
      "261/261 [==============================] - 68s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
      "Epoch 60/70\n",
      "261/261 [==============================] - 69s - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0111 - val_acc: 0.0000e+00\n",
      "Epoch 61/70\n",
      "261/261 [==============================] - 73s - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0103 - val_acc: 0.0000e+00\n",
      "Epoch 62/70\n",
      "261/261 [==============================] - 69s - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0101 - val_acc: 0.0000e+00\n",
      "Epoch 63/70\n",
      "261/261 [==============================] - 70s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 64/70\n",
      "261/261 [==============================] - 70s - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
      "Epoch 65/70\n",
      "261/261 [==============================] - 69s - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 66/70\n",
      "261/261 [==============================] - 70s - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 67/70\n",
      "261/261 [==============================] - 68s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 68/70\n",
      "261/261 [==============================] - 70s - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0094 - val_acc: 0.0000e+00\n",
      "Epoch 69/70\n",
      "261/261 [==============================] - 71s - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0106 - val_acc: 0.0000e+00\n",
      "Epoch 70/70\n",
      "261/261 [==============================] - 70s - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00271 MSE (0.05 RMSE)\n",
      "Test Score: 0.00990 MSE (0.10 RMSE)\n"
     ]
    }
   ],
   "source": [
    "window_size_list = [5, 10, 22, 60, 120, 180]\n",
    "\n",
    "window_size_result = {}\n",
    "\n",
    "for window_size in window_size_list:\n",
    "    shape = [4, window_size, 1]\n",
    "    \n",
    "    trainScore, testScore = quick_measure(file_csv_name, window_size, d, shape, neurons, epochs, decay)\n",
    "    window_size_result[window_size] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAHwCAYAAAD5BSj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhJgCmsiIiIiAaawJiIiIhJg/w/I+zVEbGtRMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2cba10128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(seq_len_result.items()) #   window_size_result\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
